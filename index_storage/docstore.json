{"docstore/data": {"0ce73017-f27e-4f71-9929-f0e5e6c1c453": {"__data__": {"id_": "0ce73017-f27e-4f71-9929-f0e5e6c1c453", "embedding": null, "metadata": {"page_label": "1", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section introduces the N-BEATS model, a deep learning architecture designed for univariate time series forecasting. Key topics and entities include:\n\n1. **Authors and Affiliations**:\n   - Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados (Element AI)\n   - Yoshua Bengio (Mila)\n\n2. **Problem Focus**:\n   - Univariate time series point forecasting using deep learning.\n\n3. **Proposed Solution**:\n   - A deep neural architecture featuring backward and forward residual links and a deep stack of fully-connected layers.\n   - The architecture is interpretable, versatile across various domains, and efficient in training.\n\n4. **Performance and Testing**:\n   - Tested on datasets like M3, M4, and TOURISM.\n   - Achieved state-of-the-art performance, improving forecast accuracy by 11% over statistical benchmarks and 3% over the previous M4 competition winner.\n\n5. **Model Configurations**:\n   - The first configuration does not use time-series-specific components, suggesting that deep learning primitives like residual blocks can effectively solve diverse forecasting problems.\n\n6. **Interpretability**:\n   - The architecture can be augmented for interpretability without significant loss in accuracy.\n\n7. **Context and Motivation**:\n   - Time series forecasting is crucial for various business applications with significant financial implications.\n   - Despite the success of deep learning in fields like computer vision and NLP, it has struggled in time series forecasting compared to classical statistical methods.\n   - The M4 competition highlighted the effectiveness of hybrid models combining neural networks with classical statistical methods.\n   - This work challenges the notion that hybrid approaches are necessary, exploring the potential of pure deep learning architectures for time series forecasting.\n\n8. **Research Question**:\n   - Can pure deep learning architectures provide accurate and interpretable time series forecasts without relying on hybrid methods?\n\n### Key Entities:\n- **N-BEATS Model**: The proposed deep learning architecture.\n- **Datasets**: M3, M4, TOURISM.\n- **Authors**: Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio.\n- **Institutions**: Element AI, Mila.\n- **Competitions**: M4 competition.\n- **Benchmark Models**: Statistical models, hybrid models (e.g., Smyl's model combining LSTM and Holt-Winters).\n\n### Key Topics:\n- Time series forecasting\n- Deep learning architectures\n- Residual links\n- Fully-connected layers\n- Model interpretability\n- Performance benchmarking\n- Hybrid vs. pure deep learning models"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_0", "node_type": "4", "metadata": {"page_label": "1", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "13c86f1b2b05dd7eb5b4b228dd0222d9c2f27d4c7e838da14b3d620c48f8bdcf", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nN-BEATS: N EURAL BASIS EXPANSION ANALYSIS FOR\nINTERPRETABLE TIME SERIES FORECASTING\nBoris N. Oreshkin\nElement AI\nboris.oreshkin@gmail.comDmitri Carpov\nElement AI\ndmitri.carpov@elementai.com\nNicolas Chapados\nElement AI\nchapados@elementai.comYoshua Bengio\nMila\nyoshua.bengio@mila.quebec\nABSTRACT\nWe focus on solving the univariate times series point forecasting problem using\ndeep learning. We propose a deep neural architecture based on backward and\nforward residual links and a very deep stack of fully-connected layers. The ar-\nchitecture has a number of desirable properties, being interpretable, applicable\nwithout modi\ufb01cation to a wide array of target domains, and fast to train. We test\nthe proposed architecture on several well-known datasets, including M3, M4 and\nTOURISM competition datasets containing time series from diverse domains. We\ndemonstrate state-of-the-art performance for two con\ufb01gurations of N-BEATS for\nall the datasets, improving forecast accuracy by 11% over a statistical benchmark\nand by 3% over last year\u2019s winner of the M4 competition, a domain-adjusted\nhand-crafted hybrid between neural network and statistical time series models.\nThe \ufb01rst con\ufb01guration of our model does not employ any time-series-speci\ufb01c\ncomponents and its performance on heterogeneous datasets strongly suggests that,\ncontrarily to received wisdom, deep learning primitives such as residual blocks are\nby themselves suf\ufb01cient to solve a wide range of forecasting problems. Finally, we\ndemonstrate how the proposed architecture can be augmented to provide outputs\nthat are interpretable without considerable loss in accuracy.\n1 I NTRODUCTION\nTime series (TS) forecasting is an important business problem and a fruitful application area for\nmachine learning (ML). It underlies most aspects of modern business, including such critical areas as\ninventory control and customer management, as well as business planning going from production and\ndistribution to \ufb01nance and marketing. As such, it has a considerable \ufb01nancial impact, often ranging\nin the millions of dollars for every point of forecasting accuracy gained (Jain, 2017; Kahn, 2003).\nAnd yet, unlike areas such as computer vision or natural language processing where deep learning\n(DL) techniques are now well entrenched, there still exists evidence that ML and DL struggle to\noutperform classical statistical TS forecasting approaches (Makridakis et al., 2018a;b). For instance,\nthe rankings of the six \u201cpure\u201d ML methods submitted to M4 competition were 23, 37, 38, 48, 54,\nand 57 out of a total of 60 entries, and most of the best-ranking methods were ensembles of classical\nstatistical techniques (Makridakis et al., 2018b).\nOn the other hand, the M4 competition winner (Smyl, 2020), was based on a hybrid between\nneural residual/attention dilated LSTM stack with a classical Holt-Winters statistical model (Holt,\n1957; 2004; Winters, 1960) with learnable parameters. Since Smyl\u2019s approach heavily depends on\nthis Holt-Winters component, Makridakis et al. (2018b) further argue that \u201chybrid approaches and\ncombinations of method are the way forward for improving the forecasting accuracy and making\nforecasting more valuable\u201d. In this work we aspire to challenge this conclusion by exploring the\npotential of pure DL architectures in the context of the TS forecasting. Moreover, in the context of\ninterpretable DL architecture design, we are interested in answering the following question: can we\n1arXiv:1905.10437v4  [cs.LG]  20 Feb 2020", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3541, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e29e45bd-fbc9-4f16-9667-b86a917e4e69": {"__data__": {"id_": "e29e45bd-fbc9-4f16-9667-b86a917e4e69", "embedding": null, "metadata": {"page_label": "2", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section discusses the development and evaluation of a deep learning (DL) architecture for time series (TS) forecasting, known as N-BEATS. Key topics and entities include:\n\n1. **Deep Neural Architecture**:\n   - The paper claims to be the first to empirically demonstrate that a pure DL model, without time-series-specific components, outperforms traditional statistical methods on datasets like M3, M4, and TOURISM.\n   - The DL model showed significant improvements over statistical benchmarks and competition winners, providing a proof of concept for using pure ML in TS forecasting.\n\n2. **Interpretable DL for Time Series**:\n   - The architecture is designed to produce interpretable outputs, similar to traditional decomposition techniques like the \"seasonality-trend-level\" approach.\n\n3. **Problem Statement**:\n   - The focus is on univariate point forecasting in discrete time.\n   - The task involves predicting future values based on a given observed series history.\n   - Common metrics for evaluating forecasting performance are discussed, including sMAPE, MAPE, MASE, and OWA.\n\n4. **N-BEATS Architecture**:\n   - The architecture is designed to be simple, generic, and expressive without relying on time-series-specific feature engineering or input scaling.\n   - The architecture aims to be extendable for human interpretability.\n\n5. **Basic Block**:\n   - The basic building block of the architecture has a fork design.\n   - The operation of the \u2113-th block is detailed, focusing on its input and output vectors.\n\n### Key Entities:\n- **Datasets**: M3, M4, TOURISM\n- **Metrics**: sMAPE, MAPE, MASE, OWA\n- **Techniques**: Seasonality-trend-level decomposition\n- **Architecture**: N-BEATS, basic block design\n\nThe section emphasizes the novelty and effectiveness of the N-BEATS architecture in TS forecasting, highlighting its accuracy and interpretability."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_1", "node_type": "4", "metadata": {"page_label": "2", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "a559f1833378c6c300d0c754cfce6e48b7674323663d863693cfcc1da42c3732", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "002bfe86-9bd0-4104-b8fd-a707a74ac352", "node_type": "1", "metadata": {}, "hash": "3024bd7ac9011e47bd786a1ab6ca02a88c33cb0315e5e31dd7ddfd7d929c5864", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\ninject a suitable inductive bias in the model to make its internal operations more interpretable, in the\nsense of extracting some explainable driving factors combining to produce a given forecast?\n1.1 S UMMARY OF CONTRIBUTIONS\nDeep Neural Architecture: To the best of our knowledge, this is the \ufb01rst work to empirically\ndemonstrate that pure DL using no time-series speci\ufb01c components outperforms well-established\nstatistical approaches on M3, M4 and TOURISM datasets (on M4, by 11% over statistical benchmark,\nby 7% over the best statistical entry, and by 3% over the M4 competition winner). In our view, this\nprovides a long-missing proof of concept for the use of pure ML in TS forecasting and strengthens\nmotivation to continue advancing the research in this area.\nInterpretable DL for Time Series: In addition to accuracy bene\ufb01ts, we also show that it is fea-\nsible to design an architecture with interpretable outputs that can be used by practitioners in very\nmuch the same way as traditional decomposition techniques such as the \u201cseasonality-trend-level\u201d\napproach (Cleveland et al., 1990).\n2 P ROBLEM STATEMENT\nWe consider the univariate point forecasting problem in discrete time. Given a length- Hforecast\nhorizon a length- Tobserved series history [y1,..., yT]\u2208RT, the task is to predict the vector of\nfuture values y\u2208RH= [yT+1,yT+2,..., yT+H]. For simplicity, we will later consider a lookback\nwindow of length t\u2264Tending with the last observed value yTto serve as model input, and denoted\nx\u2208Rt= [yT\u2212t+1,..., yT]. We denote\u02c6ythe forecast of y. The following metrics are commonly\nused to evaluate forecasting performance (Hyndman & Koehler, 2006; Makridakis & Hibon, 2000;\nMakridakis et al., 2018b; Athanasopoulos et al., 2011):\nsMAPE =200\nHH\n\u2211\ni=1|yT+i\u2212\u02c6yT+i|\n|yT+i|+|\u02c6yT+i|, MAPE =100\nHH\n\u2211\ni=1|yT+i\u2212\u02c6yT+i|\n|yT+i|,\nMASE =1\nHH\n\u2211\ni=1|yT+i\u2212\u02c6yT+i|\n1\nT+H\u2212m\u2211T+H\nj=m+1|yj\u2212yj\u2212m|, OWA=1\n2[sMAPE\nsMAPE Na\u00efve2+MASE\nMASE Na\u00efve2]\n.\nHere mis the periodicity of the data ( e.g., 12 for monthly series). MAPE (Mean Absolute Percentage\nError), sMAPE (symmetric MAPE ) and MASE (Mean Absolute Scaled Error) are standard scale-free\nmetrics in the practice of forecasting (Hyndman & Koehler, 2006; Makridakis & Hibon, 2000):\nwhereas sMAPE scales the error by the average between the forecast and ground truth, the MASE\nscales by the average error of the na\u00efve predictor that simply copies the observation measured m\nperiods in the past, thereby accounting for seasonality. OWA (overall weighted average) is a M4-\nspeci\ufb01c metric used to rank competition entries (M4 Team, 2018b), where sMAPE and MASE metrics\nare normalized such that a seasonally-adjusted na\u00efve forecast obtains OWA=1.0.\n3 N-BEATS\nOur architecture design methodology relies on a few key principles. First, the base architecture\nshould be simple and generic, yet expressive (deep). Second, the architecture should not rely on time-\nseries-speci\ufb01c feature engineering or input scaling. These prerequisites let us explore the potential\nof pure DL architecture in TS forecasting. Finally, as a prerequisite to explore interpretability, the\narchitecture should be extendable towards making its outputs human interpretable. We now discuss\nhow those principles converge to the proposed architecture.\n3.1 B ASIC BLOCK\nThe proposed basic building block has a fork architecture and is depicted in Fig. 1 (left). We focus on\ndescribing the operation of \u2113-th block in this section in detail (note that the block index \u2113is dropped\nin Fig. 1 for brevity). The \u2113-th block accepts its respective input x\u2113and outputs two vectors,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3613, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "002bfe86-9bd0-4104-b8fd-a707a74ac352": {"__data__": {"id_": "002bfe86-9bd0-4104-b8fd-a707a74ac352", "embedding": null, "metadata": {"page_label": "2", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary:\nThe section discusses the structure and functionality of the \u2113-th block in the N-BEATS model. Each block takes an input \\( x_\u2113 \\) and produces two output vectors, \\( \\hat{x}_\u2113 \\) and \\( \\hat{y}_\u2113 \\). For the initial block, the input \\( x_\u2113 \\) is the overall model input, which consists of a history lookback window ending with the most recent observation. The length of this lookback window is also mentioned but not specified in the provided text. Key topics include the input-output mechanism of the blocks and the role of the history lookback window in the model."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_1", "node_type": "4", "metadata": {"page_label": "2", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "a559f1833378c6c300d0c754cfce6e48b7674323663d863693cfcc1da42c3732", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e29e45bd-fbc9-4f16-9667-b86a917e4e69", "node_type": "1", "metadata": {"page_label": "2", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "b80b2d6222c52e63536edd56e26b077bdfe4cd0e8d0cb1e7981a131cbbc7a95f", "class_name": "RelatedNodeInfo"}}, "text": "The \u2113-th block accepts its respective input x\u2113and outputs two vectors, \u02c6x\u2113and\n\u02c6y\u2113. For the very \ufb01rst block in the model, its respective x\u2113is the overall model input \u2014 a history\nlookback window of certain length ending with the last measured observation. We set the length of\n2", "mimetype": "text/plain", "start_char_idx": 3543, "end_char_idx": 3819, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9d7d0a2-44a4-4da9-b3aa-5511ef2482b5": {"__data__": {"id_": "a9d7d0a2-44a4-4da9-b3aa-5511ef2482b5", "embedding": null, "metadata": {"page_label": "3", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section describes the architecture of a proposed neural network model for time series forecasting, which was published as a conference paper at ICLR 2020. The key components and concepts include:\n\n1. **Basic Building Block**: A multi-layer fully connected (FC) network with ReLU nonlinearities. Each block predicts basis expansion coefficients for both forward (forecast) and backward (backcast) directions.\n\n2. **Stack Organization**: Blocks are organized into stacks using a doubly residual stacking principle, allowing for the construction of a deep neural network with interpretable outputs. Each stack aggregates forecasts hierarchically.\n\n3. **Input and Output**: The model input is a lookback window, and the output is a forecast period. The input window is typically a multiple of the forecast horizon \\( H \\), ranging from \\( 2H \\) to \\( 7H \\).\n\n4. **Block Operation**: Each block has two outputs: a forward forecast (\\( \\hat{y}_\\ell \\)) and a backcast (\\( \\hat{x}_\\ell \\)). The block consists of two parts:\n   - **Fully Connected Network**: Produces forward (\\( \\theta_f \\)) and backward (\\( \\theta_b \\)) expansion coefficients.\n   - **Basis Layers**: Maps these coefficients to outputs via basis functions, producing the backcast (\\( \\hat{x}_\\ell \\)) and forecast (\\( \\hat{y}_\\ell \\)).\n\n5. **Equations**: The section provides detailed equations describing the operations within each block, including the use of linear projection layers and ReLU non-linearities.\n\n### Key Entities:\n- **ICLR 2020**: Conference where the paper was published.\n- **FC Stack**: Fully connected layers used in the network.\n- **Forecast and Backcast**: Forward and backward predictions made by the network.\n- **Stack Residual**: Residual connections between stacks.\n- **Lookback Window**: Model input.\n- **Forecast Period**: Model output.\n- **Horizon \\( H \\)**: Forecast horizon.\n- **Expansion Coefficients (\\( \\theta_f \\), \\( \\theta_b \\))**: Coefficients predicted by the network.\n- **Basis Layers (\\( g_f \\), \\( g_b \\))**: Layers that map coefficients to outputs.\n- **ReLU**: Non-linearity used in the fully connected layers.\n\nThis architecture aims to optimize forecasting accuracy by effectively utilizing basis functions and residual connections."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_2", "node_type": "4", "metadata": {"page_label": "3", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "ff41dd932d710e18e0d209f1414401380e4226450a0ce4f8e4b33f735380f71e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nFC Stack\n(4 layers)\nFC FC\n( ) \u0000\u0000\u0000\u0000( ) \u0000\u0000\u0000\u0000\nForecast Backcast\u0000\u0000Block Input\n\u0000\u0000Block 1\nBlock 2\nBlock\u00a0 K\u2013\n\u2013\n\u2013+\nStack residual\n(to next stack)Stack Input\nStack\nforecastStack 1\nStack 2\nStack M+Global forecast\n(model output)Lookback window\n(model input)Forecast Period\nHorizon HLookback\u00a0Period\nHorizon nH (here n=3)\nFigure 1: Proposed architecture. The basic building block is a multi-layer FC network with RELU\nnonlinearities. It predicts basis expansion coef\ufb01cients both forward, \u03b8f, (forecast) and backward, \u03b8b,\n(backcast). Blocks are organized into stacks using doubly residual stacking principle. A stack may\nhave layers with shared gbandgf. Forecasts are aggregated in hierarchical fashion. This enables\nbuilding a very deep neural network with interpretable outputs.\ninput window to a multiple of the forecast horizon H, and typical lengths of xin our setup range from\n2Hto7H. For the rest of the blocks, their inputs x\u2113are residual outputs of the previous blocks. Each\nblock has two outputs: \u02c6y\u2113, the block\u2019s forward forecast of length H; and\u02c6x\u2113, the block\u2019s best estimate\nofx\u2113, also known as the \u2018backcast\u2019, given the constraints on the functional space that the block can\nuse to approximate signals.\nInternally, the basic building block consists of two parts. The \ufb01rst part is a fully connected network\nthat produces the forward \u03b8f\n\u2113and the backward \u03b8b\n\u2113predictors of expansion coef\ufb01cients (again, note\nthat the block index \u2113is dropped for \u03b8b\n\u2113,\u03b8f\n\u2113,gb\n\u2113,gf\n\u2113in Fig. 1 for brevity). The second part consists of\nthe backward gb\n\u2113and the forward gf\n\u2113basis layers that accept the respective forward \u03b8f\n\u2113and backward\n\u03b8b\n\u2113expansion coef\ufb01cients, project them internally on the set of basis functions and produce the\nbackcast\u02c6x\u2113and the forecast outputs \u02c6y\u2113de\ufb01ned in the previous paragraph.\nThe operation of the \ufb01rst part of the \u2113-th block is described by the following equations:\nh\u2113,1=FC\u2113,1(x\u2113),h\u2113,2=FC\u2113,2(h\u2113,1),h\u2113,3=FC\u2113,3(h\u2113,2),h\u2113,4=FC\u2113,4(h\u2113,3).\n\u03b8b\n\u2113=LINEARb\n\u2113(h\u2113,4),\u03b8f\n\u2113=LINEARf\n\u2113(h\u2113,4).(1)\nHere LINEAR layer is simply a linear projection layer, i.e.\u03b8f\n\u2113=Wf\n\u2113h\u2113,4. The FClayer is a standard\nfully connected layer with RELUnon-linearity (Nair & Hinton, 2010), such that for FC\u2113,1we have,\nfor example: h\u2113,1=RELU(W\u2113,1x\u2113+b\u2113,1). One task of this part of the architecture is to predict the\nforward expansion coef\ufb01cients \u03b8f\n\u2113with the ultimate goal of optimizing the accuracy of the partial\nforecast\u02c6y\u2113by properly mixing the basis vectors supplied by gf\n\u2113. Additionally, this sub-network\npredicts backward expansion coef\ufb01cients \u03b8b\n\u2113used by gb\n\u2113to produce an estimate of x\u2113with the ultimate\ngoal of helping the downstream blocks by removing components of their input that are not helpful for\nforecasting.\nThe second part of the network maps expansion coef\ufb01cients \u03b8f\n\u2113and\u03b8b\n\u2113to outputs via basis layers,\n\u02c6y\u2113=gf\n\u2113(\u03b8f\n\u2113)and\u02c6x\u2113=gb\n\u2113(\u03b8b\n\u2113). Its operation is described by the following equations:\n\u02c6y\u2113=dim(\u03b8f\n\u2113)\n\u2211\ni=1\u03b8f\n\u2113,ivf\ni,\u02c6x\u2113=dim(\u03b8b\n\u2113)\n\u2211\ni=1\u03b8b\n\u2113,ivb\ni.\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2987, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9d41cf9-2096-49da-99b2-3d2b025213b9": {"__data__": {"id_": "c9d41cf9-2096-49da-99b2-3d2b025213b9", "embedding": null, "metadata": {"page_label": "4", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section discusses the architecture and interpretability of the N-BEATS model, a neural network for time series forecasting, presented at ICLR 2020. Key topics and entities include:\n\n1. **Forecast and Backcast Basis Vectors**:\n   - **vf** and **vb**: Basis vectors for forecast and backcast.\n   - **\u03b8f\u2113,i**: Expansion coefficients for these vectors.\n   - Functions **gb\u2113** and **gf\u2113**: Provide rich sets of basis vectors, which can be learnable or set to specific functional forms to reflect problem-specific inductive biases.\n\n2. **Doubly Residual Stacking**:\n   - Inspired by classical residual networks and DenseNet architectures.\n   - Introduces a novel hierarchical doubly residual topology with two residual branches: one for backcast prediction and one for forecast prediction.\n   - Equations describing the operation:\n     - \\( x\u2113 = x\u2113\u22121 \u2212 \u02c6x\u2113\u22121 \\)\n     - \\( \u02c6y = \u2211 \u2113 \u02c6y\u2113 \\)\n   - Facilitates gradient backpropagation and hierarchical decomposition of forecasts.\n\n3. **Interpretability**:\n   - Two configurations: generic deep learning (DL) and interpretable with inductive biases.\n   - **Generic DL**: Uses linear projections for **gb\u2113** and **gf\u2113**.\n     - Outputs described as:\n       - \\( \u02c6y\u2113 = Vf\u2113\u03b8f\u2113 + bf\u2113 \\)\n       - \\( \u02c6x\u2113 = Vb\u2113\u03b8b\u2113 + bb\u2113 \\)\n     - **Vf\u2113**: Matrix with dimensions H\u00d7dim(\u03b8f\u2113), interpreted as waveforms in the time domain.\n\nThe section emphasizes the model's ability to decompose forecasts hierarchically and its potential for interpretability through structured configurations."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_3", "node_type": "4", "metadata": {"page_label": "4", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "c21665b8cf898ff06c84f7535a2cf5963af7a54effb78f98112630d0d725213c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7de37615-f700-400a-8d29-5cb9dd7ae8ab", "node_type": "1", "metadata": {}, "hash": "901c0aee7b8ddbcb6afc744baa37f2f2c57a45ccc810501a46312ad112c39fe9", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nHere vf\niandvb\niare forecast and backcast basis vectors, \u03b8f\n\u2113,iis the i-th element of \u03b8f\n\u2113. The function\nofgb\n\u2113andgf\n\u2113is to provide suf\ufb01ciently rich sets {vf\ni}dim(\u03b8f\n\u2113)\ni=1and{vb\ni}dim(\u03b8b\n\u2113)\ni=1such that their respective\noutputs can be represented adequately via varying expansion coef\ufb01cients \u03b8f\n\u2113and\u03b8b\n\u2113. As shown below,\ngb\n\u2113andgf\n\u2113can either be chosen to be learnable or can be set to speci\ufb01c functional forms to re\ufb02ect\ncertain problem-speci\ufb01c inductive biases in order to appropriately constrain the structure of outputs.\nConcrete examples of gb\n\u2113andgf\n\u2113are discussed in Section 3.3.\n3.2 D OUBLY RESIDUAL STACKING\nThe classical residual network architecture adds the input of the stack of layers to its output before\npassing the result to the next stack (He et al., 2016). The DenseNet architecture proposed by Huang\net al. (2017) extends this principle by introducing extra connections from the output of each stack to\nthe input of every other stack that follows it. These approaches provide clear advantages in improving\nthe trainability of deep architectures. Their disadvantage in the context of this work is that they result\nin network structures that are dif\ufb01cult to interpret. We propose a novel hierarchical doubly residual\ntopology depicted in Fig. 1 (middle and right). The proposed architecture has two residual branches,\none running over backcast prediction of each layer and the other one is running over the forecast\nbranch of each layer. Its operation is described by the following equations:\nx\u2113=x\u2113\u22121\u2212\u02c6x\u2113\u22121,\u02c6y=\u2211\n\u2113\u02c6y\u2113.\nAs previously mentioned, in the special case of the very \ufb01rst block, its input is the model level\ninput x,x1\u2261x. For all other blocks, the backcast residual branch x\u2113can be thought of as running a\nsequential analysis of the input signal. Previous block removes the portion of the signal \u02c6x\u2113\u22121that\nit can approximate well, making the forecast job of the downstream blocks easier. This structure\nalso facilitates more \ufb02uid gradient backpropagation. More importantly, each block outputs a partial\nforecast\u02c6y\u2113that is \ufb01rst aggregated at the stack level and then at the overall network level, providing a\nhierarchical decomposition. The \ufb01nal forecast \u02c6yis the sum of all partial forecasts. In a generic model\ncontext, when stacks are allowed to have arbitrary gb\n\u2113andgf\n\u2113for each layer, this makes the network\nmore transparent to gradient \ufb02ows. In a special situation of deliberate structure enforced in gb\n\u2113andgf\n\u2113shared over a stack, explained next, this has the critical importance of enabling interpretability via the\naggregation of meaningful partial forecasts.\n3.3 I NTERPRETABILITY\nWe propose two con\ufb01gurations of the architecture, based on the selection of gb\n\u2113andgf\n\u2113. One of them\nis generic DL, the other one is augmented with certain inductive biases to be interpretable.\nThegeneric architecture does not rely on TS-speci\ufb01c knowledge. We set gb\n\u2113andgf\n\u2113to be a linear\nprojection of the previous layer output. In this case the outputs of block \u2113are described as:\n\u02c6y\u2113=Vf\n\u2113\u03b8f\n\u2113+bf\n\u2113,\u02c6x\u2113=Vb\n\u2113\u03b8b\n\u2113+bb\n\u2113.\nThe interpretation of this model is that the FC layers in the basic building block depicted in Fig. 1 learn\nthe predictive decomposition of the partial forecast \u02c6y\u2113in the basis Vf\n\u2113learned by the network. Matrix\nVf\n\u2113has dimensionality H\u00d7dim(\u03b8f\n\u2113). Therefore, the \ufb01rst dimension of Vf\n\u2113has the interpretation of\ndiscrete time index in the forecast domain. The second dimension of the matrix has the interpretation\nof the indices of the basis functions, with \u03b8f\n\u2113being the expansion coef\ufb01cients for this basis. Thus the\ncolumns of Vf\n\u2113can be thought of as waveforms in the time domain. Because no additional constraints\nare imposed on the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3721, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7de37615-f700-400a-8d29-5cb9dd7ae8ab": {"__data__": {"id_": "7de37615-f700-400a-8d29-5cb9dd7ae8ab", "embedding": null, "metadata": {"page_label": "4", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary:\nThe section discusses the interpretability of waveforms learned by a deep model in the time domain, noting that without additional constraints, these waveforms lack inherent structure and are not interpretable. To address this, the section proposes an interpretable architecture by adding structure to basis layers at the stack level, inspired by common time series decomposition methods like STL (Seasonal-Trend decomposition using Loess) and X13-ARIMA. This approach aims to make the stack outputs more interpretable for forecasting practitioners. Key topics include waveform interpretability, architectural design for interpretability, and time series decomposition methods."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_3", "node_type": "4", "metadata": {"page_label": "4", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "c21665b8cf898ff06c84f7535a2cf5963af7a54effb78f98112630d0d725213c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9d41cf9-2096-49da-99b2-3d2b025213b9", "node_type": "1", "metadata": {"page_label": "4", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "6b6fc554ebae14d46cdfad4449dcbff29aa2db32974fe4f636986e30d1f8ca12", "class_name": "RelatedNodeInfo"}}, "text": "be thought of as waveforms in the time domain. Because no additional constraints\nare imposed on the form of Vf\n\u2113, the waveforms learned by the deep model do not have inherent\nstructure (and none is apparent in our experiments). This leads to \u02c6y\u2113not being interpretable.\nTheinterpretable architecture can be constructed by reusing the overall architectural approach in\nFig. 1 and by adding structure to basis layers at stack level. Forecasting practitioners often use the\ndecomposition of time series into trend and seasonality, such as those performed by the STL(Cleveland\net al., 1990) and X13-ARIMA (U.S. Census Bureau, 2013). We propose to design the trend and\nseasonality decomposition into the model to make the stack outputs more easily interpretable. Note\n4", "mimetype": "text/plain", "start_char_idx": 3622, "end_char_idx": 4386, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23540b6f-b665-4ea7-b47a-16ded4c88f24": {"__data__": {"id_": "23540b6f-b665-4ea7-b47a-16ded4c88f24", "embedding": null, "metadata": {"page_label": "5", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThis section of the paper, published at ICLR 2020, discusses the architecture and methodology of the N-BEATS model, focusing on trend and seasonality modeling, as well as ensembling techniques.\n\n1. **Trend Model**:\n   - **Characteristics**: Trend is typically a monotonic or slowly varying function.\n   - **Modeling Approach**: Constrains the forecast function to be a polynomial of small degree \\( p \\), ensuring it varies slowly across the forecast window.\n   - **Mathematical Representation**: \n     - Partial forecast: \\( \\hat{y}_{s,\\ell} = \\sum_{i=0}^{p} \\theta^f_{s,\\ell,i} t^i \\)\n     - Matrix form: \\( \\hat{y}^{tr}_{s,\\ell} = T \\theta^f_{s,\\ell} \\)\n     - \\( \\theta^f_{s,\\ell} \\) are polynomial coefficients predicted by a fully connected (FC) network.\n\n2. **Seasonality Model**:\n   - **Characteristics**: Seasonality is a regular, cyclical, recurring fluctuation.\n   - **Modeling Approach**: Uses Fourier series to model periodic functions.\n   - **Mathematical Representation**:\n     - Partial forecast: \\( \\hat{y}_{s,\\ell} = \\sum_{i=0}^{\\lfloor H/2-1 \\rfloor} \\theta^f_{s,\\ell,i} \\cos(2\\pi it) + \\theta^f_{s,\\ell,i+\\lfloor H/2 \\rfloor} \\sin(2\\pi it) \\)\n     - Matrix form: \\( \\hat{y}^{seas}_{s,\\ell} = S \\theta^f_{s,\\ell} \\)\n     - \\( \\theta^f_{s,\\ell} \\) are Fourier coefficients predicted by a FC network.\n\n3. **Interpretable Architecture**:\n   - Consists of two stacks: trend stack followed by seasonality stack.\n   - Uses doubly residual stacking and forecast/backcast principle.\n   - Each stack has several blocks with residual connections, sharing non-learnable functions \\( g^b_{s,\\ell} \\) and \\( g^f_{s,\\ell} \\).\n   - Number of blocks is 3 for both trend and seasonality stacks.\n   - Sharing all weights across blocks in a stack improves validation performance.\n\n4. **Ensembling**:\n   - Essential for top performance in competitions like M4.\n   - Found to be a more powerful regularization technique compared to dropout or L2-norm penalty.\n   - Core property: diversity.\n   - Ensemble models are fit on three different metrics: sMAPE, MASE, and MAPE.\n   - Individual models are trained on input windows of different lengths for each horizon \\( H \\).\n\n### Key Entities:\n- **N-BEATS Model**: Neural Basis Expansion Analysis Time Series model.\n- **Trend and Seasonality**: Key components modeled using polynomial and Fourier series respectively.\n- **Ensembling**: Technique used to improve model performance by leveraging diversity.\n- **Metrics**: sMAPE, MASE, MAPE.\n- **Fully Connected (FC) Network**: Used to predict polynomial and Fourier coefficients.\n- **Residual Connections**: Used within stacks to improve model performance."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_4", "node_type": "4", "metadata": {"page_label": "5", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "befa72faa4a7138d8c104d992d7df537236de6f7aee9703ea6ea5ba5125c0b20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b2b3328-37fe-4a99-8584-6a3075514fc3", "node_type": "1", "metadata": {}, "hash": "be95e36facee35dafe46890238f87750505e2e4302cd32e732f5c5f50b440814", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nthat for the generic model the notion of stack was not necessary and the stack level indexing was\nomitted for clarity. Now we will consider both stack level and block level indexing. For example, \u02c6ys,\u2113\nwill denote the partial forecast of block \u2113within stack s.\nTrend model. A typical characteristic of trend is that most of the time it is a monotonic function, or\nat least a slowly varying function. In order to mimic this behaviour we propose to constrain gb\ns,\u2113and\ngf\ns,\u2113to be a polynomial of small degree p, a function slowly varying across forecast window:\n\u02c6ys,\u2113=p\n\u2211\ni=0\u03b8f\ns,\u2113,iti. (2)\nHere time vector t= [0,1,2,..., H\u22122,H\u22121]T/His de\ufb01ned on a discrete grid running from 0 to\n(H\u22121)/H, forecasting Hsteps ahead. Alternatively, the trend forecast in matrix form will then be:\n\u02c6ytr\ns,\u2113=T\u03b8f\ns,\u2113,\nwhere \u03b8f\ns,\u2113are polynomial coef\ufb01cients predicted by a FC network of layer \u2113of stack sdescribed by\nequations (1); and T= [1,t,...,tp]is the matrix of powers of t. Ifpis low, e.g. 2 or 3, it forces \u02c6ytr\ns,\u2113\nto mimic trend.\nSeasonality model. Typical characteristic of seasonality is that it is a regular, cyclical, recurring\n\ufb02uctuation. Therefore, to model seasonality, we propose to constrain gb\ns,\u2113andgf\ns,\u2113to belong to the\nclass of periodic functions, i.e.yt=yt\u2212\u2206, where \u2206is a seasonality period. A natural choice for the\nbasis to model periodic function is the Fourier series:\n\u02c6ys,\u2113=\u230aH/2\u22121\u230b\n\u2211\ni=0\u03b8f\ns,\u2113,icos(2\u03c0it)+\u03b8f\ns,\u2113,i+\u230aH/2\u230bsin(2\u03c0it), (3)\nThe seasonality forecast will then have the matrix form as follows:\n\u02c6yseas\ns,\u2113=S\u03b8f\ns,\u2113,\nwhere \u03b8f\ns,\u2113are Fourier coef\ufb01cients predicted by a FC network of layer \u2113of stack sdescribed by\nequations (1); and S= [1,cos(2\u03c0t),...cos(2\u03c0\u230aH/2\u22121\u230bt)),sin(2\u03c0t),..., sin(2\u03c0\u230aH/2\u22121\u230bt))]is the\nmatrix of sinusoidal waveforms. The forecast \u02c6yseas\ns,\u2113is then a periodic function mimicking typical\nseasonal patterns.\nThe overall interpretable architecture consists of two stacks: the trend stack is followed by the\nseasonality stack. The doubly residual stacking combined with the forecast/backcast principle result\nin (i) the trend component being removed from the input window xbefore it is fed into the seasonality\nstack and (ii) the partial forecasts of trend and seasonality are available as separate interpretable\noutputs. Structurally, each of the stacks consists of several blocks connected with residual connections\nas depicted in Fig. 1 and each of them shares its respective, non-learnable gb\ns,\u2113andgf\ns,\u2113. The number\nof blocks is 3 for both trend and seasonality. We found that on top of sharing gb\ns,\u2113andgf\ns,\u2113, sharing all\nthe weights across blocks in a stack resulted in better validation performance.\n3.4 E NSEMBLING\nEnsembling is used by all the top entries in the M4-competition. We rely on ensembling as well\nto be comparable. We found that ensembling is a much more powerful regularization technique\nthan the popular alternatives, e.g. dropout or L2-norm penalty. The addition of those methods\nimproved individual models, but was hurting the performance of the ensemble. The core property of\nan ensemble is diversity. We build an ensemble using several sources of diversity. First, the ensemble\nmodels are \ufb01t on three different metrics: sMAPE,MASE and MAPE , a version of sMAPE that has only\nthe ground truth value in the denominator. Second, for every horizon H, individual models are trained\non input windows of different length:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3415, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b2b3328-37fe-4a99-8584-6a3075514fc3": {"__data__": {"id_": "5b2b3328-37fe-4a99-8584-6a3075514fc3", "embedding": null, "metadata": {"page_label": "5", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary: The section discusses the methodology used for training and evaluating models in a forecasting ensemble. Key topics include the use of varying input window lengths (2H, 3H, ..., 7H) for different horizons (H), creating a multi-scale ensemble. Additionally, a bagging procedure is employed by training models with different random initializations. A total of 180 models are used to report test set results, with the median serving as the ensemble aggregation function. The section also references Appendix B for details on the ablation of ensemble size. Key entities include input windows, horizons, multi-scale ensemble, bagging procedure, random initializations, and ensemble aggregation function."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_4", "node_type": "4", "metadata": {"page_label": "5", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "befa72faa4a7138d8c104d992d7df537236de6f7aee9703ea6ea5ba5125c0b20", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23540b6f-b665-4ea7-b47a-16ded4c88f24", "node_type": "1", "metadata": {"page_label": "5", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "38c406e60c9556607602c56db346b42975cd284b3e4a0165ae48fdef7871a3d8", "class_name": "RelatedNodeInfo"}}, "text": "Second, for every horizon H, individual models are trained\non input windows of different length: 2H,3H,..., 7H, for a total of six window lengths. Thus the\noverall ensemble exhibits a multi-scale aspect. Finally, we perform a bagging procedure (Breiman,\n1996) by including models trained with different random initializations. We use 180 total models to\nreport results on the test set (please refer to Appendix B for the ablation of ensemble size). We use\nthe median as ensemble aggregation function.\n5", "mimetype": "text/plain", "start_char_idx": 3319, "end_char_idx": 3821, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13695662-913c-4328-b8eb-b715c0b9d841": {"__data__": {"id_": "13695662-913c-4328-b8eb-b715c0b9d841", "embedding": null, "metadata": {"page_label": "6", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section primarily discusses the performance of various time series (TS) forecasting models on three datasets: M4, M3, and TOURISM. Key topics and entities include:\n\n1. **Performance Metrics and Datasets**:\n   - **Datasets**: M4 (100,000 time series), M3 (3,003 time series), and TOURISM (1,311 time series).\n   - **Evaluation Metrics**: sMAPE, OWA for M4; sMAPE for M3; MAPE for TOURISM.\n   - **Performance Table**: Shows the performance of different models on these datasets, with lower values indicating better performance.\n\n2. **Models Compared**:\n   - **Pure ML**: Machine Learning models.\n   - **Statistical**: Traditional statistical models like ETS, Theta.\n   - **ProLogistica**: A specific statistical model.\n   - **ML/TS Combination**: Hybrid models combining machine learning and time series methods.\n   - **DL/TS Hybrid**: Deep learning models combined with time series methods.\n   - **N-BEATS**: Variants of the N-BEATS model (N-BEATS-G, N-BEATS-I, N-BEATS-I+G) which show superior performance.\n\n3. **Related Work**:\n   - **Statistical Approaches**: Exponential smoothing, Theta method, ARIMA, auto-ARIMA.\n   - **ML/TS Combination Approaches**: Use outputs of statistical engines as features, including top entries in the M4 competition.\n   - **Deep Learning Approaches**: Variations of recurrent neural networks (RNNs), including combinations with dilation, residual connections, and attention mechanisms.\n   - **Hybrid Models**: The winning entry of the M4 competition, which combines Holt-Winters style seasonality model with deep learning techniques.\n\n4. **Experimental Results**:\n   - **Datasets**: Detailed descriptions of M4, M3, and TOURISM datasets.\n   - **Comparison**: Results compared with the best entries reported in the literature for each dataset using customary metrics.\n\n### Key Entities:\n- **Datasets**: M4, M3, TOURISM.\n- **Models**: Pure ML, Statistical, ProLogistica, ML/TS Combination, DL/TS Hybrid, N-BEATS (N-BEATS-G, N-BEATS-I, N-BEATS-I+G).\n- **Metrics**: sMAPE, OWA, MAPE.\n- **Competitions**: M4 competition, M3 competition.\n- **Techniques**: Exponential smoothing, Theta method, ARIMA, auto-ARIMA, recurrent neural networks (RNNs), gradient boosted tree, Holt-Winters style seasonality model, dilation, residual connections, attention mechanisms."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "587e647838e18c166832c9dd42399014ac1d4315eb1e57de576a21068d7f8d03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c928c3fe-77b7-4389-a7c7-5fb90715bd44", "node_type": "1", "metadata": {}, "hash": "575398f6ec3de37646cff265ce8ce429452fdb770d4fab8931ebc7ebd1018ee5", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nTable 1: Performance on the M4, M3, TOURISM test sets, aggregated over each dataset. Evaluation\nmetrics are speci\ufb01ed for each dataset; lower values are better. The number of time series in each\ndataset is provided in brackets.\nM4 Average (100,000) M3 Average (3,003) TOURISM Average (1,311)\nsMAPE OWA sMAPE MAPE\nPure ML 12.894 0.915 Comb S-H-D 13.52 ETS 20.88\nStatistical 11.986 0.861 ForecastPro 13.19 Theta 20.88\nProLogistica 11.845 0.841 Theta 13.01 ForePro 19.84\nML/TS combination 11.720 0.838 DOTM 12.90 Stratometrics 19.52\nDL/TS hybrid 11.374 0.821 EXP 12.71 LeeCBaker 19.35\nN-BEATS-G 11.168 0.797 12.47 18.47\nN-BEATS-I 11.174 0.798 12.43 18.97\nN-BEATS-I+G 11.135 0.795 12.37 18.52\n4 R ELATED WORK\nThe approaches to TS forecasting can be split in a few distinct categories. The statistical model-\ning approaches based on exponential smoothing and its different \ufb02avors are well established and\nare often considered a default choice in the industry (Holt, 1957; 2004; Winters, 1960). More\nadvanced variations of exponential smoothing include the winner of M3 competition, the Theta\nmethod (Assimakopoulos & Nikolopoulos, 2000) that decomposes the forecast into several theta-lines\nand statistically combines them. The pinnacle of the statistical approach encapsulates ARIMA,\nauto-ARIMA and in general, the uni\ufb01ed state-space modeling approach, that can be used to ex-\nplain and analyze all of the approaches mentioned above (see Hyndman & Khandakar (2008) for\nan overview). More recently, ML/TS combination approaches started in\ufb01ltrating the domain with\ngreat success, showing promising results by using the outputs of statistical engines as features. In\nfact, 2 out of top-5 entries in the M4 competition are approaches of this type, including the second\nentry (Montero-Manso et al., 2019). The second entry computes the outputs of several statistical\nmethods on the M4 dataset and combines them using gradient boosted tree (Chen & Guestrin, 2016).\nSomewhat independently, the work in the modern deep learning TS forecasting developed based on\nvariations of recurrent neural networks (Flunkert et al., 2017; Rangapuram et al., 2018b; Toubeau\net al., 2019; Zia & Razzaq, 2018) being largely dominated by the electricity load forecasting in the\nmulti-variate setup. A few earlier works explored the combinations of recurrent neural networks\nwith dilation, residual connections and attention (Chang et al., 2017; Kim et al., 2017; Qin et al.,\n2017). These served as a basis for the winner of the M4 competition (Smyl, 2020). The winning\nentry combines a Holt-Winters style seasonality model with its parameters \ufb01tted to a given TS via\ngradient descent and a unique combination of dilation/residual/attention approaches for each forecast\nhorizon. The resulting model is a hybrid model that architecturally heavily relies on a time-series\nengine. It is hand crafted to each speci\ufb01c horizon of M4, making this approach hard to generalize to\nother datasets.\n5 E XPERIMENTAL RESULTS\nOur key empirical results based on aggregate performance metrics over several datasets\u2014M4 (M4\nTeam, 2018b; Makridakis et al., 2018b), M3 (Makridakis & Hibon, 2000; Makridakis et al., 2018a)\nand TOURISM (Athanasopoulos et al., 2011)\u2014appear in Table 1. More detailed descriptions of the\ndatasets are provided in Section 5.1 and Appendix A. For each dataset, we compare our results with\nbest 5 entries for this dataset reported in the literature, according to the customary metrics speci\ufb01c to\neach dataset (M4: OWA andsMAPE , M3: sMAPE ,TOURISM :MAPE ). More granular dataset-speci\ufb01c\nresults with data", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3623, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c928c3fe-77b7-4389-a7c7-5fb90715bd44": {"__data__": {"id_": "c928c3fe-77b7-4389-a7c7-5fb90715bd44", "embedding": null, "metadata": {"page_label": "6", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary:\nThe section discusses the performance evaluation of two configurations of the N-BEATS model: generic (N-BEATS-G) and interpretable (N-BEATS-I), as well as an ensemble model combining both configurations (N-BEATS-I+G). The evaluation is conducted on the M4 dataset, with comparisons made against five representative models from the M4 competition. The section also mentions that more detailed, dataset-specific results, including data splits over forecast horizons and types of time series, can be found in the respective appendices (M4: Appendix C.1; M3: Appendix C.2; TOURISM: Appendix C.3)."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "587e647838e18c166832c9dd42399014ac1d4315eb1e57de576a21068d7f8d03", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13695662-913c-4328-b8eb-b715c0b9d841", "node_type": "1", "metadata": {"page_label": "6", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "c37d3e4a2741082af3e3a43018d8e80f674d96aa67a39b97efcb6144ae457086", "class_name": "RelatedNodeInfo"}}, "text": ":MAPE ). More granular dataset-speci\ufb01c\nresults with data splits over forecast horizons and types of time series appear in respective appendices\n(M4: Appendix C.1; M3: Appendix C.2; TOURISM : Appendix C.3).\nIn Table 1, we study the performance of two N-BEATS con\ufb01gurations: generic (N-BEATS-G) and\ninterpretable (N-BEATS-I), as well as N-BEATS-I+G (ensemble of all models from N-BEATS-G and\nN-BEATS-I). On M4 dataset , we compare against 5 representatives from the M4 competition (Makri-\n6", "mimetype": "text/plain", "start_char_idx": 3567, "end_char_idx": 4055, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f60ab8c-bcd7-4414-8c1e-d46f7f9f69a7": {"__data__": {"id_": "6f60ab8c-bcd7-4414-8c1e-d46f7f9f69a7", "embedding": null, "metadata": {"page_label": "7", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section discusses the performance of the N-BEATS model in time series forecasting, comparing it against various benchmarks and datasets. Key topics and entities include:\n\n1. **Benchmark Models and Competitions**:\n   - **M4 Competition**: N-BEATS is compared against several models, including the M4 winner (DL/TS hybrid by Smyl, 2020), Pure ML (B. Trotta), Statistical (N.Z. Legaki and K. Koutsouri), ML/TS combination (P. Montero-Manso et al.), and ProLogistica.\n   - **M3 Dataset**: Comparisons include the Theta method (Assimakopoulos & Nikolopoulos, 2000), DOTA (Fiorucci et al., 2016), EXP (Spiliotis et al., 2019), and ForecastPro (Athanasopoulos et al., 2011).\n   - **TOURISM Dataset**: Benchmarks include ETS, Theta method, ForePro, and top entries from the TOURISM Kaggle competition (Stratometrics and LeeCBaker).\n\n2. **Performance and Results**:\n   - N-BEATS demonstrates state-of-the-art performance across three datasets (M4, M3, TOURISM), outperforming both generic and manually crafted models.\n   - The model uses minimal prior knowledge, no feature engineering, scaling, or TS-specific internal components, indicating that deep learning can perform well without statistical approaches or domain-specific feature engineering.\n\n3. **Datasets**:\n   - **M4 Dataset**: Contains 100k time series from various domains with different sampling frequencies and seasonalities.\n   - **M3 Dataset**: Comprises 3003 time series, historically significant for the development of optimal statistical models.\n   - **TOURISM Dataset**: Released as part of a Kaggle competition, used for benchmarking in the study.\n\n4. **Conclusion**:\n   - The results suggest that deep learning models like N-BEATS do not require support from statistical methods or hand-crafted features to achieve high performance in time series forecasting tasks.\n\n### Key Entities:\n- **N-BEATS Model**\n- **M4 Competition**\n- **M3 Dataset**\n- **TOURISM Dataset**\n- **Benchmark Models**: DL/TS hybrid, Pure ML, Statistical, ML/TS combination, ProLogistica, Theta method, DOTA, EXP, ForecastPro, ETS, ForePro, Stratometrics, LeeCBaker.\n- **Researchers and Contributors**: B. Trotta, N.Z. Legaki, K. Koutsouri, P. Montero-Manso, T. Talagala, R.J. Hyndman, G. Athanasopoulos, Smyl, Assimakopoulos, Nikolopoulos, Fiorucci, Spiliotis, Baker, Howard, Makridakis, Hibon."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "419eecb2bfa4977cb5800751f21c3381dc497bd20a7a1e86701227f7cb5b252b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4f1517d-d166-41f4-914f-6f786a41562c", "node_type": "1", "metadata": {}, "hash": "369c091f2fdfaf3605688af611f5ee124dd0a42e15fd9dafa263fb8050cfb059", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\ndakis et al., 2018b): each best in their respective model class. Pure ML is the submission by B. Trotta,\nthe best entry among the 6 pure ML models. Statistical is the best pure statistical model by N.Z.\nLegaki and K. Koutsouri. ML/TS combination is the model by P. Montero-Manso, T. Talagala, R.J.\nHyndman and G. Athanasopoulos, second best entry, gradient boosted tree over a few statistical time\nseries models. ProLogistica is the third entry in M4 based on the weighted ensemble of statistical\nmethods. Finally, DL/TS hybrid is the winner of M4 competition (Smyl, 2020). On the M3 dataset ,\nwe compare against the Theta method (Assimakopoulos & Nikolopoulos, 2000), the winner of M3;\nDOTA , a dynamically optimized Theta model (Fiorucci et al., 2016); EXP, the most resent statistical\napproach and the previous state-of-the-art on M3 (Spiliotis et al., 2019); as well as ForecastPro , an\noff-the-shelf forecasting software that is based on model selection between exponential smoothing,\nARIMA and moving average (Athanasopoulos et al., 2011; Assimakopoulos & Nikolopoulos, 2000).\nOn the TOURISM dataset , we compare against 3 statistical benchmarks (Athanasopoulos et al.,\n2011): ETS, exponential smoothing with cross-validated additive/multiplicative model; Theta method;\nForePro , same as ForecastPro in M3; as well as top 2 entries from the TOURISM Kaggle competi-\ntion (Athanasopoulos & Hyndman, 2011): Stratometrics , an unknown technique; LeeCBaker (Baker\n& Howard, 2011), a weighted combination of Na\u00efve, linear trend model, and exponentially weighted\nleast squares regression trend.\nAccording to Table 1, N-BEATS demonstrates state-of-the-art performance on three challenging\nnon-overlapping datasets containing time series from very different domains, sampling frequencies\nand seasonalities. As an example, on M4 dataset, the OWA gap between N-BEATS and the M4\nwinner ( 0.821\u22120.795=0.026) is greater than the gap between the M4 winner and the second entry\n(0.838\u22120.821=0.017). Generic N-BEATS model uses as little prior knowledge as possible, with\nno feature engineering, no scaling and no internal architectural components that may be considered\nTS-speci\ufb01c. Thus the result in Table 1 leads us to the conclusion that DL does not need support\nfrom the statistical approaches or hand-crafted feature engineering and domain knowledge to perform\nextremely well on a wide array of TS forecasting tasks. On top of that, the proposed general\narchitecture performs very well on three different datasets outperforming a wide variety of models,\nboth generic and manually crafted to respective dataset, including the winner of M4, a model\narchitecturally adjusted by hand to each forecast-horizon subset of the M4 data.\n5.1 D ATASETS\nM4(M4 Team, 2018b; Makridakis et al., 2018b) is the latest in an in\ufb02uential series of forecasting\ncompetitions organized by Spyros Makridakis since 1982 (Makridakis et al., 1982). The 100k-series\ndataset is large and diverse, consisting of data frequently encountered in business, \ufb01nancial and\neconomic forecasting, and sampling frequencies ranging from hourly to yearly. A table with summary\nstatistics is presented in Appendix A.1, showing wide variability in TS characteristics.\nM3(Makridakis & Hibon, 2000) is similar in its composition to M4, but has a smaller overall scale\n(3003 time series total vs. 100k in M4). A table with summary statistics is presented in Appendix A.2.\nOver the past 20 years, this dataset has supported signi\ufb01cant efforts in the design of more optimal\nstatistical models, e.g. Theta and its variants (Assimakopoulos & Nikolopoulos, 2000; Fiorucci et al.,\n2016; Spiliotis et al., 2019). Furthermore, a recent publication (Makridakis et al., 2018a) based on a\nsubset of M3 presented evidence that ML models are inferior to the classical statistical models.\nTOURISM (Athanasopoulos et al., 2011) dataset was released as part of the respective Kaggle\ncompetition conducted by Athanasopoulos & Hyndman", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4008, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4f1517d-d166-41f4-914f-6f786a41562c": {"__data__": {"id_": "a4f1517d-d166-41f4-914f-6f786a41562c", "embedding": null, "metadata": {"page_label": "7", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary:\nThe section discusses the datasets and training methodology used in the study. The datasets include monthly, quarterly, and yearly time series data provided by governmental tourism organizations (such as Tourism Australia, the Hong Kong Tourism Board, and Tourism New Zealand) and various academics. These datasets were part of a Kaggle competition conducted by Athanasopoulos & Hyndman (2011). The training methodology involves splitting each dataset into train, validation, and test subsets. The test subset is predefined for each dataset, while the train and validation subsets are created by splitting the full train sets at the boundary of the last horizon of each time series. Hyperparameters are tuned using the train and validation subsets, and once determined, the model is trained on the full train set with results reported on the test set. Detailed hyperparameter settings are provided in Appendix D. Summary statistics of the datasets are available in Appendix A.3."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "419eecb2bfa4977cb5800751f21c3381dc497bd20a7a1e86701227f7cb5b252b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f60ab8c-bcd7-4414-8c1e-d46f7f9f69a7", "node_type": "1", "metadata": {"page_label": "7", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "9cc25f3581365a19897282217232b348c59b653da148f43b427b3043c476d859", "class_name": "RelatedNodeInfo"}}, "text": "was released as part of the respective Kaggle\ncompetition conducted by Athanasopoulos & Hyndman (2011). The data include monthly, quarterly\nand yearly series supplied by both governmental tourism organizations (e.g. Tourism Australia, the\nHong Kong Tourism Board and Tourism New Zealand) as well as various academics, who had used\nthem in previous studies. A table with summary statistics is presented in Appendix A.3.\n5.2 T RAINING METHODOLOGY\nWe split each dataset into train, validation and test subsets. The test subset is the standard test set\npreviously de\ufb01ned for each dataset (M4 Team, 2018a; Makridakis & Hibon, 2000; Athanasopoulos\net al., 2011). The validation and train subsets for each dataset are obtained by splitting their full train\nsets at the boundary of the last horizon of each time series. We use the train and validation subsets to\ntune hyperparameters. Once the hyperparameters are determined, we train the model on the full train\nset and report results on the test set. Please refer to Appendix D for detailed hyperparameter settings\n7", "mimetype": "text/plain", "start_char_idx": 3913, "end_char_idx": 4973, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88868241-866f-4b83-b09b-9af6d31d5ced": {"__data__": {"id_": "88868241-866f-4b83-b09b-9af6d31d5ced", "embedding": null, "metadata": {"page_label": "8", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThis section of the N-BEATS paper, published at ICLR 2020, discusses the implementation, training, and interpretability of the N-BEATS model for time series forecasting. Key topics and entities include:\n\n1. **Implementation and Training**:\n   - N-BEATS is implemented in TensorFlow.\n   - Parameters are shared across different forecasting horizons, allowing one model per horizon for each dataset.\n   - The architecture and hyperparameters (e.g., width, number of layers, number of stacks) are consistent across horizons and datasets.\n   - Training involves sampling batches of size 1024 and using the Adam optimizer with an initial learning rate of 0.001.\n   - Early stopping is used, and training duration varies between 30 minutes to 2 hours depending on settings and hardware.\n\n2. **Generalization**:\n   - The architecture generalizes well across different time series datasets, including the M4 Monthly subset (48k time series) and the M3 Others subset (174 time series).\n   - This generalization is stronger compared to previous models like S. Smyl's, which required different architectures for different horizons.\n\n3. **Interpretability**:\n   - The model can be made interpretable by constraining the first stack to polynomial form and the second stack to Fourier basis form.\n   - Outputs of the interpretable model show distinct trend and seasonality components, with trends being monotonic and seasonality being cyclical.\n   - The magnitude of outputs varies based on the presence of trend or seasonality in the time series.\n   - The interpretable architecture effectively decomposes forecasts into trend and seasonality components without compromising performance.\n\n4. **Training Details**:\n   - The length of historical data (LH) used for training varies based on the number of time series in the dataset.\n   - For large subsets, LH is smaller, while for smaller subsets, LH is larger.\n\n5. **Evaluation**:\n   - The model's performance is evaluated using the sMAPE metric, with gradient flows stopped in the denominator for numerical stability.\n\nOverall, the section highlights the robustness, generalization capability, and interpretability of the N-BEATS model in time series forecasting."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_7", "node_type": "4", "metadata": {"page_label": "8", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "3d1abdd7871338e28986e74aeb8872854e3c4a4525f865f2235b6396099dd266", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17a6f5b5-7bff-4bbd-a974-d6d61c9346a7", "node_type": "1", "metadata": {}, "hash": "1093b66a3f32480d61aa8d03e557feedde749d0629bed86581e6e0a81a88c8c7", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nat the block level. N-BEATS is implemented and trained in Tensor\ufb02ow (Abadi et al., 2015). We\nshare parameters of the network across horizons, therefore we train one model per horizon for each\ndataset. If every time series is interpreted as a separate task, this can be linked back to the multitask\nlearning and furthermore to meta-learning (see discussion in Section 6), in which a neural network\nis regularized by learning on multiple tasks to improve generalization. We would like to stress that\nmodels for different horizons and datasets reuse the same architecture. Architectural hyperparameters\n(width, number of layers, number of stacks, etc.) are \ufb01xed to the same values across horizons and\nacross datasets (see Appendix D). The fact that we can reuse architecture and even hyperparameters\nacross horizons indicates that the proposed architecture design generalizes well across time series of\ndifferent nature. The same architecture is successfully trained on the M4 Monthly subset with 48k\ntime series and the M3 Others subset with 174 time series. This is a much stronger result than e.g.the\nresult of S. Smyl (Makridakis et al., 2018b) who had to use very different architectures hand crafted\nfor different horizons.\nTo update network parameters for one horizon, we sample train batches of \ufb01xed size 1024. We pick\n1024 TS ids from this horizon, uniformly at random with replacement. For each selected TS id we\npick a random forecast point from the historical range of length LHimmediately preceding the last\npoint in the train part of the TS. LHis a cross-validated hyperparameter. We observed that for subsets\nwith large number of time series it tends to be smaller and for subsets with smaller number of time\nseries it tends to be larger. For example, in massive Yearly, Monthly, Quarterly subsets of M4 LHis\nequal to 1.5; and in moderate to small Weekly, Daily, Hourly subsets of M4 LHis equal to 10. Given\na sampled forecast point, we set one horizon worth of points following it to be the target forecast\nwindow yand we set the history of points of one of lengths 2H,3H,..., 7Hpreceding it to be the\ninput xto the network. We use the Adam optimizer with default settings and initial learning rate\n0.001. While optimising the ensemble members relying on the minimization of sMAPE metric, we\nstop the gradient \ufb02ows in the denominator to make training numerically stable. The neural network\ntraining is run with early stopping and the number of batches is determined on the validation set. The\nGPU based training of one ensemble member for entire M4 dataset takes between 30 min and 2 hours\ndepending on neural network settings and hardware.\n5.3 I NTERPRETABILITY RESULTS\nFig. 2 studies the outputs of the proposed model in the generic and the interpretable con\ufb01gurations.\nAs discussed in Section 3.3, to make the generic architecture presented in Fig. 1 interpretable, we\nconstrain g\u03b8in the \ufb01rst stack to have the form of polynomial (2)while the second one has the form\nof Fourier basis (3). Furthermore, we use the outputs of the generic con\ufb01guration of N-BEATS as\ncontrol group (the generic model of 30 residual blocks depicted in Fig. 1 is divided into two stacks)\nand we plot both generic (suf\ufb01x \u201c-G\u201d) and interpretable (suf\ufb01x \u201c-I\u201d) stack outputs side by side in\nFig. 2. The outputs of generic model are arbitrary and non-interpretable: either trend or seasonality\nor both of them are present at the output of both stacks. The magnitude of the output (peak-to-peak)\nis generally smaller at the output of the second stack. The outputs of the interpretable model exhibit\ndistinct properties: the trend output is monotonic and slowly moving, the seasonality output is\nregular, cyclical and has recurring \ufb02uctuations. The peak-to-peak magnitude of the seasonality output\nis signi\ufb01cantly larger than that of the trend, if signi\ufb01cant seasonality is present in the time series.\nSimilarly, the peak-to-peak magnitude of trend output tends to be small when no obvious trend\nis present in the ground truth signal. Thus the proposed interpretable architecture decomposes its\nforecast into two distinct components. Our conclusion is that the outputs of the DL model can be\nmade interpretable by encoding a sensible inductive bias in the architecture. Table 1 con\ufb01rms that\nthis does not result", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4342, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17a6f5b5-7bff-4bbd-a974-d6d61c9346a7": {"__data__": {"id_": "17a6f5b5-7bff-4bbd-a974-d6d61c9346a7", "embedding": null, "metadata": {"page_label": "8", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary:\nThe section discusses the concept of inductive bias in the architecture and its impact on performance, noting that there is no performance drop as confirmed by Table 1. It then delves into the topic of meta-learning, explaining the distinction between inner and outer learning procedures. The inner learning procedure is influenced by the outer learning procedure, with a reference to Bengio et al. (1991). The analogy of individual learning in an animal's lifetime versus the evolution of learning procedures over generations is used to illustrate the two levels of learning. The section also mentions inner parameters, such as synaptic weights, which are modified within the inner learning procedure."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_7", "node_type": "4", "metadata": {"page_label": "8", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "3d1abdd7871338e28986e74aeb8872854e3c4a4525f865f2235b6396099dd266", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88868241-866f-4b83-b09b-9af6d31d5ced", "node_type": "1", "metadata": {"page_label": "8", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "a6b4edbdeca0ca63ab332fcbf97ccfb0d9746dd1e509903c92e6155282470bc0", "class_name": "RelatedNodeInfo"}}, "text": "inductive bias in the architecture. Table 1 con\ufb01rms that\nthis does not result in performance drop.\n6 D ISCUSSION : CONNECTIONS TO META-LEARNING\nMeta-learning de\ufb01nes an inner learning procedure and an outer learning procedure . The inner\nlearning procedure is parameterized, conditioned or otherwise in\ufb02uenced by the outer learning\nprocedure (Bengio et al., 1991). The prototypical inner vs. outer learning is individual learning in\nthe lifetime of an animal vs. evolution of the inner learning procedure itself over many generations\nof individuals. To see the two levels, it often helps to refer to two sets of parameters, the inner\nparameters (e.g. synaptic weights) which are modi\ufb01ed inside the inner learning procedure, and the\n8", "mimetype": "text/plain", "start_char_idx": 4265, "end_char_idx": 4997, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6e46701-f118-4e1e-8c0b-6ff2db986182": {"__data__": {"id_": "c6e46701-f118-4e1e-8c0b-6ff2db986182", "embedding": null, "metadata": {"page_label": "9", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section presents a series of graphs illustrating the performance of generic and interpretable configurations of a forecasting model on the M4 dataset. The key topics and entities include:\n\n1. **Forecasting Models**: The section compares two types of forecasting models\u2014generic (FORECAST-G) and interpretable (FORECAST-I).\n2. **Data Frequencies**: The models are evaluated across various data frequencies, including yearly, quarterly, monthly, weekly, daily, and hourly time series.\n3. **Stacked Models**: The outputs of different stacked models (STACK1-G, STACK2-G, STACK1-I, STACK2-I) are shown for each data frequency.\n4. **Normalization**: The magnitudes in each row are normalized by the maximal value of the actual time series for convenience.\n5. **Visualization**: The graphs in column (a) display the actual values, the generic model forecast, and the interpretable model forecast. Subsequent columns show the outputs of the stacked models.\n\nThe section aims to provide a visual comparison of the forecasting accuracy and behavior of the different model configurations across various time series data frequencies."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_8", "node_type": "4", "metadata": {"page_label": "9", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "d9f47862c5663d4917bc29bdd803bd5c015325dae160f6bb754b8b6515dde4e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56532a7f-ba08-4b2e-a2b0-fa53bc555c9b", "node_type": "1", "metadata": {}, "hash": "189363402098c421676de3ac2d10f0b523a75d9a1e76eea6049a9fafb614f131", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\n0 1 2 3 4 5\nt0.80.91.0\nACTUAL\nFORECAST-I\nFORECAST-G\n0 1 2 3 4 5\nt0.800.850.90STACK1-G\n0 1 2 3 4 5\nt0.0250.0500.075STACK2-G\n0 1 2 3 4 5\nt0.800.850.900.95\nSTACK1-I\n0 1 2 3 4 5\nt0.020.030.040.05STACK2-I\n0 2 4 6\nt0.850.900.951.00\nACTUAL\nFORECAST-I\nFORECAST-G\n0 2 4 6\nt0.860.880.90STACK1-G\n0 2 4 6\nt0.025\n0.0000.0250.050\nSTACK2-G\n0 2 4 6\nt0.880.890.90STACK1-I\n0 2 4 6\nt0.05\n0.000.05STACK2-I\n0 5 10 15\nt0.40.60.81.0\nACTUAL\nFORECAST-I\nFORECAST-G\n0 5 10 15\nt0.80.9 STACK1-G\n0 5 10 15\nt0.1\n0.0\nSTACK2-G\n0 5 10 15\nt0.850.90STACK1-I\n0 5 10 15\nt0.3\n0.2\n0.1\n0.0STACK2-I\n0 2 4 6 8 10 12\nt0.60.81.0\nACTUAL\nFORECAST-I\nFORECAST-G\n0 2 4 6 8 10 12\nt0.650.700.750.80\nSTACK1-G\n0 2 4 6 8 10 12\nt0.0000.0250.0500.075\nSTACK2-G\n0 2 4 6 8 10 12\nt0.650.700.750.80STACK1-I\n0 2 4 6 8 10 12\nt0.000.020.04STACK2-I\n0.0 2.5 5.0 7.5 10.0 12.5\nt0.960.981.00\nACTUAL\nFORECAST-I\nFORECAST-G\n0.0 2.5 5.0 7.5 10.0 12.5\nt0.9740.976STACK1-G\n0.0 2.5 5.0 7.5 10.0 12.5\nt0.002\n0.001\nSTACK2-G\n0.0 2.5 5.0 7.5 10.0 12.5\nt0.9740.976STACK1-I\n0.0 2.5 5.0 7.5 10.0 12.5\nt0.0003\n0.0002\n0.0001\nSTACK2-I\n0 10 20 30 40\nt0.250.500.751.00\nACTUAL\nFORECAST-I\nFORECAST-G\n(a) Combined\n0 10 20 30 40\nt0.20.40.6\nSTACK1-G (b) Stack1-G\n0 10 20 30 40\nt0.02\n0.00\nSTACK2-G (c) Stack2-G\n0 10 20 30 40\nt0.360.380.40\nSTACK1-I (d) StackT-I\n0 10 20 30 40\nt0.2\n0.00.2\nSTACK2-I (e) StackS-I\nFigure 2: The outputs of generic and the interpretable con\ufb01gurations, M4 dataset. Each row is one\ntime series example per data frequency, top to bottom (Yearly: id Y3974, Quarterly: id Q11588,\nMonthly: id M19006, Weekly: id W246, Daily: id D404, Hourly: id H344). The magnitudes in a row\nare normalized by the maximal value of the actual time series for convenience. Column (a) shows the\nactual values (ACTUAL), the generic model forecast (FORECAST-G) and the interpretable model\nforecast", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1848, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56532a7f-ba08-4b2e-a2b0-fa53bc555c9b": {"__data__": {"id_": "56532a7f-ba08-4b2e-a2b0-fa53bc555c9b", "embedding": null, "metadata": {"page_label": "9", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary:\n\nThis section discusses the architecture and learning procedures of the N-BEATS model, focusing on its generic and interpretable variants. Key topics include:\n\n1. **Model Outputs**: The section compares the actual data (ACTUAL) with the forecasts from the generic model (FORECAST-G) and the interpretable model (FORECAST-I). It explains that FORECAST-G is the sum of outputs from two stacks, while FORECAST-I is the sum of outputs from Trend and Seasonality stacks.\n\n2. **Meta-Learning Framework**: N-BEATS is framed as a meta-learning model. The outer learning procedure involves the entire network's parameters learned via gradient descent. The inner learning procedure involves basic building blocks that modify expansion coefficients (\u03b8f) through a sequence of stages, each corresponding to a block within the stack.\n\n3. **Block Functionality**: Each block in the stack performs an update step that modifies the expansion coefficients, which are then used to form the final prediction. The inner learning procedure uses a single history from a time series (TS) as a training set to produce forward expansion coefficients (\u03b8f).\n\n4. **Backward Expansion Coefficients**: Each block also produces backward expansion coefficients (\u03b8b) that condition the learning and output of the next block.\n\n5. **Meta-Parameters**: In the interpretable model, meta-parameters are confined to the fully connected (FC) layers, while in the generic model, they also include variables (V\u2019s) that define the basis functions (gf) non-parametrically.\n\n6. **Ablation Study**: An ablation study (detailed in Appendix B) shows that increasing the number of blocks and stacks improves generalization performance, interpreted as more iterations of the inner learning procedure.\n\nEntities:\n- N-BEATS model\n- Generic model (FORECAST-G)\n- Interpretable model (FORECAST-I)\n- Expansion coefficients (\u03b8f, \u03b8b)\n- Blocks and stacks\n- Meta-parameters\n- Fully connected (FC) layers\n- Basis functions (gf)\n- Ablation study\n\nOverall, the section provides a detailed explanation of how the N-BEATS model operates, emphasizing its meta-learning aspects and the roles of different components in the forecasting process."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_8", "node_type": "4", "metadata": {"page_label": "9", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "d9f47862c5663d4917bc29bdd803bd5c015325dae160f6bb754b8b6515dde4e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6e46701-f118-4e1e-8c0b-6ff2db986182", "node_type": "1", "metadata": {"page_label": "9", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "e6044935abe6241e58e2477be77ceb285a13c6890644d0c9310e6b2450d5b966", "class_name": "RelatedNodeInfo"}}, "text": "(ACTUAL), the generic model forecast (FORECAST-G) and the interpretable model\nforecast (FORECAST-I). Columns (b) and (c) show the outputs of stacks 1 and 2 of the generic model,\nrespectively; FORECAST-G is their summation. Columns (d) and (e) show the output of the Trend\nand the Seasonality stacks of the interpretable model, respectively; FORECAST-I is their summation.\nouter parameters or meta-parameters (e.g. genes) which get modi\ufb01ed only in the outer learning\nprocedure.\nN-BEATS can be cast as an instance of meta-learning by drawing the following parallels. The outer\nlearning procedure is encapsulated in the parameters of the whole network, learned by gradient\ndescent. The inner learning procedure is encapsulated in the set of basic building blocks and modi\ufb01es\nthe expansion coef\ufb01cients \u03b8fthat basis gftakes as inputs. The inner learning proceeds through a\nsequence of stages, each corresponding to a block within the stack of the architecture. Each of the\nblocks can be thought of as performing the equivalent of an update step which gradually modi\ufb01es\nthe expansion coef\ufb01cients \u03b8fwhich eventually feed into gfin each block (which get added together\nto form the \ufb01nal prediction). The inner learning procedure takes a single history from a piece of a\nTS and sees that history as a training set. It produces forward expansion coef\ufb01cients \u03b8f(see Fig. 1),\nwhich parametrically map inputs to predictions. In addition, each preceding block modi\ufb01es the input\nto the next block by producing backward expansion coef\ufb01cients \u03b8b, thus conditioning the learning\nand the output of the next block. In the case of the interpretable model, the meta-parameters are only\nin the FC layers because the gf\u2019s are \ufb01xed. In the case of the generic model, the meta-parameters\nalso include the V\u2019s which de\ufb01ne the gfnon-parametrically. This point of view is further reinforced\nby the results of the ablation study reported in Appendix B showing that increasing the number of\nblocks in the stack, as well as the number of stacks improves generalization performance, and can be\ninterpreted as more iterations of the inner learning procedure.\n9", "mimetype": "text/plain", "start_char_idx": 1762, "end_char_idx": 3887, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d97989ac-2c82-4c04-b3a2-0f1cb698362b": {"__data__": {"id_": "d97989ac-2c82-4c04-b3a2-0f1cb698362b", "embedding": null, "metadata": {"page_label": "10", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section primarily discusses the conclusions of a study on a novel architecture for univariate time series (TS) forecasting, presented at ICLR 2020. Key points include:\n\n1. **Architecture and Performance**: The proposed architecture is general, flexible, and performs well across various TS forecasting problems. It was tested on three challenging datasets (M4, M3, and TOURISM) and achieved state-of-the-art performance in both generic and interpretable configurations.\n\n2. **Hypotheses Validated**:\n   - The generic deep learning (DL) approach is highly effective for heterogeneous univariate TS forecasting without requiring domain-specific knowledge.\n   - It is feasible to constrain a DL model to produce human-interpretable outputs by decomposing its forecasts.\n\n3. **Multi-task Learning**: The study demonstrated that DL models could be trained on multiple time series in a multi-task fashion, allowing for successful transfer and sharing of individual learnings.\n\n4. **Speculation on Meta-Learning**: The authors speculate that the performance of N-BEATS may be partly due to a form of meta-learning, suggesting this as a topic for future research.\n\n5. **References**: The section includes a list of references, citing works related to machine learning, forecasting methods, and specific models like TensorFlow, the theta model, and XGBoost.\n\n### Key Entities:\n- **N-BEATS**: The novel architecture proposed for univariate TS forecasting.\n- **Datasets**: M4, M3, and TOURISM.\n- **Deep Learning (DL)**: The approach used for forecasting.\n- **Meta-Learning**: A potential factor contributing to the model's performance.\n- **References**: Various authors and works cited, including TensorFlow, the theta model, and XGBoost."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_9", "node_type": "4", "metadata": {"page_label": "10", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "a4aa78b85b5ed61fc57cfe55b6cd4cf4039306d74ae3ab4284f8088388a0cd7c", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\n7 C ONCLUSIONS\nWe proposed and empirically validated a novel architecture for univariate TS forecasting. We showed\nthat the architecture is general, \ufb02exible and it performs well on a wide array of TS forecasting prob-\nlems. We applied it to three non-overlapping challenging competition datasets: M4, M3 and TOURISM\nand demonstrated state-of-the-art performance in two con\ufb01gurations: generic and interpretable. This\nallowed us to validate two important hypotheses: (i) the generic DL approach performs exceptionally\nwell on heterogeneous univariate TS forecasting problems using no TS domain knowledge, (ii) it is\nviable to additionally constrain a DL model to force it to decompose its forecast into distinct human\ninterpretable outputs. We also demonstrated that the DL models can be trained on multiple time series\nin a multi-task fashion, successfully transferring and sharing individual learnings. We speculate that\nN-BEATS\u2019s performance can be attributed in part to it carrying out a form of meta-learning, a deeper\ninvestigation of which should be the subject of future work.\nREFERENCES\nMart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew\nHarp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath\nKudlur, Josh Levenberg, Dan Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike\nSchuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent\nVanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Wattenberg,\nMartin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning\non heterogeneous systems, 2015. URL http://tensorflow.org/ . Software available from\ntensor\ufb02ow.org.\nV . Assimakopoulos and K. Nikolopoulos. The theta model: a decomposition approach to forecasting.\nInternational Journal of Forecasting , 16(4):521\u2013530, 2000.\nGeorge Athanasopoulos and Rob J. Hyndman. The value of feedback in forecasting competitions.\nInternational Journal of Forecasting , 27(3):845\u2013849, 2011.\nGeorge Athanasopoulos, Rob J. Hyndman, Haiyan Song, and Doris C. Wu. The tourism forecasting\ncompetition. International Journal of Forecasting , 27(3):822\u2013844, 2011.\nLee C. Baker and Jeremy Howard. Winning methods for forecasting tourism time series. International\nJournal of Forecasting , 27(3):850\u2013852, 2011.\nYoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. In Proceedings\nof the International Joint Conference on Neural Networks , pp. II\u2013A969, Seattle, USA, 1991.\nChristoph Bergmeir, Rob J. Hyndman, and Jos\u00e9 M. Ben\u00edtez. Bagging exponential smoothing methods\nusing STL decomposition and Box\u2013Cox transformation. International Journal of Forecasting , 32\n(2):303\u2013312, 2016.\nLeo Breiman. Bagging predictors. Machine Learning , 24(2):123\u2013140, Aug 1996.\nPhil Brierley. Winning methods for forecasting seasonal tourism time series. International Journal of\nForecasting , 27(3):853\u2013854, 2011.\nShiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael\nWitbrock, Mark A Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks.\nInNIPS , pp. 77\u201387, 2017.\nTianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In ACM SIGKDD , pp.\n785\u2013794, 2016.\nRobert B. Cleveland, William S. Cleveland, Jean E. McRae, and Irma Terpenning. STL: A seasonal-\ntrend decomposition procedure based on Loess (with discussion). Journal of Of\ufb01cial Statistics , 6:\n3\u201373, 1990.\nDheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.\nuci.edu/ml .\n10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3742, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ce1a22b-3d6f-442b-8f05-7d1f86ab4153": {"__data__": {"id_": "6ce1a22b-3d6f-442b-8f05-7d1f86ab4153", "embedding": null, "metadata": {"page_label": "11", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "The section primarily consists of a list of references cited in a conference paper published at ICLR 2020. The key topics and entities mentioned include:\n\n1. **Forecasting Methods and Models**:\n   - Theta method and state space models (Fiorucci et al., 2016).\n   - DeepAR for probabilistic forecasting (Flunkert et al., 2017).\n   - Exponentially weighted averages for forecasting trends and seasonals (Holt, 1957, 2004).\n   - Automatic time series forecasting with the forecast package for R (Hyndman and Khandakar, 2008).\n   - Feature-based Forecast Model Averaging (FFORMA) (Montero-Manso et al., 2019).\n\n2. **Deep Learning and Neural Networks**:\n   - Deep residual learning for image recognition (He et al., 2016).\n   - Densely connected convolutional networks (Huang et al., 2017).\n   - Residual LSTM for distant speech recognition (Kim et al., 2017).\n   - Rectified linear units (ReLU) in restricted Boltzmann machines (Nair and Hinton, 2010).\n   - Dual-stage attention-based recurrent neural network for time series prediction (Qin et al., 2017).\n\n3. **Forecasting Competitions and Datasets**:\n   - M4 dataset and competition (M4 Team, 2018a, 2018b).\n   - M3-Competition results and implications (Makridakis and Hibon, 2000).\n   - Accuracy of extrapolation methods from a forecasting competition (Makridakis et al., 1982).\n   - M4-Competition results and findings (Makridakis et al., 2018b).\n\n4. **Forecast Accuracy and Impact**:\n   - Measures of forecast accuracy (Hyndman and Koehler, 2006).\n   - Impact of forecast error on enterprises (Kahn, 2003).\n\n5. **General Forecasting Insights**:\n   - Answers to forecasting questions (Jain, 2017).\n   - Concerns and ways forward in statistical and machine learning forecasting methods (Makridakis et al., 2018a).\n\nThe section is a comprehensive collection of references that cover various aspects of forecasting methods, deep learning applications in forecasting, and the results and implications of major forecasting competitions."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_10", "node_type": "4", "metadata": {"page_label": "11", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "1eaefb36e78540037d77cd614a0d5e89eb92b26993592be6e6a183c9fbd4357c", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nJose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, and Anne B. Koehler.\nModels for optimising the Theta method and their relationship to state space models. International\nJournal of Forecasting , 32(4):1151\u20131161, 2016.\nValentin Flunkert, David Salinas, and Jan Gasthaus. DeepAR: Probabilistic forecasting with autore-\ngressive recurrent networks. CoRR , abs/1704.04110, 2017.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In CVPR , pp. 770\u2013778. IEEE Computer Society, 2016.\nC. C. Holt. Forecasting trends and seasonals by exponentially weighted averages. Technical Report\nONR memorandum no. 5, Carnegie Institute of Technology, Pittsburgh, PA, 1957.\nCharles C. Holt. Forecasting seasonals and trends by exponentially weighted moving averages.\nInternational Journal of Forecasting , 20(1):5\u201310, 2004.\nGao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected\nconvolutional networks. In CVPR , pp. 2261\u20132269. IEEE Computer Society, 2017.\nRob Hyndman and Anne B. Koehler. Another look at measures of forecast accuracy. International\nJournal of Forecasting , 22(4):679\u2013688, 2006.\nRob J Hyndman and Yeasmin Khandakar. Automatic time series forecasting: the forecast package\nfor R. Journal of Statistical Software , 26(3):1\u201322, 2008.\nChaman L. Jain. Answers to your forecasting questions. Journal of Business Forecasting , 36, Spring\n2017.\nKenneth B. Kahn. How to measure the impact of a forecast error on an enterprise? The Journal of\nBusiness Forecasting Methods & Systems , 22(1), Spring 2003.\nJaeyoung Kim, Mostafa El-Khamy, and Jungwon Lee. Residual lstm: Design of a deep recurrent\narchitecture for distant speech recognition. In Interspeech 2017 , pp. 1591\u20131595, 2017.\nM4 Team. M4 dataset, 2018a. URL https://github.com/M4Competition/M4-methods/tree/\nmaster/Dataset .\nM4 Team. M4 competitor\u2019s guide: prizes and rules, 2018b. URL www.m4.unic.ac.cy/\nwp-content/uploads/2018/03/M4-CompetitorsGuide.pdf .\nS Makridakis, E Spiliotis, and V Assimakopoulos. Statistical and machine learning forecasting\nmethods: Concerns and ways forward. PLoS ONE , 13(3), 2018a.\nSpyros Makridakis and Mich\u00e8le Hibon. The M3-Competition: results, conclusions and implications.\nInternational Journal of Forecasting , 16(4):451\u2013476, 2000.\nSpyros Makridakis, A Andersen, Robert Carbone, Robert Fildes, Michele Hibon, Rudolf\nLewandowski, Joseph Newton, Emanuel Parzen, and Robert Winkler. The accuracy of extrapo-\nlation (time series) methods: Results of a forecasting competition. Journal of forecasting , 1(2):\n111\u2013153, 1982.\nSpyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The M4-Competition:\nResults, \ufb01ndings, conclusion and way forward. International Journal of Forecasting , 34(4):\n802\u2013808, 2018b.\nPablo Montero-Manso, George Athanasopoulos, Rob J Hyndman, and Thiyanga S Talagala.\nFFORMA: Feature-based Forecast Model Averaging. International Journal of Forecasting , 2019.\nto appear.\nVinod Nair and Geoffrey E. Hinton. Recti\ufb01ed linear units improve restricted boltzmann machines. In\nICML , pp. 807\u2013814, 2010.\nYao Qin, Dongjin Song, Haifeng Chen, Wei Cheng, Guofei Jiang, and Garrison W. Cottrell. A\ndual-stage attention-based recurrent neural network for time series prediction. In IJCAI-17 , pp.\n2627\u20132633, 2017.\n11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3378, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fceb242-60ea-4e4c-8ad7-44d5cfabeb74": {"__data__": {"id_": "5fceb242-60ea-4e4c-8ad7-44d5cfabeb74", "embedding": null, "metadata": {"page_label": "12", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "The section primarily lists references related to time series forecasting, highlighting various methods and models used in the field. Key topics include:\n\n1. **Deep State Space Models**: Referenced in works by Syama Sundar Rangapuram et al. (2018a, 2018b) for time series forecasting.\n2. **Hybrid Methods**: Slawek Smyl's work (2020) on combining exponential smoothing with recurrent neural networks, and another hybrid method by Evangelos Spiliotis et al. (2019) involving data smoothing, the theta method, and seasonal factor shrinkage.\n3. **Data Preprocessing and Augmentation**: Discussed by Slawek Smyl and Karthik Kuber (2016) for short time series forecasting using recurrent neural networks.\n4. **Demand Pattern Categorization**: Addressed by A. A. Syntetos et al. (2005).\n5. **Multivariate Probabilistic Forecasting**: J. Toubeau et al. (2019) focus on short-term scheduling in power markets using deep learning.\n6. **Deep Factors for Forecasting**: Yuyang Wang et al. (2019) discuss this in the context of ICML.\n7. **Temporal Regularized Matrix Factorization**: Hsiang-Fu Yu et al. (2016) for high-dimensional time series prediction.\n8. **Residual Recurrent Highway Networks**: Tehseen Zia and Saad Razzaq (2018) for deep sequence prediction models.\n9. **Exponential Smoothing**: Peter R. Winters (1960) on forecasting sales using exponentially weighted moving averages.\n10. **U.S. Census Bureau**: Reference manual for the X-13ARIMA-SEATS Program (2013).\n\nEntities include researchers, journals, conferences (ICLR, NeurIPS, ICML), and specific forecasting methods and models."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_11", "node_type": "4", "metadata": {"page_label": "12", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "31e208e78c830d276ac324ba5b923cf65446fd2da9c03683829d25fb95d642f4", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nSyama Sundar Rangapuram, Matthias Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim\nJanuschowski. Deep state space models for time series forecasting. In NeurIPS , 2018a.\nSyama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and\nTim Januschowski. Deep state space models for time series forecasting. In NeurIPS 31 , pp.\n7785\u20137794, 2018b.\nSlawek Smyl. A hybrid method of exponential smoothing and recurrent neural networks for time\nseries forecasting. International Journal of Forecasting , 36(1):75 \u2013 85, 2020.\nSlawek Smyl and Karthik Kuber. Data preprocessing and augmentation for multiple short time series\nforecasting with recurrent neural networks. In 36th International Symposium on Forecasting , 2016.\nEvangelos Spiliotis, Vassilios Assimakopoulos, and Konstantinos Nikolopoulos. Forecasting with a\nhybrid method utilizing data smoothing, a variation of the theta method and shrinkage of seasonal\nfactors. International Journal of Production Economics , 209:92\u2013102, 2019.\nA. A. Syntetos, J. E. Boylan, and J. D. Croston. On the categorization of demand patterns. Journal of\nthe Operational Research Society , 56(5):495\u2013503, 2005.\nJ. Toubeau, J. Bottieau, F. Vall\u00e9e, and Z. De Gr\u00e8ve. Deep learning-based multivariate probabilistic\nforecasting for short-term scheduling in power markets. IEEE Transactions on Power Systems , 34\n(2):1203\u20131215, March 2019.\nU.S. Census Bureau. Reference manual for the X-13ARIMA-SEATS Program, version 1.0, 2013.\nURL http://www.census.gov/ts/x13as/docX13AS.pdf .\nYuyang Wang, Alex Smola, Danielle C. Maddix, Jan Gasthaus, Dean Foster, and Tim Januschowski.\nDeep factors for forecasting. In ICML , 2019.\nPeter R. Winters. Forecasting sales by exponentially weighted moving averages. Management\nScience , 6(3):324\u2013342, 1960.\nHsiang-Fu Yu, Nikhil Rao, and Inderjit S. Dhillon. Temporal regularized matrix factorization for\nhigh-dimensional time series prediction. In NIPS , 2016.\nTehseen Zia and Saad Razzaq. Residual recurrent highway networks for learning deep sequence\nprediction models. Journal of Grid Computing , Jun 2018.\n12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2150, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "778044aa-ad89-4dcc-a73b-0067ea674ceb": {"__data__": {"id_": "778044aa-ad89-4dcc-a73b-0067ea674ceb", "embedding": null, "metadata": {"page_label": "13", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section primarily discusses the details of three datasets used in forecasting: the M4, M3, and TOURISM datasets. Key topics and entities include:\n\n1. **M4 Dataset**:\n   - Composition: Number of time series based on frequency (Yearly, Quarterly, Monthly, Weekly, Daily, Hourly) and type (Demographic, Finance, Industry, Macro, Micro, Other).\n   - Summary statistics: Minimum, maximum, mean, and standard deviation of series lengths.\n   - Characteristics: Classification of series as smooth or erratic based on the squared coefficient of variation.\n   - Notable: All series have positive observed values at all time-steps, meaning none are intermittent or lumpy.\n\n2. **M3 Dataset**:\n   - Composition: Similar to M4, listing the number of time series based on frequency and type.\n   - Summary statistics: Similar metrics as M4.\n   - Characteristics: Also classified as smooth or erratic.\n   - Notable: All series have positive observed values at all time-steps, with no intermittent or lumpy series.\n\n3. **TOURISM Dataset**:\n   - Composition: Number of time series based on frequency.\n   - Summary statistics: Similar metrics as M4 and M3.\n   - Characteristics: Higher fraction of erratic series compared to M4 and M3.\n   - Notable: All series have positive observed values at all time-steps.\n\nThe section emphasizes the diversity and heterogeneity of the datasets, particularly in the context of business, financial, and economic forecasting."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_12", "node_type": "4", "metadata": {"page_label": "13", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "0b1902a6b20d6e4c846a3be3b186ca0aa311333d478a0ab37211dd0403999079", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nTable 2: Composition of the M4 dataset: the number of time series based on their sampling frequency\nand type.\nFrequency / Horizon\nType Yearly/6 Qtly/8 Monthly/18 Wkly/13 Daily/14 Hrly/48 Total\nDemographic 1,088 1,858 5,728 24 10 0 8,708\nFinance 6,519 5,305 10,987 164 1,559 0 24,534\nIndustry 3,716 4,637 10,017 6 422 0 18,798\nMacro 3,903 5,315 10,016 41 127 0 19,402\nMicro 6,538 6,020 10,975 112 1,476 0 25,121\nOther 1,236 865 277 12 633 414 3,437\nTotal 23,000 24,000 48,000 359 4,227 414 100,000\nMin. Length 19 24 60 93 107 748\nMax. Length 841 874 2812 2610 9933 1008\nMean Length 37.3 100.2 234.3 1035.0 2371.4 901.9\nSD Length 24.5 51.1 137.4 707.1 1756.6 127.9\n% Smooth 82% 89% 94% 84% 98% 83%\n% Erratic 18% 11% 6% 16% 2% 17%\nA D ATASET DETAILS\nA.1 M4 D ATASET DETAILS\nTable 2 outlines the composition of the M4 dataset across domains and forecast horizons by listing the\nnumber of time series based on their frequency and type (M4 Team, 2018b). The M4 dataset is large\nand diverse: all forecast horizons are composed of heterogeneous time series types (with exception of\nHourly) frequently encountered in business, \ufb01nancial and economic forecasting. Summary statistics\non series lengths are also listed, showing wide variability therein, as well as a characterization ( smooth\nvserratic ) that follows Syntetos et al. (2005), and is based on the squared coef\ufb01cient of variation of\nthe series. All series have positive observed values at all time-steps; as such, none can be considered\nintermittent orlumpy per Syntetos et al. (2005).\nA.2 M3 D ATASET DETAILS\nTable 3 outlines the composition of the M3 dataset across domains and forecast horizons by listing\nthe number of time series based on their frequency and type (Makridakis & Hibon, 2000). The\nM3 is smaller than the M4, but it is still large and diverse: all forecast horizons are composed\nof heterogeneous time series types frequently encountered in business, \ufb01nancial and economic\nforecasting. Summary statistics on series lengths are also listed, showing wide variability in length,\nas well as a characterization ( smooth vserratic ) that follows Syntetos et al. (2005), and is based\non the squared coef\ufb01cient of variation of the series. All series have positive observed values at all\ntime-steps; as such, none can be considered intermittent orlumpy per Syntetos et al. (2005).\nA.3 TOURISM DATASET DETAILS\nTable 4 outlines the composition of the TOURISM dataset across forecast horizons by listing the\nnumber of time series based on their frequency. Summary statistics on series lengths are listed,\nshowing wide variability in length. All series have positive observed values at all time-steps. In\ncontrast to M4 and M3 datasets, TOURISM includes a much higher fraction of erratic series.\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2799, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae6b3568-0f6f-460b-82b0-9efec80d6402": {"__data__": {"id_": "ae6b3568-0f6f-460b-82b0-9efec80d6402", "embedding": null, "metadata": {"page_label": "14", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary:\nThe section provides detailed compositions of two datasets, M3 and TOURISM, used in time series forecasting. \n\n1. **M3 Dataset**:\n   - **Types and Frequencies**: The dataset is categorized by sampling frequency (Yearly, Quarterly, Monthly, Other) and type (Demographic, Finance, Industry, Macro, Micro, Other).\n   - **Statistics**: It includes the number of time series for each category, minimum, maximum, mean, and standard deviation (SD) lengths of the time series, and the percentage of smooth and erratic series.\n   - **Total Time Series**: 3,003 time series in total.\n\n2. **TOURISM Dataset**:\n   - **Frequencies**: The dataset is categorized by sampling frequency (Yearly, Quarterly, Monthly).\n   - **Statistics**: It includes the number of time series for each frequency, minimum, maximum, mean, and SD lengths of the time series, and the percentage of smooth and erratic series.\n   - **Total Time Series**: 1,311 time series in total.\n\nThe section provides a comprehensive overview of the datasets' compositions, which are crucial for understanding the characteristics and variability of the time series data used in the study."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_13", "node_type": "4", "metadata": {"page_label": "14", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "7b8b7c70e1b8b02fa93529692c6b4c03f53dd75f9bc91c2ee61893613a2ee626", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nTable 3: Composition of the M3 dataset: the number of time series based on their sampling frequency\nand type.\nFrequency / Horizon\nType Yearly/6 Quarterly/8 Monthly/18 Other/8 Total\nDemographic 245 57 111 0 413\nFinance 58 76 145 29 308\nIndustry 102 83 334 0 519\nMacro 83 336 312 0 731\nMicro 146 204 474 4 828\nOther 11 0 52 141 204\nTotal 645 756 1,428 174 3,003\nMin. Length 20 24 66 71\nMax. Length 47 72 144 104\nMean Length 28.4 48.9 117.3 76.6\nSD Length 9.9 10.6 28.5 10.9\n% Smooth 90% 99% 98% 100%\n% Erratic 10% 1% 2% 0%\nTable 4: Composition of the TOURISM dataset: the number of time series based on their sampling\nfrequency.\nFrequency / Horizon\nYearly/4 Quarterly/8 Monthly/24 Total\n518 427 366 1,311\nMin. Length 11 30 91\nMax. Length 47 130 333\nMean Length 24.4 99.6 298\nSD Length 5.5 20.3 55.7\n% Smooth 77% 61% 49%\n% Erratic 23% 39% 51%\n14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 887, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f58e6968-bee1-4597-93bc-f7225760642e": {"__data__": {"id_": "f58e6968-bee1-4597-93bc-f7225760642e", "embedding": null, "metadata": {"page_label": "15", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section primarily discusses various ablation studies conducted on the N-BEATS architecture to evaluate its performance using the sMAPE metric. Key topics and entities include:\n\n1. **Layer Stacking and Basis Synergy**:\n   - **Generic Architecture**: The study shows that increasing the number of stacks in the generic architecture reduces the error, with diminishing returns after a certain point. The network with 30 stacks (150 layers deep) performs best.\n   - **Interpretable Architecture**: This architecture combines trend and seasonality models in two stacks. The study finds that sharing weights within stacks and using different basis functions for each stack improves performance. The best configuration is a 60-layer deep network.\n\n2. **Ensemble Size**:\n   - Increasing the ensemble size improves performance. N-BEATS achieves state-of-the-art performance with an ensemble size of 18 models, indicating computational efficiency.\n\n3. **Doubly Residual Stacking (DRESS)**:\n   - The DRESS principle involves running a residual backcast connection and producing partial block-level forecasts aggregated at stack and model levels. The study confirms the effectiveness of this topology by comparing it to alternatives where either the backcast or partial forecast links are removed.\n\n### Key Entities:\n- **sMAPE (Symmetric Mean Absolute Percentage Error)**: The performance metric used.\n- **Stacks**: Layers of residual blocks in the network.\n- **Residual Blocks**: Basic building units of the network.\n- **FC Layers (Fully Connected Layers)**: Layers within each residual block.\n- **Trend and Seasonality Models**: Components of the interpretable architecture.\n- **Ensemble Size**: Number of models used in the ensemble.\n- **DRESS (Doubly Residual Stacking)**: A topological principle used in N-BEATS.\n\n### Key Findings:\n- Increasing the number of stacks improves performance up to a point.\n- Combining different basis functions in the interpretable architecture yields better results.\n- Larger ensemble sizes enhance performance, but N-BEATS performs well even with smaller ensembles.\n- The DRESS principle is effective in improving forecasting accuracy."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_14", "node_type": "4", "metadata": {"page_label": "15", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "42af229044f5b5c0d720fd4e64e42fbfd07ea061266181815ae9a1009645860d", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nTable 5: sMAPE on the validation set, generic ar-\nchitecture. sMAPE for varying number of stacks,\neach having one residual block.\nStacks s MAPE\n1 11.154\n3 11.061\n9 10.998\n18 10.950\n30 10.937Table 6: sMAPE on the validation set, inter-\npretable architecture. Ablation of the synergy\nof the layers with different basis functions and\nmulti-block stack gain.\nDetrend Seasonality s MAPE\n0 2 11.189\n2 0 11.572\n1 1 11.040\n3 3 10.986\nB A BLATION STUDIES\nB.1 L AYER STACKING AND BASIS SYNERGY\nWe performed an ablation study on the validation set, using sMAPE metric as performance criterion.\nWe addressed two speci\ufb01c questions with this study. First, Is stacking layers helpful? Second, Does\nthe architecture based on the combination of layers with different basis functions results in better\nperformance than the architecture using only one layer type?\nLayer stacking. We start our study with the generic architecture that consists of stacks of one\nresidual block of 5 FC layers each of the form Fig. 1 and we increase the number of stacks. Results\npresented in Table 5 con\ufb01rm that increasing the number of stacks decreases error and at certain point\nthe gain saturates. We would like to mention that the network having 30 stack of depth 5 is in fact a\nvery deep network of total depth 150 layers.\nBasis synergy. Stacking works well for the interpretable architecture as can be seen in Table 6\ndepicting the results of ablating the interpretable architecture con\ufb01guration. Here we experiment\nwith the architecture that is composed of 2 stacks, stack one is trend model and stack two is the\nseasonality model. Each stack has variable number of residual blocks and each residual block has 5\nFC layers. We found that this architecture works best when all weights are shared within stack. We\nclearly see that increasing the number of layers improves performance. The largest network is 60\nlayers deep. On top of that, we observe that the architecture that consists of stacks based on different\nbasis functions wins over the architecture based on the same stack. It looks like chaining stacks of\ndifferent nature results in synergistic effects. This is logical as function classes that can be modelled\nby trend and seasonality stacks have small overlap.\nB.2 E NSEMBLE SIZE\nFigure 3 demonstrates that increasing the ensemble size results in improved performance. Most\nimportantly, according to Figure 3, N-BEATS achieves state-of-the-art performance even if compara-\ntively small ensemble size of 18 models is used. Therefore, computational ef\ufb01ciency of N-BEATS\ncan be traded very effectively for performance and there is no over-reliance of the results on large\nensemble size.\nB.3 D OUBLY RESIDUAL STACKING\nIn Section 3.2 we described the proposed doubly residual stacking (DRESS) principle, which is the\ntopological foundation of N-BEATS. The topology is based on both (i) running a residual backcast\nconnection and (ii) producing partial block-level forecasts that are further aggregated at stack and\nmodel levels to produce the \ufb01nal model-level forecast. In this section we conduct a study to con\ufb01rm\nthe accuracy effectiveness of this topology compared to several alternatives. The methodology\nunderlying this study is that we remove either the backcast or partial forecast links or both and track\nhow this affects the forecasting metrics. We keep the number of parameters in the network for each\nof the architectural alternatives \ufb01xed by using the same number of layers in the network (we used\ndefault hyperparameter settings reported in Table 18). The architectural alternatives are depicted in\nFigure 4 and described in detail below.\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3674, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b480e810-917e-46f6-a546-0525660b8293": {"__data__": {"id_": "b480e810-917e-46f6-a546-0525660b8293", "embedding": null, "metadata": {"page_label": "16", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "The section primarily discusses the performance and architecture variations of the N-BEATS model, a neural network for time series forecasting, as evaluated on the M4 dataset. Key topics include:\n\n1. **Ensemble Size and Performance**: Figure 3 illustrates that N-BEATS maintains high performance (measured by OWA) even with a significantly reduced ensemble size.\n2. **N-BEATS-DRESS**: The default configuration using doubly residual stacking, described in Section 3.2.\n3. **Alternative Architectures**:\n   - **PARALLEL**: Disables backward residual connections, with each block forecasting in parallel.\n   - **NO-RESIDUAL**: Disables backward residual connections, with each block's backcast forecast fed to the next block.\n   - **LAST-FORWARD**: Uses backward residual connections but only the last block's forecast is used.\n   - **NO-RESIDUAL-LAST-FORWARD**: Disables both backward residual and partial forward connections, resembling a deep feed-forward network.\n4. **Ablation Study Results**: Quantitative results on the M4 dataset (Tables 7\u201310) show that the doubly residual stacking topology (N-BEATS-DRESS) outperforms other architectures. The study used an ensemble size of 18 for N-BEATS-DRESS, leading to a higher OWA metric compared to N-BEATS-G with an ensemble size of 180.\n\nThe section concludes that the doubly residual stacking topology provides a clear advantage over other architectural variations."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_15", "node_type": "4", "metadata": {"page_label": "16", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "5c40a64c54be140a91341020fdd01188f0b8db66ad43fcf687e6a20214db1de2", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\n18 36 90 180\nEnsemble Size0.7970.7980.7990.8000.8010.802OWA\nFigure 3: M4 test performance ( OWA ) as a function of ensemble size, based on N-BEATS-G. This\n\ufb01gure shows that N-BEATS loses less than 0.5% in terms of OWA performance even if 10 times\nsmaller ensemble size is used.\nN-BEATS-DRESS is depicted in Fig. 4a. This is the default con\ufb01guration of N-BEATS using doubly\nresidual stacking described in Section 3.2.\nPARALLEL is depicted in Fig. 4b. This is the alternative where the backward residual connection is\ndisabled and the overall model input is fed to every block. The blocks then forecast in parallel using\nthe same input and their individual outputs are summed to make the \ufb01nal forecast.\nNO-RESIDUAL is depicted in Fig. 4c. This is the alternative where the backward residual connection\nis disabled. Unlike PARALLEL, in this case the backcast forecast of the previous block is fed as input\nto the next block. Unlike the usual feed-forward network, in the NO-RESIDUAL architecture, each\nblock makes a partial forecast and their individual outputs are summed to make the \ufb01nal forecast.\nLAST-FORWARD is depicted in Fig. 4d. This is the alternative where the backward residual\nconnection is active, however the model level forecast is derived only from the last block. So, the\npartial forward forecasts are disabled. This is the architecture that is closest to the classical residual\nnetwork.\nNO-RESIDUAL-LAST-FORWARD is depicted in Fig. 4f. This is the alternative where both\nbackward residual and the partial forward connections are disabled. This is therefore a simple\nfeed-forward network, but very deep.\nThe quantitative ablation study results on the M4 dataset are reported in Tables 7\u201310. N-BEATS-\nDRESS model is essentially N-BEATS model in this study. For this study we used ensemble size\nof 18. Since the ensemble size is 18 for N-BEATS-DRESS, as opposed to 180 used for N-BEATS,\nthe OWA metric reported in Table 9 for N-BEATS-DRESS is higher than the OWA reported for\nN-BEATS-G in Table 12. Note that both results align well with OWA reported in Figure 3 for different\nensemble sizes, as part of the ensemble size ablation conducted in Section B.2.\nThe results presented in Tables 7\u201310 demonstrate that the doubly residual stacking topology provides\na clear overall advantage over the alternative architectures in which either backcast residual links or\nthe partial forward forecast links are disabled.\n16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2469, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2f1be17-0961-48eb-a916-72dfb50220da": {"__data__": {"id_": "d2f1be17-0961-48eb-a916-72dfb50220da", "embedding": null, "metadata": {"page_label": "17", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThis section of the document, published as a conference paper at ICLR 2020, focuses on the architectural configurations and performance evaluations of the N-BEATS model, specifically in the context of an ablation study of the doubly residual stack. The section includes:\n\n1. **Architectural Configurations**:\n   - **N-BEATS-DRESS**: A specific configuration of the N-BEATS model.\n   - **PARALLEL**: Another configuration where stacks operate in parallel.\n   - **NO-RESIDUAL**: Configuration without residual connections.\n   - **LAST-FORWARD**: Configuration where the last stack's output is forwarded.\n   - **NO-RESIDUAL-LAST-FORWARD**: Combination of no residuals and last-forward.\n   - **RESIDUAL-INPUT**: Configuration with residuals at the input.\n\n2. **Performance Evaluation**:\n   - **Tables 7 and 8**: These tables present the performance of different configurations on the M4 test set using the sMAPE metric (Symmetric Mean Absolute Percentage Error). Lower sMAPE values indicate better performance.\n   - **Table 7**: Performance of an ensemble of 18 generic models across different data frequencies (Yearly, Quarterly, Monthly, Others) and their average.\n   - **Table 8**: Performance of an ensemble of 18 interpretable models across the same data frequencies and their average.\n\n3. **Key Findings**:\n   - **N-BEATS-DRESS-G** and **N-BEATS-DRESS-I** configurations generally show the best performance among the tested configurations.\n   - **NO-RESIDUAL-LAST-FORWARD-G** and **NO-RESIDUAL-LAST-FORWARD-I** configurations tend to perform worse, especially in the \"Others\" category.\n\n### Key Entities:\n- **N-BEATS Model**: A neural network-based forecasting model.\n- **sMAPE**: Symmetric Mean Absolute Percentage Error, a performance metric.\n- **M4 Test Set**: A dataset used for evaluating forecasting models.\n- **Architectural Configurations**: Different setups of the N-BEATS model (e.g., N-BEATS-DRESS, PARALLEL, NO-RESIDUAL, etc.).\n- **Ensemble Models**: Groups of models used together to improve forecasting accuracy.\n- **Data Frequencies**: Categories of data (Yearly, Quarterly, Monthly, Others).\n\nThis section provides a detailed comparison of various architectural configurations of the N-BEATS model and their performance on a standard forecasting dataset."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_16", "node_type": "4", "metadata": {"page_label": "17", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "12874285b9bf98242484773e8da6e9bec796fd5f526dd19708f38107a513a2d6", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nBlock 1\nBlock 2\nBlock\u00a0 K\u2013\n\u2013\n\u2013+Stack Input\nStack\nforecastStack 1\nStack 2\nStack M+Global forecast\n(model output)Model Input\nStack output\n(to next stack)\n(a) N-BEATS-DRESS\nBlock 1\nBlock 2\nBlock\u00a0 K+\nStack output\n(to next stack)Stack Input\nStack\nforecastStack 1\nStack 2\nStack M+Global forecast\n(model output)Model Input (b) PARALLEL\nBlock 1\nBlock 2\nBlock\u00a0 K+\nStack residual\n(to next stack)Stack Input\nStack\nforecastStack 1\nStack 2\nStack M+Global forecast\n(model output)Model Input (c) NO-RESIDUAL\nBlock 1\nBlock 2\nBlock\u00a0 K\u2013\n\u2013\n\u2013\nStack residual\n(to next stack)Stack Input\nStack\nforecastStack 1\nStack 2\nStack MGlobal forecast\n(model output)Model Input\n(d) LAST-FORWARD\nBlock 1\nBlock 2\nBlock\u00a0 K\nStack residual\n(to next stack)Stack Input\nStack\nforecastStack 1\nStack 2\nStack MGlobal forecast\n(model output)Model Input (e) NO-RESIDUAL-LAST-\nFORWARD\nBlock 1\nBlock 2\nBlock\u00a0 K\u2013\n\u2013\n\u2013+Stack Input\nStack\nforecastStack 1\nStack 2\nStack M+Global forecast\n(model output)Model Input\nStack output\n(to next stack)Model Input(f) RESIDUAL-INPUT\nFigure 4: The architectural con\ufb01gurations used in the ablation study of the doubly residual stack.\nSymbol\u22c4denotes unconnected output.\nTable 7: Performance on the M4 test set, sMAPE . Lower values are better. The results are obtained on\nthe ensemble of 18 generic models.\nYearly Quarterly Monthly Others Average\n(23k) (24k) (48k) (5k) (100k)\nPARALLEL-G 13.279 9.558 12.510 3.691 11.538\nNO-RESIDUAL-G 13.195 9.555 12.451 3.759 11.493\nLAST-FORWARD-G 13.200 9.322 12.352 3.703 11.387\nNO-RESIDUAL-LAST-FORWARD-G 15.386 11.346 15.282 6.673 13.931\nRESIDUAL-INPUT-G 13.264 9.545 12.316 3.692 11.438\nN-BEATS-DRESS-G 13.211 9.217 12.122 3.636 11.251\nTable 8: Performance on the M4 test set, sMAPE . Lower values are better. The results are obtained on\nthe ensemble of 18 interpretable models.\nYearly Quarterly Monthly Others Average\n(23k) (24k) (48k) (5k) (100k)\nPARALLEL-I 13.207 9.530 12.500 3.710 11.510\nNO-RESIDUAL-I 13.075 9.707 12.708 4.007 11.637\nLAST-FORWARD-I 13.168 9.547 12.111 3.599 11.313\nNO-RESIDUAL-LAST-FORWARD-I 13.067 10.207 15.177 4.912 12.986\nRESIDUAL-INPUT-I 13.104 9.716 12.814 4.005 11.697\nN-BEATS-DRESS-I 13.155 9.286 12.009 3.642 11.201\n17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2215, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8c9dd29-8ce2-4b77-b757-f1e627a59bc1": {"__data__": {"id_": "f8c9dd29-8ce2-4b77-b757-f1e627a59bc1", "embedding": null, "metadata": {"page_label": "18", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary: \n\nThe section presents performance results of various models on the M4 test set, measured using the Overall Weighted Average (OWA) metric, where lower values indicate better performance. The results are divided into two tables: one for an ensemble of 18 generic models and another for an ensemble of 18 interpretable models. Each table provides OWA scores across different time series categories: Yearly, Quarterly, Monthly, and Others, along with an overall average.\n\nKey topics and entities include:\n- Performance metrics (OWA) for time series forecasting models.\n- Comparison of different model configurations: PARALLEL, NO-RESIDUAL, LAST-FORWARD, NO-RESIDUAL-LAST-FORWARD, RESIDUAL-INPUT, and N-BEATS-DRESS.\n- Categories of time series data: Yearly, Quarterly, Monthly, and Others.\n- Two types of model ensembles: generic models and interpretable models.\n- Specific OWA scores for each model configuration and category."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_17", "node_type": "4", "metadata": {"page_label": "18", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "8b3547f98beb544ef2038f01255c690b2cd24782ad906f8ef3bcb79524efdd8c", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nTable 9: Performance on the M4 test set, OWA . Lower values are better. The results are obtained on\nthe ensemble of 18 generic models.\nYearly Quarterly Monthly Others Average\n(23k) (24k) (48k) (5k) (100k)\nPARALLEL-G 0.780 0.832 0.852 0.844 0.822\nNO-RESIDUAL-G 0.774 0.831 0.851 0.853 0.819\nLAST-FORWARD-G 0.774 0.808 0.840 0.846 0.811\nNO-RESIDUAL-LAST-FORWARD-G 0.948 1.029 1.095 1.296 1.030\nRESIDUAL-INPUT-G 0.779 0.831 0.840 0.844 0.817\nN-BEATS-DRESS-G 0.776 0.800 0.823 0.835 0.803\nTable 10: Performance on the M4 test set, OWA . Lower values are better. The results are obtained on\nthe ensemble of 18 interpretable models.\nYearly Quarterly Monthly Others Average\n(23k) (24k) (48k) (5k) (100k)\nPARALLEL-I 0.776 0.831 0.857 0.845 0.821\nNO-RESIDUAL-I 0.769 0.848 0.886 0.886 0.833\nLAST-FORWARD-I 0.773 0.836 0.825 0.817 0.808\nNO-RESIDUAL-LAST-FORWARD-I 0.771 0.900 1.085 1.016 0.922\nRESIDUAL-INPUT-I 0.771 0.848 0.892 0.887 0.836\nN-BEATS-DRESS-I 0.771 0.805 0.819 0.836 0.800\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1024, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38f09dd6-6aa0-4e1b-861b-5951c618ebee": {"__data__": {"id_": "38f09dd6-6aa0-4e1b-861b-5951c618ebee", "embedding": null, "metadata": {"page_label": "19", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section presents detailed empirical results of the N-BEATS model on the M4 dataset, highlighting its state-of-the-art performance. Key topics and entities include:\n\n1. **Performance Metrics**:\n   - **sMAPE (Symmetric Mean Absolute Percentage Error)**: Lower values indicate better performance.\n   - **OWA (Overall Weighted Average)**: Lower values are better, with rankings provided.\n\n2. **Model Configurations**:\n   - **N-BEATS-G**: Generic configuration.\n   - **N-BEATS-I**: Interpretable configuration.\n   - **N-BEATS-I+G**: Ensemble of both generic and interpretable models.\n\n3. **Comparative Analysis**:\n   - **Best Pure ML**: Submission by B. Trotta.\n   - **Best Statistical**: Model by N.Z. Legaki and K. Koutsouri.\n   - **Best ML/TS Combination**: Model by P. Montero-Manso et al.\n   - **DL/TS Hybrid (M4 Winner)**: Model by S. Smyl.\n\n4. **Results**:\n   - N-BEATS models outperform other approaches across all subsets of time series in the M4 dataset.\n   - The average OWA gap between N-BEATS and the M4 winner is significant, indicating superior performance.\n   - Detailed statistical analysis shows a preponderance of statistically significant differences favoring N-BEATS.\n\n5. **Tables**:\n   - **Table 11**: sMAPE performance across different time series categories (Yearly, Quarterly, Monthly, Others).\n   - **Table 12**: OWA performance and M4 rank across the same categories.\n   - **Table 13**: Granular sMAPE results and statistical significance analysis.\n\nOverall, the section underscores the effectiveness of the N-BEATS model in time series forecasting, particularly in comparison to other leading models from the M4 competition."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_18", "node_type": "4", "metadata": {"page_label": "19", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "570b1ab43e5ae337327c65991bec5454501eea58f5fae408ea2c26d227cad4ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0ba3c88-afc7-40c4-8e66-ccb74692fdc3", "node_type": "1", "metadata": {}, "hash": "3970b4dc0d81bbc3554446bf308718d98801d09170db0d3a73ae30f2fe202c22", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nTable 11: Performance on the M4 test set, s MAPE . Lower values are better. Red \u2013 second best.\nYearly Quarterly Monthly Others Average\n(23k) (24k) (48k) (5k) (100k)\nBest pure ML 14.397 11.031 13.973 4.566 12.894\nBest statistical 13.366 10.155 13.002 4.682 11.986\nBest ML/TS combination 13.528 9.733 12.639 4.118 11.720\nDL/TS hybrid, M4 winner 13.176 9.679 12.126 4.014 11.374\nN-BEATS-G 13.023 9.212 12.048 3.574 11.168\nN-BEATS-I 12.924 9.287 12.059 3.684 11.174\nN-BEATS-I+G 12.913 9.213 12.024 3.643 11.135\nTable 12: Performance on the M4 test set, OWA and M4 rank. Lower values are better. Red \u2013 second\nbest.\nYearly Quarterly Monthly Others Average Rank\n(23k) (24k) (48k) (5k) (100k)\nBest pure ML 0.859 0.939 0.941 0.991 0.915 23\nBest statistical 0.788 0.898 0.905 0.989 0.861 8\nBest ML/TS combination 0.799 0.847 0.858 0.914 0.838 2\nDL/TS hybrid, M4 winner 0.778 0.847 0.836 0.920 0.821 1\nN-BEATS-G 0.765 0.800 0.820 0.822 0.797\nN-BEATS-I 0.758 0.807 0.824 0.849 0.798\nN-BEATS-I+G 0.758 0.800 0.819 0.840 0.795\nC D ETAILED EMPIRICAL RESULTS\nC.1 D ETAILED RESULTS : M4 D ATASET\nTables 11 and 12 present our key quantitative empirical results showing that the proposed model\nachieves the state of the art performance on the challenging M4 benchmark. We study the performance\nof two model con\ufb01gurations: generic (Ours-G) and interpretable (Ours-I), as well as Ours-I+G\n(ensemble of all models from Ours-G and Ours-I). We compare against 4 representatives from the\nM4 competition: each best in their respective model class. Best pure ML is the submission by B.\nTrotta, the best entry among the 6 pure ML models. Best statistical is the best pure statistical model\nby N.Z. Legaki and K. Koutsouri. Best ML/TS combination is the model by P. Montero-Manso, T.\nTalagala, R.J. Hyndman and G. Athanasopoulos, second best entry, gradient boosted tree over a few\nstatistical time series models. Finally, DL/TS hybrid is the winner of M4 competition (Smyl, 2020).\nN-BEATS outperforms all other approaches on all the studied subsets of time series. The average\nOWA gap between our generic model and the M4 winner ( 0.821\u22120.795=0.026) is greater than the\ngap between the M4 winner and the second entry (0 .838\u22120.821=0.017).\nA more granular and detailed statistical analysis of our results on M4 is provided in Table 13. This\ntable \ufb01rst presents the sMAPE for N-BEATS, decomposed by M4 time series sub-type and sampling\nfrequency (upper part). Then (lower part), it shows the average sMAPE difference between the\nN-BEATS results and the M4 winner (TS/DL hybrid by S. Smyl), adding the standard error of that\ndifference (in parentheses); bold entries indicate statistical signi\ufb01cance at the 99% level based on a\ntwo-sided paired t-test.\nWe note that each cross-section of the M4 dataset into horizon and type may be regarded as an\nindependent mini-dataset. We observe that over those mini-datasets there is a preponderance of\nstatistically signi\ufb01cant differences between N-BEATS", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3008, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0ba3c88-afc7-40c4-8e66-ccb74692fdc3": {"__data__": {"id_": "b0ba3c88-afc7-40c4-8e66-ccb74692fdc3", "embedding": null, "metadata": {"page_label": "19", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary:\nThe section discusses the statistical significance of the performance differences between the N-BEATS model and the Smyl model. It highlights that N-BEATS shows a statistically significant advantage in 18 out of 31 cases. This indicates that the improvements observed in the average performance metrics (as shown in Tables 11 and 12) are both statistically significant and consistent across various subsets of the M4 dataset. Additionally, it suggests that N-BEATS generalizes well across different types of time series and sampling frequencies."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_18", "node_type": "4", "metadata": {"page_label": "19", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "570b1ab43e5ae337327c65991bec5454501eea58f5fae408ea2c26d227cad4ca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38f09dd6-6aa0-4e1b-861b-5951c618ebee", "node_type": "1", "metadata": {"page_label": "19", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "77653f1b87dfd6cf3a2abd1f0e8b0596f8daf761f8f2c6ebb1db1a7777e997d9", "class_name": "RelatedNodeInfo"}}, "text": "is a preponderance of\nstatistically signi\ufb01cant differences between N-BEATS and Smyl (18 cases out of 31) to the advantage\nof N-BEATS. This provides evidence that (i) the improvement observed on average in Tables 11\nand 12 is statistically signi\ufb01cant and consistent over smaller subsets of M4 and (ii) N-BEATS\ngeneralizes well over time series of different types and sampling frequencies.\n19", "mimetype": "text/plain", "start_char_idx": 2934, "end_char_idx": 3324, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3e4e04c-1705-4346-adfe-5319cfc2bf39": {"__data__": {"id_": "f3e4e04c-1705-4346-adfe-5319cfc2bf39", "embedding": null, "metadata": {"page_label": "20", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section presents a performance analysis of the N-BEATS model compared to the Smyl model on non-overlapping subsets of the M4 test set. The analysis is broken down by different series types (Demographic, Finance, Industry, Macro, Micro, Other) and sampling frequencies (Yearly, Quarterly, Monthly, Weekly, Daily, Hourly). Key metrics include the symmetric Mean Absolute Percentage Error (sMAPE) for each series type and frequency.\n\n**Key Topics:**\n1. **Performance Decomposition:** The section details the performance of the N-BEATS model across various subsets of the M4 test set.\n2. **Comparison with Smyl Model:** The performance of N-BEATS is compared against the Smyl model, with differences in sMAPE highlighted.\n3. **Statistical Significance:** Bold entries indicate statistically significant results at the 99% confidence level, determined using a 2-sided paired t-test.\n4. **Error Metrics:** The standard error of the mean is provided for each comparison.\n\n**Key Entities:**\n1. **N-BEATS Model:** The model being evaluated.\n2. **Smyl Model:** The benchmark model for comparison.\n3. **M4 Test Set:** The dataset used for performance evaluation.\n4. **sMAPE (symmetric Mean Absolute Percentage Error):** The primary performance metric.\n5. **Series Types:** Demographic, Finance, Industry, Macro, Micro, Other.\n6. **Sampling Frequencies:** Yearly, Quarterly, Monthly, Weekly, Daily, Hourly.\n\nThe section provides detailed numerical results and statistical analysis to support the performance claims of the N-BEATS model."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_19", "node_type": "4", "metadata": {"page_label": "20", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "c098249891647218dbf5dc7ba69e967efb3b2272695368b8a5bf4224bcf6295e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nTable 13: Performance decomposition on non-overlapping subsets of the M4 test set and comparison\nwith the Smyl model results.\nDemographic Finance Industry Macro Micro Other\nsMAPE per M4 series type and sampling frequency\nYearly 8 .931 13 .741 16 .317 13 .327 10 .489 13 .320\nQuarterly 9 .219 10 .787 8 .628 8 .576 9 .264 6 .250\nMonthly 4 .357 13 .353 12 .657 12 .571 13 .627 11 .595\nWeekly 4 .580 3 .004 9 .258 7 .220 10 .425 6 .183\nDaily 6 .351 3 .467 3 .835 2 .525 2 .299 2 .885\nHourly 8 .197\nAverage sMAPE difference vs Smyl model , computed as N-BEATS \u2013 Smyl.\nStandard error of the mean displayed in parenthesis.\nBold entries are signi\ufb01cant at the 99% level (2-sided paired t-test).\nYearly\u22120.749\u22120.337\u22120.065\u22120.386\u22120.168\u22120.157\n(0.119) ( 0.065) ( 0.087) ( 0.085) ( 0.056) ( 0.140)\nQuarterly\u22120.651\u22120.281\u22120.328\u22120.712\u22120.523\u22120.029\n(0.085) ( 0.047) ( 0.043) ( 0.060) ( 0.051) ( 0.083)\nMonthly\u22120.185\u22120.379\u22120.419 0.089 0.338\u22120.279\n(0.023) ( 0.034) ( 0.036) ( 0.039) ( 0.034) ( 0.162)\nWeekly\u22120.336\u22121.075\u22120.937\u22121.627\u22123.029\u22121.193\n(0.270) ( 0.221) ( 1.399) ( 0.770) ( 0.378) ( 0.772)\nDaily 0 .191\u22120.098\u22120.124\u22120.026\u22120.367\u22120.037\n(0.231) ( 0.018) ( 0.025) ( 0.057) ( 0.013) ( 0.015)\nHourly \u22121.132\n(0.163)\n20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1240, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53368c95-b561-405f-acfc-d7ca3a02e4bf": {"__data__": {"id_": "53368c95-b561-405f-acfc-d7ca3a02e4bf", "embedding": null, "metadata": {"page_label": "21", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section primarily discusses the performance of various forecasting methods on the M3 dataset, with a focus on the sMAPE (Symmetric Mean Absolute Percentage Error) metric. Key topics and entities include:\n\n1. **Performance Metrics**:\n   - The section provides a detailed table (Table 14) showing the sMAPE performance of different forecasting methods across various data splits (Yearly, Quarterly, Monthly, Others, and Average).\n   - Lower sMAPE values indicate better performance.\n\n2. **Forecasting Methods**:\n   - Various methods are compared, including Na\u00efve2, ARIMA, Comb S-H-D, ForecastPro, Theta, DOTM, EXP, LGT, BaggedETS.BC, and different versions of N-BEATS (N-BEATS-G, N-BEATS-I, N-BEATS-I+G).\n\n3. **Specific Observations**:\n   - Some methods did not report sMAPE for certain splits or overall averages.\n   - LGT had issues with Monthly and Quarterly data due to computational constraints.\n   - EXP's average performance was recomputed using a different methodology for consistency.\n\n4. **Calculation Methodology**:\n   - The section explains the formula used to compute the average sMAPE, which aggregates performance over all time series and forecast horizons.\n   - The formula takes into account the number of time series and the largest forecast horizon for each data split.\n\n5. **References**:\n   - The section references various studies and authors, including Fiorucci et al. (2016), Spiliotis et al. (2019), Smyl & Kuber (2016), and Bergmeir et al. (2016).\n\nOverall, the section provides a comprehensive comparison of forecasting methods using the sMAPE metric on the M3 dataset, highlighting the strengths and limitations of each method."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_20", "node_type": "4", "metadata": {"page_label": "21", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "2faf5ddfb0326de650e77fab64d07243a88d2a3b9a964243e934e3a90ab31c6b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nTable 14: Performance on the M3 test set, Average sMAPE , aggregate over all forecast horizons\n(Yearly: 1-6, Quarterly: 1-8, Monthly: 1-18, Other: 1-8, Average: 1-18). Lower values are better.\nRed \u2013 second best.\u2020Numbers are computed by us.\nYearly Quarterly Monthly Others Average\n(645) (756) (1428) (174) (3003)\nNa\u00efve2 17.88 9.95 16.91 6.30 15.47\nARIMA (B\u2013J automatic) 17.73 10.26 14.81 5.06 14.01\nComb S-H-D 17.07 9.22 14.48 4.56 13.52\nForecastPro 17.14 9.77 13.86 4.60 13.19\nTheta 16.90 8.96 13.85 4.41 13.01\nDOTM (Fiorucci et al., 2016) 15.94 9.28 13.74 4.58 12.90\nEXP (Spiliotis et al., 2019) 16.39 8.98 13.43 5.46 12 .71\u2020\nLGT (Smyl & Kuber, 2016) 15.23 n/a n/a 4.26 n/a\nBaggedETS.BC (Bergmeir et al., 2016) 17.49 9.89 13.74 n/a n/a\nN-BEATS-G 16.2 8.92 13.19 4.19 12.47\nN-BEATS-I 15.84 9.03 13.15 4.30 12.43\nN-BEATS-I+G 15.93 8.84 13.11 4.24 12.37\nC.2 D ETAILED RESULTS : M3 D ATASET\nResults for M3 dataset are provided in Table 14. The performance metric is calculated using the\nearlier version of s MAPE , de\ufb01ned speci\ufb01cally for the M3 competition:1\nsMAPE =200\nHH\n\u2211\ni=1|yT+i\u2212\u02c6yT+i|\nyT+i+\u02c6yT+i. (4)\nFor some of the methods, either average sMAPE was not reported or sMAPE for some of the splits was\nnot reported in their respective publications. Below, we list those cases. BaggedETS.BC (Bergmeir\net al., 2016) has not reported numbers on Others. LGT (Smyl & Kuber, 2016) did not report results on\nMonthly and Quarterly data. According to the authors, the underlying RNN had problems dealing with\nraw seasonal data, the ETS based pre-processing was not effective and the LGT pre-processing was\nnot computationally feasible given comparatively large number of time series and their comparatively\nlarge length (Smyl & Kuber, 2016). Finally, EXP (Spiliotis et al., 2019) reported average performance\ncomputed using a different methodology than the default M3 and M4 methodology (source: personal\ncommunication with the authors). For the latter method we recomputed the Average sMAPE based on\nthe previously reported Yearly, Quarterly and Monthly splits. To calculate it, we follow the M3, M4\nand TOURISM competition methodology and compute the average metric as the average over all time\nseries and over all forecast horizons. Given the performance metric values aggregated over Yearly,\nQuarterly and Monthly splits, the average can be computed straightforwardly as:\nsMAPE Average =NYear\nNTotsMAPE Year+NQuart\nNTotsMAPE Quart+NMonth\nNTotsMAPE Month+NOthers\nNTotsMAPE Others.\n(5)\nHere NTot=NYear+NQuart+NMonth+NOthers andNYear=6\u00d7645,NQuart=8\u00d7756,NMonth =18\u00d7\n1428,NOthers =8\u00d7174. It is clear that for each split, its Nis the product of its respective number of\ntime series and its largest forecast horizon.\n1With minor differences compared to the sMAPE de\ufb01nition used for M4. Please refer to Appendix A\nin (Makridakis & Hibon, 2000) for the mathematical de\ufb01nition.\n21", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2911, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4ca3105-6443-47af-801c-fea94043106b": {"__data__": {"id_": "a4ca3105-6443-47af-801c-fea94043106b", "embedding": null, "metadata": {"page_label": "22", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section primarily discusses the performance of various forecasting models on the TOURISM test set, with a focus on the Mean Absolute Percentage Error (MAPE) metric. Key topics and entities include:\n\n1. **Performance Metrics**: The section presents a detailed comparison of MAPE values for different models across Yearly, Quarterly, and Monthly forecast horizons. Lower MAPE values indicate better performance.\n\n2. **Models Compared**:\n   - **Statistical Benchmarks**: Includes models like SNa\u00efve, Theta, ForePro, ETS, Damped, and ARIMA.\n   - **Kaggle Competitors**: Includes teams like SaliMali, LeeCBaker, Stratometrics, Robert, and Idalgo.\n   - **N-BEATS Variants**: Includes N-BEATS-G, N-BEATS-I, and N-BEATS-I+G, which are the authors' models.\n\n3. **Results**:\n   - **N-BEATS Models**: Achieved state-of-the-art performance across all subsets of the TOURISM dataset, outperforming other models.\n   - **Comparison**: N-BEATS models showed a significant improvement over the best-known approach (LeeCBaker) and auto-ARIMA.\n\n4. **Calculation Methodology**: The section explains how the Average MAPE was calculated using the M4 competition methodology, aggregating results over Yearly, Quarterly, and Monthly splits.\n\n5. **Additional Information**: \n   - The Kaggle competition was divided into Yearly and Quarterly/Monthly forecasting parts.\n   - Some participants only took part in the Quarterly/Monthly forecasting.\n   - The SaliMali team won the Quarterly/Monthly competition using a weighted ensemble of statistical methods.\n\nOverall, the section highlights the superior performance of the N-BEATS models in time series forecasting for the TOURISM dataset."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_21", "node_type": "4", "metadata": {"page_label": "22", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "8613e17c189112e311d44895a141e2c8c72a0e812fdd7f95e76850231ade2022", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nTable 15: Performance on the TOURISM test set, Average MAPE , aggregate over all forecast horizons\n(Yearly: 1-4, Quarterly: 1-8, Monthly: 1-24, Average: 1-24). Lower values are better. Red \u2013 second\nbest.\nYearly Quarterly Monthly Average\n(518) (427) (366) (1311)\nStatistical benchmarks (Athanasopoulos et al., 2011)\nSNa\u00efve 23.61 16.46 22.56 21.25\nTheta 23.45 16.15 22.11 20.88\nForePro 26.36 15.72 19.91 19.84\nETS 27.68 16.05 21.15 20.88\nDamped 28.15 15.56 23.47 22.26\nARIMA 28.03 16.23 21.13 20.96\nKaggle competitors (Athanasopoulos & Hyndman, 2011)\nSaliMali n/a 14.83 19.64 n/a\nLeeCBaker 22.73 15.14 20.19 19.35\nStratometrics 23.15 15.14 20.37 19.52\nRobert n/a 14.96 20.28 n/a\nIdalgo n/a 15.07 20.55 n/a\nN-BEATS-G (Ours) 21.67 14.71 19.17 18.47\nN-BEATS-I (Ours) 21.55 15.22 19.82 18.97\nN-BEATS-I+G (Ours) 21.44 14.78 19.29 18.52\nC.3 D ETAILED RESULTS :TOURISM DATASET\nDetailed results for the TOURISM competition dataset are provided in Table 15. The respective Kaggle\ncompetition was divided into two parts: (i) Yearly time series forecasting and (ii) Quarterly/Monthly\ntime series forecasting (Athanasopoulos & Hyndman, 2011). Some of the participants chose to\ntake part only in the second part. Therefore, In addition to entries present in Table 1, we report\ncompetitors from (Athanasopoulos & Hyndman, 2011) that have missing results in Yearly compe-\ntition. In particular, SaliMali team is the winner of the Quarterly/Monthly time series forecasting\ncompetition (Brierley, 2011). Their approach is based on a weighted ensemble of statistical methods.\nTeams Robert andIdalgo used unknown approaches. We can see from Table 15 that N-BEATS\nachieves state-of-the-art performance on all subsets of TOURISM dataset. On average, it is state of the\nart and it gains 4.2% over the best-known approach LeeCBaker , and 11.5% over auto-ARIMA.\nThe average metrics have not been reported in the original competition results (Athanasopoulos et al.,\n2011; Athanasopoulos & Hyndman, 2011). Therefore, in Table 15, we present the Average MAPE\nmetric calculated by us based on the previously reported Yearly, Quarterly and Monthly splits. To\ncalculate it, we follow the M4 competition methodology and compute the average metric as the\naverage over all time series and over all forecast horizons. Given the performance metric values\naggregated over Yearly, Quarterly and Monthly splits, the average can be computed straightforwardly\nas:\nMAPE Average =NYear\nNTotMAPE Year+NQuart\nNTotMAPE Quart+NMonth\nNTotMAPE Month. (6)\nHere NTot=NYear+NQuart+NMonth andNYear=4\u00d7518,NQuart=8\u00d7427,NMonth =24\u00d7366. It is\nclear that for each split, its Nis the product of its respective number of time series and its largest\nforecast horizon.\n22", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2753, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "891ff756-e87e-43bb-af2b-cf8524b6cb53": {"__data__": {"id_": "891ff756-e87e-43bb-af2b-cf8524b6cb53", "embedding": null, "metadata": {"page_label": "23", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section details an experiment comparing the performance of various forecasting models\u2014MatFact, DeepAR, Deep State, Deep Factors, and N-BEATS\u2014on the ELECTRICITY and TRAFFIC datasets. Key points include:\n\n1. **Datasets and Aggregation**:\n   - ELECTRICITY dataset is aggregated using the sum operation.\n   - TRAFFIC dataset is aggregated using the mean operation.\n   - Aggregation is done hourly, with specific handling of time points.\n\n2. **Data Preparation**:\n   - The first year of the ELECTRICITY dataset is removed to match previous studies.\n   - Efforts were made to match data points and splits used in previous research for comparability.\n\n3. **Challenges in Data Handling**:\n   - Different split points used by various papers create challenges in reproducibility.\n   - Specific difficulties were encountered with the TRAFFIC dataset, including matching dates and handling gaps due to holidays and anomalies.\n\n4. **List of Gaps**:\n   - A detailed list of holidays and anomaly days was provided, which were excluded from the dataset.\n\n5. **Metrics**:\n   - Performance is measured using Normalized Deviation (ND), equivalent to the p50 loss metric used in some models.\n\n### Key Entities:\n- **Models**: MatFact, DeepAR, Deep State, Deep Factors, N-BEATS\n- **Datasets**: ELECTRICITY, TRAFFIC\n- **Metrics**: Normalized Deviation (ND), p50 loss\n- **Dates and Events**: Specific holidays and anomaly days between Jan 1, 2008, and Mar 30, 2009\n\n### Key Topics:\n- Data aggregation methods\n- Data preparation and cleaning\n- Challenges in data handling and reproducibility\n- Performance metrics for forecasting models"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_22", "node_type": "4", "metadata": {"page_label": "23", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "bad89977c50ef8d685973153aace80955c940d97d0322f4684d83254628abc33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "326d5ef8-10b3-4593-8964-64c93d044057", "node_type": "1", "metadata": {}, "hash": "4ec371e7d927d19e51fe46fc39953c090df6f469325aeea8df45c00e66ebba4a", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nC.4 D ETAILED RESULTS :ELECTRICITY AND TRAFFIC DATASETS\nIn this experiment we are comparing the performances of MatFact (Yu et al., 2016), DeepAR (Flunkert\net al., 2017) (Amazon Labs), Deep State (Rangapuram et al., 2018a) (Amazon Labs), Deep Fac-\ntors (Wang et al., 2019) (Amazon Labs), and N-BEATS models on ELECTRICITY2(Dua & Graff,\n2017) and TRAFFIC3(Dua & Graff, 2017) datasets. The results are presented in in Table 16.\nBoth datasets are aggregated to hourly data, but using different aggregation operations: sum for\nELECTRICITY and mean for TRAFFIC . The hourly aggregation is done so that all the points available\nin(h\u22121 : 00,h: 00]hours are aggregated to hour h, thus if original dataset starts on 2011-01-01\n00:15 then the \ufb01rst time point after aggregation will be 2011-01-01 01:00. For the ELECTRICITY\ndataset we removed the \ufb01rst year from training set, to match the training set used in (Yu et al., 2016),\nbased on the aggregated dataset downloaded from, presumable authors\u2019, github repository4. We also\nmade sure that data points for both ELECTRICITY and TRAFFIC datasets after aggregation match\nthose used in (Yu et al., 2016). The authors of MatFact model were using the last 7 days of datasets\nas test set, but papers from Amazon are using different splits, where the split points are provided by a\ndate. Changing split points without a well grounded reason adds uncertainties to the comparability of\nthe models performances and creates challenges to the reproducibility of the results, thus we were\ntrying to match all different splits in our experiments. It was especially challenging on TRAFFIC\ndataset, where we had to use some heuristics to \ufb01nd records dates; the dataset authors state: \u201c The\nmeasurements cover the period from Jan. 1st 2008 to Mar. 30th 2009\u201d and \u201c We remove public\nholidays from the dataset, as well as two days with anomalies (March 8th 2009 and March 9th 2008)\nwhere all sensors were muted between 2:00 and 3:00 AM. \u201d , but we failed to match a part of the\nprovided labels of week days to actual dates. Therefore, we had to assume that the actual list of gaps,\nwhich include holidays and anomalous days, is the following:\n1. Jan. 1, 2008 (New Year\u2019s Day)\n2. Jan. 21, 2008 (Martin Luther King Jr. Day)\n3. Feb. 18, 2008 (Washington\u2019s Birthday)\n4. Mar. 9, 2008 (Anomaly day)\n5. May 26, 2008 (Memorial Day)\n6. Jul. 4, 2008 (Independence Day)\n7. Sep. 1, 2008 (Labor Day)\n8. Oct. 13, 2008 (Columbus Day)\n9. Nov. 11, 2008 (Veterans Day)\n10. Nov. 27, 2008 (Thanksgiving)\n11. Dec. 25, 2008 (Christmas Day)\n12. Jan. 1, 2009 (New Year\u2019s Day)\n13. Jan. 19, 2009 (Martin Luther King Jr. Day)\n14. Feb. 16, 2009 (Washington\u2019s Birthday)\n15. Mar. 8, 2009 (Anomaly day)\nThe \ufb01rst 6 gaps were con\ufb01rmed by the gaps in labels, but the rest were more than 1 day apart from any\npublic holiday of years 2008 and 2009 in San Francisco, California and US. More over the number of\ngaps we found in the labels provided by dataset authors is 10, while the number of days between Jan.\n1st 2008 and Mar. 30th 2009 is 455, assuming that Jan. 1st 2008 was skipped from the values and\nlabels we should end up with either 454 \u221210=444 instead of 440 days or different end date.\nThe metric is reported in Normalized deviation (ND) as in (Yu et al., 2016) which is equal to p50\nloss used in DeepAR, Deep State, and Deep Factors", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3374, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "326d5ef8-10b3-4593-8964-64c93d044057": {"__data__": {"id_": "326d5ef8-10b3-4593-8964-64c93d044057", "embedding": null, "metadata": {"page_label": "23", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary:\nThe section discusses the loss function used in the DeepAR, Deep State, and Deep Factors papers, specifically mentioning that it is equivalent to p50 loss. Additionally, it provides references to datasets and resources, including links to the Electricity Load Diagrams 2011-2014 dataset, the PEMS-SF dataset, and a GitHub repository for downloading data related to the TRMF (Temporal Regularized Matrix Factorization) experiments. The page label for this section is 23."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_22", "node_type": "4", "metadata": {"page_label": "23", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "bad89977c50ef8d685973153aace80955c940d97d0322f4684d83254628abc33", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "891ff756-e87e-43bb-af2b-cf8524b6cb53", "node_type": "1", "metadata": {"page_label": "23", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "0f6c8938a7fd975ce9a27395600d6f9f3cd05006e72740dc3db17eb617f0e064", "class_name": "RelatedNodeInfo"}}, "text": "which is equal to p50\nloss used in DeepAR, Deep State, and Deep Factors papers.\n2https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014\n3https://archive.ics.uci.edu/ml/datasets/PEMS-SF\n4https://github.com/rofuyu/exp-trmf-nips16/blob/master/python/exp-scripts/datasets/download-data.sh\n23", "mimetype": "text/plain", "start_char_idx": 3303, "end_char_idx": 3607, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8c3ee76-6f95-4cd0-8de3-5dc337da4d6d": {"__data__": {"id_": "f8c3ee76-6f95-4cd0-8de3-5dc337da4d6d", "embedding": null, "metadata": {"page_label": "24", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary:\nThis section, published as part of a conference paper at ICLR 2020, discusses the Normalized Deviation (ND) performance of various forecasting models on the ELECTRICITY and TRAFFIC test sets. The models compared include MatFact, DeepAR, Deep State, Deep Factors, and different variants of N-BEATS (N-BEATS-G, N-BEATS-I, and N-BEATS-I+G). The results show that N-BEATS models, which do not use any covariates like day-of-week or hour-of-day, generally perform well compared to other models. The N-BEATS architecture used in this experiment is consistent with its application in M4, M3, and TOURISM datasets, with adjustments only in history size and the number of iterations based on validation set performance. The validation set consists of 7 consecutive days before the test set, and the model is retrained on the training set, including the validation set, before being tested using a rolling window operation."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_23", "node_type": "4", "metadata": {"page_label": "24", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "69fcb9e027e724ffddc765c3be9f560c62e5f751a57940998167a65729555991", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nND=\u2211i,t|\u02c6Yit\u2212Yit|\n\u2211i,t|Yit|(7)\nTable 16: ND Performance on the ELECTRICITY and TRAFFIC test sets.\n1Split used in DeepAR (Flunkert et al., 2017) and Deep State (Rangapuram et al., 2018a).\n2Split used in Deep Factors (Wang et al., 2019).\n\u2020Numbers reported by (Flunkert et al., 2017), which are different from the original MatFact paper,\nhypothetically due to changed split point.\nELECTRICITY TRAFFIC\n2014-09-0112014-03-312last 7 days 2008-06-1512008-01-142last 7 days\nMatFact 0.16\u2020n/a 0.255 0.20\u2020n/a 0.187\nDeepAR 0.07 0.272 n/a 0.17 0.296 n/a\nDeep State 0.083 n/a n/a 0.167 n/a n/a\nDeep Factors n/a 0.112 n/a n/a 0.225 n/a\nN-BEATS-G (ours) 0.064 0.065 0.171 0.114 0.230 0.112\nN-BEATS-I (ours) 0.073 0.072 0.185 0.114 0.231 0.110\nN-BEATS-I+G (ours) 0.067 0.067 0.178 0.114 0.230 0.111\nContrary to Amazon models N-BEATS does not use any covariates, like day-of-week, hour-of-day,\netc.\nThe N-BEATS architecture used in this experiment is exactly the same as used in M4, M3 and\nTOURISM datasets, the only difference is history size and the number of iterations. These parameters\nwere chosen based on performance on validation set. Where the validation set consists of 7 consecutive\ndays right before the test set. After the parameters are chosen the model is retrained on training set\nwhich includes the validation set, then tested on test set. The model is trained once and tested on test\nset using rolling window operation described in (Yu et al., 2016).\n24", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1498, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40e7bb39-54b7-4699-b9e2-98de51550dac": {"__data__": {"id_": "40e7bb39-54b7-4699-b9e2-98de51550dac", "embedding": null, "metadata": {"page_label": "25", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary:\nThe section, published as a conference paper at ICLR 2020, presents detailed results comparing the performance of different forecasting models: DeepAR, DeepState, and N-BEATS. Specifically, it focuses on the ND (Normalized Deviation) performance metrics of these models on the M4-Hourly and TOURISM datasets. The results are summarized in Table 17, which shows that N-BEATS variants (N-BEATS-G, N-BEATS-I, and N-BEATS-I+G) generally outperform DeepAR and DeepState models across the datasets. The key entities discussed are the forecasting models (DeepAR, DeepState, N-BEATS) and the datasets (M4-Hourly, TOURISM Monthly, TOURISM Quarterly)."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_24", "node_type": "4", "metadata": {"page_label": "25", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "0640947aa998b3bd518df39205cbe5ecf5f5ba0115dc528300f913b428a20687", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nC.5 D ETAILED RESULTS :COMPARE TO DEEPAR, D EEPSTATE SPACE MODELS\nTable 17 compares ND (7) performance of DeepAR, DeepState models published in (Rangapuram\net al., 2018a) and N-BEATS.\nTable 17: ND Performance of DeepAR, Deep State Space, and N-BEATS models on M4-Hourly and\nTOURISM datasets\nM4 (Hourly) TOURISM (Monthly) TOURISM (Quarterly)\nDeepAR 0.09 0.107 0.11\nDeepState 0.044 0.138 0.098\nN-BEATS-G (ours) 0.023 0.097 0.080\nN-BEATS-I (ours) 0.027 0.103 0.079\nN-BEATS-I+G (ours) 0.025 0.099 0.077\n25", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 546, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da8a39c8-376e-4408-a15e-0ddefd38603e": {"__data__": {"id_": "da8a39c8-376e-4408-a15e-0ddefd38603e", "embedding": null, "metadata": {"page_label": "26", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section from the document \"n_beats.pdf\" primarily discusses the hyperparameter settings used for training models on different subsets of the M4, M3, and TOURISM datasets. Key topics and entities include:\n\n1. **Datasets and Subsets**:\n   - **M4, M3, TOURISM**: These are the datasets used.\n   - **Subsets**: Yearly (Yly), Quarterly (Qly), Monthly (Mly), Weekly (Wly), Daily (Dly), Hourly (Hly), and Other frequency subsets.\n\n2. **Model Configurations**:\n   - **N-BEATS-I**: Interpretable model configuration.\n   - **N-BEATS-G**: Generic model configuration.\n\n3. **Hyperparameters**:\n   - **LH (Lookback Horizon)**: Defines the length of training history used to generate training samples.\n   - **Iterations**: Number of batches used to train N-BEATS.\n   - **Losses**: Metrics used for model evaluation (e.g., sMAPE, MAPE, MASE).\n   - **S-width, S-blocks, S-block-layers, T-width, T-degree, T-blocks, T-block-layers**: Structural parameters for the models.\n   - **Sharing**: Indicates whether parameters are shared at the stack level.\n   - **Lookback period**: Time periods considered for lookback (e.g., 2H, 3H, etc.).\n   - **Batch size**: Number of samples per batch (1024).\n\n4. **Parameter Settings**:\n   - Detailed settings for both N-BEATS-I and N-BEATS-G across different subsets of the datasets.\n\n5. **Common Parameters**:\n   - Explanation of how LH is used to generate training samples.\n   - Observations on the relationship between the number of time series in subsets and the value of LH.\n\nThe section provides a comprehensive overview of the hyperparameter configurations and their implications for training the N-BEATS models on various time series datasets."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_25", "node_type": "4", "metadata": {"page_label": "26", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "3f7bd661f246e5eaf9573a00421d50fe251eda8c2bb01e9b2193a5575816f793", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nTable 18: Settings of hyperparameters across subsets of M4, M3, TOURISM datasets.\nM4 M3 TOURISM\nYly Qly Mly Wly Dly Hly Yly Qly Mly Other Yly Qly Mly\nParameter N-BEATS-I\nLH 1.5 1.5 1.5 10 10 10 20 5 5 20 20 10 20\nIterations 15K 15K 15K 5K 5K 5K 50 6K 6K 250 30 500 300\nLosses s MAPE /MAPE /MASE sMAPE /MAPE /MASE MAPE\nS-width 2048\nS-blocks 3\nS-block-layers 4\nT-width 256\nT-degree 2\nT-blocks 3\nT-block-layers 4\nSharing STACK LEVEL\nLookback period 2 H,3H,4H,5H,6H,7H\nBatch 1024\nParameter N-BEATS-G\nLH 1.5 1.5 1.5 10 10 10 20 20 20 10 5 10 20\nIterations 15K 15K 15K 5K 5K 5K 20 250 10K 250 30 100 100\nLosses s MAPE /MAPE /MASE sMAPE /MAPE /MASE MAPE\nWidth 512\nBlocks 1\nBlock-layers 4\nStacks 30\nSharing NO\nLookback period 2 H,3H,4H,5H,6H,7H\nBatch 1024\nD H YPER -PARAMETER SETTINGS\nTable 18 presents the hyperparameter settings used to train models on different subsets of M4, M3\nand TOURISM datasets. A brief discussion of \ufb01eld names in the table is warranted.\nSubset names Yly, Qly, Mly, Wly, Dly, Hly, Other correspond to yearly, quarterly, monthly, weekly,\ndaily, hourly and other frequency subsets de\ufb01ned in the original datasets.\nN-BEATS-I andN-BEATS-G correspond to the interpretable and generic model con\ufb01gurations\nde\ufb01ned in Section 3.3.\nD.1 C OMMON PARAMETERS\nLHis the coef\ufb01cient de\ufb01ning the length of training history immediately preceding the last point in\nthe train part of the TS that is used to generate training samples. For example, if for M4 Yearly the\nforecast horizon is 6 and LHis 1.5, then we consider 1.5\u00b76=9most recent points in the train dataset\nfor each time series to generate training samples. A training sample from a given TS in M4 Yearly is\nthen generated by choosing one of the most recent 9 points as an anchor. All the points preceding the\nanchor are used to create the input to N-BEATS, while the points following and including the anchor\nbecome training target. Target and history points that fall outside of the time series limits given the\nanchor position are \ufb01lled with zeros and masked during the training. We observed that for subsets\nwith large number of time series LHtends to be smaller and for subsets with smaller number of time\nseries it tends to be larger. For example, in massive Yearly, Monthly, Quarterly subsets of M4 LHis\nequal to 1 .5; and in moderate to small Weekly, Daily, Hourly subsets of M4 LHis equal to 10.\nIterations is the number of batches used to train N-BEATS.\n26", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2469, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "697afe0f-89f1-4a89-82d4-45fb7719586e": {"__data__": {"id_": "697afe0f-89f1-4a89-82d4-45fb7719586e", "embedding": null, "metadata": {"page_label": "27", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThis section from the N-BEATS paper, published at ICLR 2020, discusses various parameters and configurations used in the N-BEATS model, specifically focusing on the ensemble building, model sharing, lookback period, and batch size. It also details the parameters for two variants of the N-BEATS model: N-BEATS-I (interpretable) and N-BEATS-G (generic).\n\n#### Key Topics and Entities:\n\n1. **Loss Functions**:\n   - Different loss functions are used to build ensembles.\n   - Mixing models trained on various metrics improved performance for M4 and M3 datasets.\n   - For the TOURISM dataset, training only on MAPE yielded the best validation scores.\n\n2. **Sharing**:\n   - Defines whether coefficients in fully-connected layers are shared.\n   - Interpretable model performs best with shared weights across stacks.\n   - Generic model performs best with no shared weights.\n\n3. **Lookback Period**:\n   - Length of the history window forming the input to the model.\n   - Function of the forecast horizon length (H).\n   - Models with lookback periods of 2H, 3H, 4H, 5H, 6H, and 7H were mixed in one ensemble.\n\n4. **Batch Size**:\n   - Batch size used was 1024.\n   - Larger batch sizes sped up training, but gains were minimal beyond 1024.\n\n#### N-BEATS-I Parameters:\n- **S-width**: Width of fully connected layers in the seasonality stack.\n- **S-blocks**: Number of blocks in the seasonality stack.\n- **S-block-layers**: Number of fully-connected layers in one block of the seasonality stack.\n- **T-width**: Width of fully connected layers in the trend stack.\n- **T-degree**: Degree of polynomial in the trend stack.\n- **T-blocks**: Number of blocks in the trend stack.\n- **T-block-layers**: Number of fully-connected layers in one block of the trend stack.\n\n#### N-BEATS-G Parameters:\n- **Width**: Width of fully connected layers in the blocks.\n- **Blocks**: Number of blocks in the stack.\n- **Block-layers**: Number of fully-connected layers in one block of the stack.\n\nThe section provides detailed insights into the configurations and parameters that influence the performance and structure of the N-BEATS model, highlighting the differences between the interpretable and generic variants."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_26", "node_type": "4", "metadata": {"page_label": "27", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "e49069465918053d9c734f150950805577112f151878cd4963a473f516dd56cd", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nLosses is the set of loss functions that is used to build ensemble. We observed on the respective\nvalidation sets that for M4 and M3 mixing models trained on a variety of metrics resulted in\nperformance gain. In the case of TOURISM dataset training only on MAPE led to the best validation\nscores.\nSharing de\ufb01nes whether the coef\ufb01cients in the fully-connected layers are shared. We observed that\nthe interpretable model works best when weights are shared across stack, while generic model works\nbest when none of the weights are shared.\nLookback period is the length of the history window forming the input to the model (please refer to\nFigure 1). This is the function of the forecast horizon length, H. In our experiments we mixed models\nwith lookback periods 2H,3H,4H,5H,6H,7Hin one ensemble. As an example, for a forecast\nhorizon length H=8and a lookback period 7H, the model\u2019s input will consist of the history window\nof 7\u00b78=56 samples.\nBatch is the batch size. We used batch size of 1024. We observed that the training was faster with\nlarger batch sizes, however in our setup little gain was observed with batch sizes beyond 1024.\nD.2 N-BEATS-I PARAMETERS\nS-width is the width of the fully connected layers in the blocks comprising the seasonality stack of\nthe interpretable model (please refer to Figure 1).\nS-blocks is the number of blocks comprising the seasonality stack of the interpretable model (please\nrefer to Figure 1).\nS-block-layers is the number of fully-connected layers comprising one block in the seasonality\nstack of the interpretable model (preceding the \ufb01nal fully-connected projection layers forming the\nbackcast/forecast fork, please refer to Figure 1).\nT-width is the width of the fully connected layers in the blocks comprising the trend stack of the\ninterpretable model (please refer to Figure 1).\nT-degree is the degree pof polynomial in the trend stack of the interpretable model (please refer to\nequation (2)).\nT-blocks is the number of blocks comprising the trend stack of the interpretable model (please refer\nto Figure 1).\nT-block-layers is the number of fully-connected layers comprising one block in the trend stack\nof the interpretable model (preceding the \ufb01nal fully-connected projection layers forming the back-\ncast/forecast fork, please refer to Figure 1).\nD.3 N-BEATS-G PARAMETERS\nWidth is the width of the fully connected layers in the blocks comprising the stacks of the generic\nmodel (please refer to Figure 1).\nBlocks is the number of blocks comprising the stack of the generic model (please refer to Figure 1).\nBlock-layers is the number of fully-connected layers comprising one block in the stack of the generic\nmodel (preceding the \ufb01nal fully-connected projection layers forming the backcast/forecast fork,\nplease refer to Figure 1).\n27", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2830, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0022fdcf-c7a0-4568-bb48-cda98a48edf9": {"__data__": {"id_": "0022fdcf-c7a0-4568-bb48-cda98a48edf9", "embedding": null, "metadata": {"page_label": "28", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section presents a series of graphs and figures illustrating the performance of generic and interpretable configurations of a forecasting model on the M4 dataset. Each row in the figures represents a different time series example, categorized by data frequency: Yearly, Quarterly, Monthly, Weekly, Daily, and Hourly. The magnitudes in each row are normalized by the maximal value of the actual time series for convenience. The columns in the figures show:\n\n- **Column (a):** Actual values (ACTUAL), generic model forecast (FORECAST-G), and interpretable model forecast (FORECAST-I).\n- **Column (b):** STACK1-G (first stack of the generic model).\n- **Column (c):** STACK2-G (second stack of the generic model).\n- **Column (d):** STACK1-I (first stack of the interpretable model).\n- **Column (e):** STACK2-I (second stack of the interpretable model).\n\nThe figures aim to compare the outputs of the generic and interpretable configurations across different time series frequencies, providing insights into their forecasting performance."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_27", "node_type": "4", "metadata": {"page_label": "28", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "31bf0539f1df03ef70a244ca9f764c90aa7fe3cbe638f004a40d5053747ace3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "956d8558-3ac8-46c3-ace8-69856f9ba622", "node_type": "1", "metadata": {}, "hash": "30dbcd68ffeeabf798c3454b33561b2a5932585c8ca83fa20e648277e15f2e05", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\n0 1 2 3 4 5\nt0.80.91.0\nACTUAL\nFORECAST-I\nFORECAST-G\n0 1 2 3 4 5\nt0.800.850.90STACK1-G\n0 1 2 3 4 5\nt0.0250.0500.075STACK2-G\n0 1 2 3 4 5\nt0.800.850.900.95\nSTACK1-I\n0 1 2 3 4 5\nt0.020.030.040.05STACK2-I\n0 2 4 6\nt0.850.900.951.00\nACTUAL\nFORECAST-I\nFORECAST-G\n0 2 4 6\nt0.860.880.90STACK1-G\n0 2 4 6\nt0.025\n0.0000.0250.050\nSTACK2-G\n0 2 4 6\nt0.880.890.90STACK1-I\n0 2 4 6\nt0.05\n0.000.05STACK2-I\n0 5 10 15\nt0.40.60.81.0\nACTUAL\nFORECAST-I\nFORECAST-G\n0 5 10 15\nt0.80.9 STACK1-G\n0 5 10 15\nt0.1\n0.0\nSTACK2-G\n0 5 10 15\nt0.850.90STACK1-I\n0 5 10 15\nt0.3\n0.2\n0.1\n0.0STACK2-I\n0 2 4 6 8 10 12\nt0.60.81.0\nACTUAL\nFORECAST-I\nFORECAST-G\n0 2 4 6 8 10 12\nt0.650.700.750.80\nSTACK1-G\n0 2 4 6 8 10 12\nt0.0000.0250.0500.075\nSTACK2-G\n0 2 4 6 8 10 12\nt0.650.700.750.80STACK1-I\n0 2 4 6 8 10 12\nt0.000.020.04STACK2-I\n0.0 2.5 5.0 7.5 10.0 12.5\nt0.960.981.00\nACTUAL\nFORECAST-I\nFORECAST-G\n0.0 2.5 5.0 7.5 10.0 12.5\nt0.9740.976STACK1-G\n0.0 2.5 5.0 7.5 10.0 12.5\nt0.002\n0.001\nSTACK2-G\n0.0 2.5 5.0 7.5 10.0 12.5\nt0.9740.976STACK1-I\n0.0 2.5 5.0 7.5 10.0 12.5\nt0.0003\n0.0002\n0.0001\nSTACK2-I\n0 10 20 30 40\nt0.250.500.751.00\nACTUAL\nFORECAST-I\nFORECAST-G\n(a) Combined\n0 10 20 30 40\nt0.20.40.6\nSTACK1-G (b) Stack1-G\n0 10 20 30 40\nt0.02\n0.00\nSTACK2-G (c) Stack2-G\n0 10 20 30 40\nt0.360.380.40\nSTACK1-I (d) StackT-I\n0 10 20 30 40\nt0.2\n0.00.2\nSTACK2-I (e) StackS-I\nFigure 5: The outputs of generic and the interpretable con\ufb01gurations, M4 dataset. Each row is one\ntime series example per data frequency, top to bottom (Yearly: id Y3974, Quarterly: id Q11588,\nMonthly: id M19006, Weekly: id W246, Daily: id D404, Hourly: id H344). The magnitudes in a row\nare normalized by the maximal value of the actual time series for convenience. Column (a) shows the\nactual values (ACTUAL), the generic model forecast (FORECAST-G) and the interpretable model\nforecast", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1848, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "956d8558-3ac8-46c3-ace8-69856f9ba622": {"__data__": {"id_": "956d8558-3ac8-46c3-ace8-69856f9ba622", "embedding": null, "metadata": {"page_label": "28", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary:\nThe section discusses the comparison between the generic model forecast (FORECAST-G) and the interpretable model forecast (FORECAST-I) in the context of time series forecasting. It highlights the outputs of different stacks within these models: Stack1-G and Stack2-G for the generic model, and the Trend (StackT-I) and Seasonality (StackS-I) stacks for the interpretable model. The section aims to show that despite differences in individual stack outputs, the combined forecasts (FORECAST-G and FORECAST-I) can be very similar. Detailed numeric traces of the signals visualized in Figure 2 are provided in Tables 19-24, demonstrating the summation properties of the stacks and the overall similarity between FORECAST-G and FORECAST-I."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_27", "node_type": "4", "metadata": {"page_label": "28", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "31bf0539f1df03ef70a244ca9f764c90aa7fe3cbe638f004a40d5053747ace3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0022fdcf-c7a0-4568-bb48-cda98a48edf9", "node_type": "1", "metadata": {"page_label": "28", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "15293d487f59058d4efb613c1e4d98f17daab85dd193c269c34547e9f315afba", "class_name": "RelatedNodeInfo"}}, "text": "(ACTUAL), the generic model forecast (FORECAST-G) and the interpretable model\nforecast (FORECAST-I). Columns (b) and (c) show the outputs of stacks 1 and 2 of the generic model,\nrespectively; FORECAST-G is their summation. Columns (d) and (e) show the output of the Trend\nand the Seasonality stacks of the interpretable model, respectively; FORECAST-I is their summation.\nE D ETAILED SIGNAL TRACES OF INTERPRETABLE INPUTS PRESENTED IN\nFIGURE 2\nThe goal of this section is to show the detailed traces (numeric values) of signals visualized in Fig. 2.\nThis is to demonstrate that even though the StackT-I (Fig. 2 (d)) and StackS-I (Fig. 2 (e)) provide\nresponse lines different from the counterparts in Stack1-G (Fig. 2 (b)) and Stack2-G (Fig. 2 (c)), the\nsummations in the combined line (Fig. 2 (a)) can still be very similar.\nFirst, we reproduce Fig. 5 for the convenience of the reader. Second, for each row in the \ufb01gure, we\nproduce a table showing the numeric values of each signal depicted in corresponding plots (please\nrefer to Tables 19\u2013 24). We make sure that the names of signals in \ufb01gure legends and in the table\ncolumns match, such that they can easily be cross-referenced. It can be clearly seen in Tables 19\u2013 24\nthat (i) traces STACK1-I and STACK2-I sum up to trace FORECAST-I, (ii) traces STACK1-G and\nSTACK2-G sum up to trace FORECAST-G, (iii) traces FORECAST-I and FORECAST-G are overall\nvery similar even though their components may signi\ufb01cantly differ from each other.\n28", "mimetype": "text/plain", "start_char_idx": 1762, "end_char_idx": 3249, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82db0eb3-2e6f-4ac1-baec-e58704ee8706": {"__data__": {"id_": "82db0eb3-2e6f-4ac1-baec-e58704ee8706", "embedding": null, "metadata": {"page_label": "29", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section presents detailed traces of signals for different time series data, corresponding to specific figures and rows in a referenced figure (Fig. 5). The data is categorized into three types of time series: Yearly, Quarterly, and Monthly, each identified by unique IDs (Y3974, Q11588, and M19006 respectively). For each time series, the tables provide actual values and various forecast values (FORECAST-I, FORECAST-G) along with intermediate stack values (STACK1-I, STACK2-I, STACK1-G, STACK2-G) at different time points (t).\n\n#### Key Topics:\n1. **Time Series Analysis**: The section focuses on analyzing time series data.\n2. **Forecasting**: It includes different forecast models (FORECAST-I, FORECAST-G).\n3. **Stacked Models**: Intermediate stack values (STACK1-I, STACK2-I, STACK1-G, STACK2-G) are provided.\n4. **Data Representation**: Detailed numerical data is presented in tabular form.\n\n#### Key Entities:\n1. **Time Series Types**:\n   - Yearly (ID: Y3974)\n   - Quarterly (ID: Q11588)\n   - Monthly (ID: M19006)\n2. **Forecast Models**:\n   - FORECAST-I\n   - FORECAST-G\n3. **Stack Values**:\n   - STACK1-I\n   - STACK2-I\n   - STACK1-G\n   - STACK2-G\n4. **Time Points (t)**: Specific time points at which data is recorded.\n\nThe section is part of a conference paper published at ICLR 2020 and provides a detailed numerical analysis of the forecasting performance for different time series data."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_28", "node_type": "4", "metadata": {"page_label": "29", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "e7afbca94f319047d03adc1ded39392b2ba8424d9ec1ff482824ad3328692be8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2298b389-60ff-41ff-9f2f-8849aaecf3e6", "node_type": "1", "metadata": {}, "hash": "a771f6700272257073cf7f5b986d45ec717fe70a93400175e45b0d237ea2716e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nTable 19: Detailed traces of signals depicted in row 1 of Fig. 5, corresponding to the time series\nYearly: id Y3974.\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\nt\n0 0.780182 0.802068 0.806608 0.781290 0.020778 0.801294 0.005314\n1 0.802337 0.829223 0.841406 0.798422 0.030801 0.825271 0.016135\n2 0.840317 0.863683 0.883136 0.820196 0.043487 0.853114 0.030022\n3 0.889376 0.905962 0.929258 0.850250 0.055712 0.880833 0.048425\n4 0.930521 0.947028 0.967846 0.892221 0.054807 0.904393 0.063453\n5 0.976414 0.982307 1.000000 0.949748 0.032559 0.921360 0.078640\nTable 20: Detailed traces of signals depicted in row 2 of Fig. 5, corresponding to the time series\nQuarterly: id Q11588.\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\nt\n0 0.830068 0.835964 0.829417 0.880435 -0.044471 0.852018 -0.022601\n1 0.927155 0.898949 0.891168 0.881626 0.017324 0.880124 0.011044\n2 0.979204 0.957379 0.948799 0.882549 0.074831 0.907149 0.041650\n3 0.857250 0.900612 0.891967 0.883830 0.016782 0.877959 0.014008\n4 0.895082 0.857230 0.847029 0.886096 -0.028866 0.852232 -0.005204\n5 0.981590 0.923832 0.911001 0.889972 0.033860 0.881140 0.029861\n6 1.000000 0.978128 0.965236 0.896085 0.082043 0.907475 0.057761\n7 0.910528 0.920632 0.915460 0.905062 0.015571 0.886941 0.028519\nTable 21: Detailed traces of signals depicted in row 3 of Fig. 5, corresponding to the time series\nMonthly: id M19006.\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\nt\n0 1.000000 0.923394 0.928279 0.944660 -0.021266 0.922835 0.005444\n1 0.865248 0.822588 0.829924 0.937575 -0.114987 0.867619 -0.037695\n2 0.638298 0.693820 0.717119 0.930295 -0.236475 0.810818 -0.093699\n3 0.531915 0.594375 0.612377 0.922890 -0.328515 0.757199 -0.144823\n4 0.468085 0.579403 0.595221 0.915428 -0.336025 0.747151 -0.151930\n5 0.539007 0.602615 0.620809 0.907977 -0.305362 0.755078 -0.134269\n6 0.581560 0.653387 0.682669 0.900606 -0.247219 0.774561 -0.091891\n7 0.666667 0.747440 0.765814 0.893385 -0.145945 0.799594 -0.033781\n8 0.737589 0.817883", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2071, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2298b389-60ff-41ff-9f2f-8849aaecf3e6": {"__data__": {"id_": "2298b389-60ff-41ff-9f2f-8849aaecf3e6", "embedding": null, "metadata": {"page_label": "29", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary:\nThe section appears to contain numerical data, likely related to a performance evaluation or experimental results. The data includes various metrics across different instances or iterations, labeled from 8 to 17. Each row contains multiple values, possibly representing different performance indicators or measurements. The numbers seem to be in a tabular format, with each column representing a specific metric. The last number, 29, could be a page number or another identifier. The context suggests this might be part of a research paper or technical document, possibly related to machine learning or statistical analysis, given the file name \"n_beats.pdf.\""}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_28", "node_type": "4", "metadata": {"page_label": "29", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "e7afbca94f319047d03adc1ded39392b2ba8424d9ec1ff482824ad3328692be8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82db0eb3-2e6f-4ac1-baec-e58704ee8706", "node_type": "1", "metadata": {"page_label": "29", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "4d0834d394639aa6462498cad113d7fd670c4a8b72ee9905e0435017c0b93740", "class_name": "RelatedNodeInfo"}}, "text": "-0.033781\n8 0.737589 0.817883 0.835577 0.886382 -0.068498 0.817218 0.018359\n9 0.765957 0.862568 0.856962 0.879665 -0.017097 0.822099 0.034862\n10 0.851064 0.873448 0.880074 0.873304 0.000145 0.833473 0.046601\n11 0.893617 0.878186 0.871103 0.867367 0.010819 0.829537 0.041566\n12 0.858156 0.834448 0.853549 0.861923 -0.027475 0.816527 0.037022\n13 0.695035 0.785341 0.776687 0.857040 -0.071699 0.782536 -0.005850\n14 0.446809 0.662443 0.697788 0.852789 -0.190345 0.745623 -0.047835\n15 0.382979 0.623196 0.624614 0.849236 -0.226040 0.711553 -0.086939\n16 0.453901 0.598511 0.625150 0.846451 -0.247941 0.712130 -0.086980\n17 0.539007 0.668231 0.652175 0.844504 -0.176272 0.716925 -0.064750\n29", "mimetype": "text/plain", "start_char_idx": 2042, "end_char_idx": 2725, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f1d4f20-a765-40f1-b0a6-6c1f8f7448b1": {"__data__": {"id_": "5f1d4f20-a765-40f1-b0a6-6c1f8f7448b1", "embedding": null, "metadata": {"page_label": "30", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section provides detailed traces of signals for two specific time series datasets, Weekly (id W246) and Daily (id D404), as depicted in rows 4 and 5 of Figure 5 from the paper published at ICLR 2020. The tables (Table 22 and Table 23) list the actual values and various forecast values (FORECAST-I, FORECAST-G) along with intermediate stack values (STACK1-I, STACK2-I, STACK1-G, STACK2-G) for different time steps (t).\n\n#### Key Topics:\n1. **Time Series Analysis**: The section focuses on the analysis of time series data, specifically weekly and daily datasets.\n2. **Forecasting**: It includes forecast values generated by different models or methods.\n3. **Signal Traces**: Detailed traces of signals are provided, showing how the actual values compare with forecasted values and intermediate stack values.\n\n#### Key Entities:\n1. **Time Series IDs**: W246 (Weekly), D404 (Daily)\n2. **Forecast Values**: FORECAST-I, FORECAST-G\n3. **Intermediate Stack Values**: STACK1-I, STACK2-I, STACK1-G, STACK2-G\n4. **Actual Values**: Represented as \"ACTUAL\"\n5. **Time Steps**: Represented as \"t\" ranging from 0 to 12 for Weekly and 0 to 10 for Daily.\n\nThe section is highly data-centric, providing numerical insights into the performance and behavior of forecasting models over specified time periods."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_29", "node_type": "4", "metadata": {"page_label": "30", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "8a50e4ca279501aac359ecc3d50913394066c6fbd221de8fe6b167e04ede20f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc2b9c8b-0307-4334-ae4a-289c97e92026", "node_type": "1", "metadata": {}, "hash": "03ccad671f7a66be8e7cbbfbe36d7fe2664895329547ed29253b75c526b872b5", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nTable 22: Detailed traces of signals depicted in row 4 of Fig. 5, corresponding to the time series\nWeekly: id W246.\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\nt\n0 0.630056 0.629703 0.625108 0.639236 -0.009534 0.625416 -0.000309\n1 0.607536 0.643509 0.639846 0.647549 -0.004039 0.639592 0.000254\n2 0.641731 0.656171 0.652584 0.656696 -0.000526 0.643665 0.008919\n3 0.628783 0.669636 0.661163 0.666739 0.002897 0.652107 0.009056\n4 0.816799 0.687287 0.683860 0.677738 0.009549 0.662176 0.021683\n5 0.817020 0.709211 0.717187 0.689752 0.019459 0.686589 0.030598\n6 0.766724 0.731732 0.742824 0.702841 0.028891 0.705234 0.037590\n7 0.770320 0.750834 0.755154 0.717066 0.033768 0.716986 0.038167\n8 0.794113 0.769671 0.778460 0.732487 0.037184 0.731113 0.047347\n9 0.874011 0.793373 0.810332 0.749164 0.044209 0.750939 0.059392\n10 1.000000 0.816386 0.847545 0.767157 0.049229 0.776405 0.071140\n11 0.979251 0.834532 0.858604 0.786526 0.048006 0.783939 0.074665\n12 0.933160 0.850010 0.866116 0.807332 0.042678 0.792134 0.073982\nTable 23: Detailed traces of signals depicted in row 5 of Fig. 5, corresponding to the time series\nDaily: id D404.\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\nt\n0 0.968704 0.972314 0.971950 0.972589 -0.000275 0.972964 -0.001014\n1 0.954319 0.972637 0.972131 0.972808 -0.000171 0.972822 -0.000690\n2 0.954599 0.972972 0.972188 0.973060 -0.000088 0.973798 -0.001610\n3 0.959959 0.973230 0.972140 0.973341 -0.000112 0.973686 -0.001546\n4 0.975472 0.973481 0.972125 0.973649 -0.000168 0.974060 -0.001934\n5 0.970391 0.973715 0.972174 0.973979 -0.000264 0.974800 -0.002626\n6 0.977728 0.974056 0.972403 0.974328 -0.000272 0.974368 -0.001965\n7 0.985624 0.974445 0.972428 0.974693 -0.000248 0.973870 -0.001442\n8 0.979695 0.974823 0.972567 0.975069 -0.000246 0.974870 -0.002303\n9 0.985345 0.975079 0.973089 0.975455 -0.000376 0.975970 -0.002881\n10 0.983088 0.975547 0.973881 0.975845 -0.000298 0.975796", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1989, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc2b9c8b-0307-4334-ae4a-289c97e92026": {"__data__": {"id_": "cc2b9c8b-0307-4334-ae4a-289c97e92026", "embedding": null, "metadata": {"page_label": "30", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary:\nThe section appears to contain numerical data, likely related to performance metrics or experimental results. Key entities include numerical values and potential identifiers such as \"11,\" \"12,\" and \"13,\" which could represent different data points or iterations. The data includes values such as \"0.973881,\" \"0.975845,\" and \"1.000000,\" among others, which might correspond to specific measurements or calculations. The context suggests a focus on precision or accuracy metrics, given the format and the presence of small decimal differences. The page label is 30, and the document is named \"n_beats.pdf,\" indicating it might be related to a study or analysis involving the N-BEATS model or a similar forecasting method."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_29", "node_type": "4", "metadata": {"page_label": "30", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "8a50e4ca279501aac359ecc3d50913394066c6fbd221de8fe6b167e04ede20f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f1d4f20-a765-40f1-b0a6-6c1f8f7448b1", "node_type": "1", "metadata": {"page_label": "30", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "d51c4784be0122c02c59474766838ace22dd5c2579fda4bb54483cd89c381c69", "class_name": "RelatedNodeInfo"}}, "text": "0.973881 0.975845 -0.000298 0.975796 -0.001915\n11 0.983368 0.975991 0.974537 0.976238 -0.000247 0.976757 -0.002220\n12 0.998312 0.976365 0.974924 0.976628 -0.000263 0.977579 -0.002655\n13 1.000000 0.976821 0.975291 0.977013 -0.000193 0.977213 -0.001922\n30", "mimetype": "text/plain", "start_char_idx": 1953, "end_char_idx": 2206, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c82aee7a-594a-4708-9ad7-f89c0a84b4a6": {"__data__": {"id_": "c82aee7a-594a-4708-9ad7-f89c0a84b4a6", "embedding": null, "metadata": {"page_label": "31", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section presents detailed traces of signals for a specific time series identified as \"Hourly: id H344,\" which is depicted in row 6 of Figure 5 in the document. The data is organized in a table (Table 24) and includes various columns representing actual values and different forecast models (FORECAST-I, FORECAST-G) as well as stack values (STACK1-I, STACK2-I, STACK1-G, STACK2-G) over a series of time points (t). The table provides numerical values for each of these categories at different time intervals, illustrating the performance and comparison of the forecast models against the actual data.\n\n### Key Topics and Entities:\n- **Time Series Data**: Specifically for \"Hourly: id H344.\"\n- **Forecast Models**: FORECAST-I and FORECAST-G.\n- **Stack Values**: STACK1-I, STACK2-I, STACK1-G, STACK2-G.\n- **Actual Values**: Represented in the column labeled \"ACTUAL.\"\n- **Time Points (t)**: Sequential time intervals from 0 to 25.\n- **Numerical Data**: Detailed values for each category at each time point.\n\nThe section is part of a conference paper published at ICLR 2020 and provides a granular view of the performance of different forecasting models in comparison to actual observed values."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_30", "node_type": "4", "metadata": {"page_label": "31", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "5e60f40a6bd46457966402dfdeec2a92e02896fc1a026af4351df78541c11680", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f1440b0-4581-4f95-8c77-0c01ef8d1b2b", "node_type": "1", "metadata": {}, "hash": "831b69f67c17ac94a9556138a1d87a79ba124ba8d97fa2956e62b5a1f92b52b0", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2020\nTable 24: Detailed traces of signals depicted in row 6 of Fig. 5, corresponding to the time series\nHourly: id H344.\nACTUAL FORECAST-I FORECAST-G STACK1-I STACK2-I STACK1-G STACK2-G\nt\n0 0.226804 0.256799 0.277159 0.346977 -0.090179 0.280489 -0.003329\n1 0.175258 0.228913 0.234605 0.347615 -0.118701 0.241790 -0.007185\n2 0.164948 0.209208 0.207347 0.348265 -0.139057 0.218575 -0.011228\n3 0.164948 0.197360 0.193084 0.348928 -0.151568 0.208458 -0.015374\n4 0.216495 0.190397 0.186586 0.349606 -0.159209 0.205701 -0.019115\n5 0.195876 0.194204 0.189433 0.350297 -0.156094 0.214399 -0.024966\n6 0.319588 0.221026 0.216221 0.351004 -0.129978 0.241574 -0.025353\n7 0.226804 0.279857 0.276414 0.351726 -0.071869 0.293580 -0.017167\n8 0.371134 0.357292 0.359372 0.352464 0.004828 0.364392 -0.005020\n9 0.536082 0.438540 0.446126 0.353218 0.085322 0.442703 0.003423\n10 0.711340 0.511441 0.519928 0.353989 0.157452 0.510142 0.009787\n11 0.752577 0.571604 0.578186 0.354777 0.216827 0.571596 0.006590\n12 0.783505 0.617085 0.618778 0.355584 0.261501 0.613425 0.005353\n13 0.773196 0.651777 0.655123 0.356409 0.295368 0.649259 0.005864\n14 0.618557 0.670202 0.676814 0.357253 0.312950 0.669555 0.007260\n15 0.793814 0.679884 0.692592 0.358116 0.321768 0.684208 0.008384\n16 0.793814 0.672488 0.696440 0.359000 0.313488 0.684764 0.011676\n17 0.680412 0.648851 0.677696 0.359904 0.288947 0.662714 0.014983\n18 0.525773 0.602496 0.630922 0.360828 0.241667 0.620368 0.010554\n19 0.505155 0.537698 0.552296 0.361775 0.175923 0.552599 -0.000304\n20 0.701031 0.463760 0.466442 0.362743 0.101016 0.477429 -0.010987\n21 0.484536 0.395795 0.390958 0.363734 0.032061 0.408708 -0.017750\n22 0.247423 0.337809 0.338500 0.364748 -0.026939 0.354028 -0.015528\n23 0.371134 0.292452 0.303902 0.365786 -0.073334 0.312588 -0.008686\n24 0.216495 0.254359 0.258435 0.366848 -0.112489 0.270568 -0.012133\n25 0.412371 0.227557 0.224291", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1923, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f1440b0-4581-4f95-8c77-0c01ef8d1b2b": {"__data__": {"id_": "0f1440b0-4581-4f95-8c77-0c01ef8d1b2b", "embedding": null, "metadata": {"page_label": "31", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "The section appears to contain a series of numerical data points, likely representing some form of time series or sequential measurements. The data includes multiple columns, each with different values, possibly indicating various metrics or parameters over a sequence of steps or time intervals. The key topics and entities in this section include:\n\n1. **Numerical Data Points**: The section lists numerical values across multiple columns, which could represent different metrics or parameters.\n2. **Sequential Measurements**: The data is organized in a sequence, possibly indicating time steps or iterations (e.g., 26, 27, 28, etc.).\n3. **Metrics/Parameters**: Each row contains multiple values, suggesting that several metrics or parameters are being tracked simultaneously.\n4. **Trends and Changes**: The values in the columns change over the sequence, indicating trends or variations in the measured metrics.\n\nSummary:\nThe section presents a series of numerical data points organized sequentially, likely representing various metrics or parameters over time or iterations. The data shows trends and changes in these metrics across the sequence."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "data/n_beats.pdf_part_30", "node_type": "4", "metadata": {"page_label": "31", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "5e60f40a6bd46457966402dfdeec2a92e02896fc1a026af4351df78541c11680", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c82aee7a-594a-4708-9ad7-f89c0a84b4a6", "node_type": "1", "metadata": {"page_label": "31", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16"}, "hash": "495303120b2b62c3b87d1808852c892c91940edd5d532ef1fc132e56c17ef654", "class_name": "RelatedNodeInfo"}}, "text": "0.412371 0.227557 0.224291 0.367934 -0.140377 0.237846 -0.013555\n26 0.237113 0.207962 0.201250 0.369046 -0.161084 0.219420 -0.018169\n27 0.206186 0.196049 0.189439 0.370183 -0.174133 0.209743 -0.020304\n28 0.206186 0.189030 0.182843 0.371346 -0.182316 0.207727 -0.024884\n29 0.237113 0.194524 0.185734 0.372536 -0.178011 0.213194 -0.027460\n30 0.206186 0.220227 0.215444 0.373753 -0.153526 0.242485 -0.027041\n31 0.329897 0.279614 0.274624 0.374998 -0.095383 0.292834 -0.018210\n32 0.371134 0.355078 0.358020 0.376270 -0.021193 0.365332 -0.007312\n33 0.494845 0.437103 0.445832 0.377572 0.059531 0.441323 0.004510\n34 0.690722 0.509515 0.520006 0.378903 0.130612 0.512064 0.007942\n35 0.989691 0.570761 0.579003 0.380263 0.190497 0.569851 0.009152\n36 1.000000 0.615868 0.623981 0.381654 0.234214 0.617254 0.006728\n37 0.845361 0.651487 0.656782 0.383076 0.268411 0.650336 0.006446\n38 0.742268 0.670664 0.678412 0.384528 0.286136 0.673055 0.005357\n39 0.721649 0.680534 0.691961 0.386013 0.294521 0.684347 0.007614\n40 0.567010 0.671607 0.692853 0.387530 0.284078 0.683297 0.009555\n41 0.546392 0.648851 0.672476 0.389079 0.259771 0.660613 0.011863\n42 0.432990 0.599785 0.621940 0.390662 0.209123 0.615426 0.006514\n43 0.391753 0.537520 0.544543 0.392279 0.145241 0.549961 -0.005417\n44 0.443299 0.462772 0.457700 0.393930 0.068842 0.471080 -0.013380\n45 0.422680 0.397098 0.380324 0.395616 0.001482 0.401229 -0.020905\n46 0.381443 0.342213 0.325583 0.397337 -0.055124 0.347827 -0.022244\n47 0.257732 0.297711 0.287130 0.399094 -0.101384 0.304270 -0.017140\n31", "mimetype": "text/plain", "start_char_idx": 1897, "end_char_idx": 3437, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"0ce73017-f27e-4f71-9929-f0e5e6c1c453": {"doc_hash": "baedec5da6da097e6b2bcb14cdc57716229b64ce08a938a9c704a10e7511ca58", "ref_doc_id": "data/n_beats.pdf_part_0"}, "e29e45bd-fbc9-4f16-9667-b86a917e4e69": {"doc_hash": "832c48fef13fb44e92e2816ba1b6f4d9c6f944b365056014f1c7966481b713e9", "ref_doc_id": "data/n_beats.pdf_part_1"}, "002bfe86-9bd0-4104-b8fd-a707a74ac352": {"doc_hash": "95e5f23afcad99bf21927d036911a8abf3ea08f2d19d6306a3f5ee14d13dea83", "ref_doc_id": "data/n_beats.pdf_part_1"}, "a9d7d0a2-44a4-4da9-b3aa-5511ef2482b5": {"doc_hash": "3cbef230647304cb0dcb4d04f18499a0049e8fbd4f667bb15ecb57948a2c070e", "ref_doc_id": "data/n_beats.pdf_part_2"}, "c9d41cf9-2096-49da-99b2-3d2b025213b9": {"doc_hash": "72080e917237350a54845a50c5be6f57eddc0be29fd0a49bf17cbf6ee863401a", "ref_doc_id": "data/n_beats.pdf_part_3"}, "7de37615-f700-400a-8d29-5cb9dd7ae8ab": {"doc_hash": "21fbef6f764630491e5dbe3817a08af90c9c79e8fd0900e5e855b45d005a13ee", "ref_doc_id": "data/n_beats.pdf_part_3"}, "23540b6f-b665-4ea7-b47a-16ded4c88f24": {"doc_hash": "0ee73b9c92807edfcb875427483295d39aa0ecf792f79694407e9ad716ad2a78", "ref_doc_id": "data/n_beats.pdf_part_4"}, "5b2b3328-37fe-4a99-8584-6a3075514fc3": {"doc_hash": "0b85c0173470a75ee3012fa42751b5b7895f5c53d77549b7a530732619bc6b42", "ref_doc_id": "data/n_beats.pdf_part_4"}, "13695662-913c-4328-b8eb-b715c0b9d841": {"doc_hash": "53bf65a566d7736b4840a12bb4cb31247f072b5a7fc2253cef2cbb3f96b8aaef", "ref_doc_id": "data/n_beats.pdf_part_5"}, "c928c3fe-77b7-4389-a7c7-5fb90715bd44": {"doc_hash": "dffedd31200a1dc368276b930a30c81146c6f5f604128ab518bd3cd4a77c1414", "ref_doc_id": "data/n_beats.pdf_part_5"}, "6f60ab8c-bcd7-4414-8c1e-d46f7f9f69a7": {"doc_hash": "7802dd490010260e32f87ff02892f344dcf7bbb70212b1de9ac7b8553c800890", "ref_doc_id": "data/n_beats.pdf_part_6"}, "a4f1517d-d166-41f4-914f-6f786a41562c": {"doc_hash": "cc8541a22fae43517622a7e44a0efb3b1e345bb6d714e8f89deea75990b679bc", "ref_doc_id": "data/n_beats.pdf_part_6"}, "88868241-866f-4b83-b09b-9af6d31d5ced": {"doc_hash": "528c10f74f00a38988ba93c3affbadc81fb6e556863d174e8969d136d333838e", "ref_doc_id": "data/n_beats.pdf_part_7"}, "17a6f5b5-7bff-4bbd-a974-d6d61c9346a7": {"doc_hash": "7eb6d86c303fd3c24a55ae078b80d041163450c71bf3d4534058f094e3417048", "ref_doc_id": "data/n_beats.pdf_part_7"}, "c6e46701-f118-4e1e-8c0b-6ff2db986182": {"doc_hash": "8016088fd5ee62f26064cca82420c2dcb85bfc0c4d7ae90bec6ba9f7e7ff90b0", "ref_doc_id": "data/n_beats.pdf_part_8"}, "56532a7f-ba08-4b2e-a2b0-fa53bc555c9b": {"doc_hash": "4a0b72c4f7d83005da735e65f111b083988956cd14bdb2919a6016c01710f8df", "ref_doc_id": "data/n_beats.pdf_part_8"}, "d97989ac-2c82-4c04-b3a2-0f1cb698362b": {"doc_hash": "b2e0ac0ab84610815ac2120fc64bc80814d43dfbd05156ab4843435b3c08292e", "ref_doc_id": "data/n_beats.pdf_part_9"}, "6ce1a22b-3d6f-442b-8f05-7d1f86ab4153": {"doc_hash": "dbed2ed8e550f62e3ee6f39fe8bbcde8085ddba8635615de67d5926ebc961bc7", "ref_doc_id": "data/n_beats.pdf_part_10"}, "5fceb242-60ea-4e4c-8ad7-44d5cfabeb74": {"doc_hash": "777b3925f0da6292f3a3c6c0f28c89b6ee4351bd2e35318d42731b1a59c8acf0", "ref_doc_id": "data/n_beats.pdf_part_11"}, "778044aa-ad89-4dcc-a73b-0067ea674ceb": {"doc_hash": "739d96085b0205ebb87ebf057d022dbee9a73712cb5112c9e76ca311068160ba", "ref_doc_id": "data/n_beats.pdf_part_12"}, "ae6b3568-0f6f-460b-82b0-9efec80d6402": {"doc_hash": "d6840bca860aae0c5544551c647f48f65ca57db169e59b4137fe3d135e743d1b", "ref_doc_id": "data/n_beats.pdf_part_13"}, "f58e6968-bee1-4597-93bc-f7225760642e": {"doc_hash": "e87733c60bcf0eb7b5b2073c316a8ec367d9fde9fc5b42c576a20f7c6bd19732", "ref_doc_id": "data/n_beats.pdf_part_14"}, "b480e810-917e-46f6-a546-0525660b8293": {"doc_hash": "0d95972e86eefd089d18a63929bc1152729601b148ad35478652e21c64cc4839", "ref_doc_id": "data/n_beats.pdf_part_15"}, "d2f1be17-0961-48eb-a916-72dfb50220da": {"doc_hash": "076a1c29a6d1d239220d908f8dee59b06c3d49c8aa176f481a8859b119cddc4a", "ref_doc_id": "data/n_beats.pdf_part_16"}, "f8c9dd29-8ce2-4b77-b757-f1e627a59bc1": {"doc_hash": "fbac357268b996c3be2f33372a323d24a25879f5cd812b8215b2b0613e2bf570", "ref_doc_id": "data/n_beats.pdf_part_17"}, "38f09dd6-6aa0-4e1b-861b-5951c618ebee": {"doc_hash": "6edd2c7751aa799a91fd449e1493f74feeaf9abb1df6d1e727a73a129d8bea67", "ref_doc_id": "data/n_beats.pdf_part_18"}, "b0ba3c88-afc7-40c4-8e66-ccb74692fdc3": {"doc_hash": "f9c1ad2d3cc3a4b7bb8e212a9f634fafa07629a279eafc8ededa6c999f30a39c", "ref_doc_id": "data/n_beats.pdf_part_18"}, "f3e4e04c-1705-4346-adfe-5319cfc2bf39": {"doc_hash": "7a00a5a49abe86ac8025195f3117970fef7930da57d20fc845b7b78329d24686", "ref_doc_id": "data/n_beats.pdf_part_19"}, "53368c95-b561-405f-acfc-d7ca3a02e4bf": {"doc_hash": "6e69f7d03be58e4f58272aa91c0bea0eadce02d73258cf597cf0ca4b70f5fff0", "ref_doc_id": "data/n_beats.pdf_part_20"}, "a4ca3105-6443-47af-801c-fea94043106b": {"doc_hash": "4bd7a164078e5bc5db1de341c9ef7a8c295d83041b381e1fa5434d714dd2c56f", "ref_doc_id": "data/n_beats.pdf_part_21"}, "891ff756-e87e-43bb-af2b-cf8524b6cb53": {"doc_hash": "8e3c479afc4cf22b2c11738edddd03b607650af81d27115ecdf521ea1ee1ceb4", "ref_doc_id": "data/n_beats.pdf_part_22"}, "326d5ef8-10b3-4593-8964-64c93d044057": {"doc_hash": "4b089f328c551e703428b1846b806349630d92ec3fb00c774f78493db39bb758", "ref_doc_id": "data/n_beats.pdf_part_22"}, "f8c3ee76-6f95-4cd0-8de3-5dc337da4d6d": {"doc_hash": "0c5a5b30d5f7057bd2b060ae27666d17d1215536a0b6957285ea05cc9ac079f9", "ref_doc_id": "data/n_beats.pdf_part_23"}, "40e7bb39-54b7-4699-b9e2-98de51550dac": {"doc_hash": "b238ef6153af39552a01285f5713dbcb4f1d2f35eab7286f85adee052a80860a", "ref_doc_id": "data/n_beats.pdf_part_24"}, "da8a39c8-376e-4408-a15e-0ddefd38603e": {"doc_hash": "4a717d4fa1af9fca27e7b2fc30d21f2e2c103121c83d6835e07573f00bdcf747", "ref_doc_id": "data/n_beats.pdf_part_25"}, "697afe0f-89f1-4a89-82d4-45fb7719586e": {"doc_hash": "55327fa2226558457a8da022a72df7222e35399014413f1440201ca91d28f271", "ref_doc_id": "data/n_beats.pdf_part_26"}, "0022fdcf-c7a0-4568-bb48-cda98a48edf9": {"doc_hash": "fbbd47c3303998e44fc840b1f0f9b525cba4d852d7f58ab2072b31a28ad9a536", "ref_doc_id": "data/n_beats.pdf_part_27"}, "956d8558-3ac8-46c3-ace8-69856f9ba622": {"doc_hash": "c0f49b8ea28160fd336fef14fc14427cf0fe0420a07d5e90b6d55b96fb5aa880", "ref_doc_id": "data/n_beats.pdf_part_27"}, "82db0eb3-2e6f-4ac1-baec-e58704ee8706": {"doc_hash": "89c241aa352910cbfc07a7355a077877bfd573fb84f1954a8a6a301c2baa00fa", "ref_doc_id": "data/n_beats.pdf_part_28"}, "2298b389-60ff-41ff-9f2f-8849aaecf3e6": {"doc_hash": "f78564b82ce978cf333ffff7c647d09195ca05addc21a7c8e057c91dbf608ab8", "ref_doc_id": "data/n_beats.pdf_part_28"}, "5f1d4f20-a765-40f1-b0a6-6c1f8f7448b1": {"doc_hash": "4d5934a22d064e7262a04ba1c093cb1970e236946874fdef634fe4b52510f2c1", "ref_doc_id": "data/n_beats.pdf_part_29"}, "cc2b9c8b-0307-4334-ae4a-289c97e92026": {"doc_hash": "e7dd5507e935fe5f804fc0d7800cfea3a54931cbbe56021b191f6431da361772", "ref_doc_id": "data/n_beats.pdf_part_29"}, "c82aee7a-594a-4708-9ad7-f89c0a84b4a6": {"doc_hash": "f8d51636defa23277094a207657061cf5105e1f8e72989c580df28387c939158", "ref_doc_id": "data/n_beats.pdf_part_30"}, "0f1440b0-4581-4f95-8c77-0c01ef8d1b2b": {"doc_hash": "f7b7a2c1f3c798bc84fa01ffc4800b37d507b89fe5a4f1b820d42ab86aaf8ace", "ref_doc_id": "data/n_beats.pdf_part_30"}}, "docstore/ref_doc_info": {"data/n_beats.pdf_part_0": {"node_ids": ["0ce73017-f27e-4f71-9929-f0e5e6c1c453"], "metadata": {"page_label": "1", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section introduces the N-BEATS model, a deep learning architecture designed for univariate time series forecasting. Key topics and entities include:\n\n1. **Authors and Affiliations**:\n   - Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados (Element AI)\n   - Yoshua Bengio (Mila)\n\n2. **Problem Focus**:\n   - Univariate time series point forecasting using deep learning.\n\n3. **Proposed Solution**:\n   - A deep neural architecture featuring backward and forward residual links and a deep stack of fully-connected layers.\n   - The architecture is interpretable, versatile across various domains, and efficient in training.\n\n4. **Performance and Testing**:\n   - Tested on datasets like M3, M4, and TOURISM.\n   - Achieved state-of-the-art performance, improving forecast accuracy by 11% over statistical benchmarks and 3% over the previous M4 competition winner.\n\n5. **Model Configurations**:\n   - The first configuration does not use time-series-specific components, suggesting that deep learning primitives like residual blocks can effectively solve diverse forecasting problems.\n\n6. **Interpretability**:\n   - The architecture can be augmented for interpretability without significant loss in accuracy.\n\n7. **Context and Motivation**:\n   - Time series forecasting is crucial for various business applications with significant financial implications.\n   - Despite the success of deep learning in fields like computer vision and NLP, it has struggled in time series forecasting compared to classical statistical methods.\n   - The M4 competition highlighted the effectiveness of hybrid models combining neural networks with classical statistical methods.\n   - This work challenges the notion that hybrid approaches are necessary, exploring the potential of pure deep learning architectures for time series forecasting.\n\n8. **Research Question**:\n   - Can pure deep learning architectures provide accurate and interpretable time series forecasts without relying on hybrid methods?\n\n### Key Entities:\n- **N-BEATS Model**: The proposed deep learning architecture.\n- **Datasets**: M3, M4, TOURISM.\n- **Authors**: Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio.\n- **Institutions**: Element AI, Mila.\n- **Competitions**: M4 competition.\n- **Benchmark Models**: Statistical models, hybrid models (e.g., Smyl's model combining LSTM and Holt-Winters).\n\n### Key Topics:\n- Time series forecasting\n- Deep learning architectures\n- Residual links\n- Fully-connected layers\n- Model interpretability\n- Performance benchmarking\n- Hybrid vs. pure deep learning models"}}, "data/n_beats.pdf_part_1": {"node_ids": ["e29e45bd-fbc9-4f16-9667-b86a917e4e69", "002bfe86-9bd0-4104-b8fd-a707a74ac352"], "metadata": {"page_label": "2", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section discusses the development and evaluation of a deep learning (DL) architecture for time series (TS) forecasting, known as N-BEATS. Key topics and entities include:\n\n1. **Deep Neural Architecture**:\n   - The paper claims to be the first to empirically demonstrate that a pure DL model, without time-series-specific components, outperforms traditional statistical methods on datasets like M3, M4, and TOURISM.\n   - The DL model showed significant improvements over statistical benchmarks and competition winners, providing a proof of concept for using pure ML in TS forecasting.\n\n2. **Interpretable DL for Time Series**:\n   - The architecture is designed to produce interpretable outputs, similar to traditional decomposition techniques like the \"seasonality-trend-level\" approach.\n\n3. **Problem Statement**:\n   - The focus is on univariate point forecasting in discrete time.\n   - The task involves predicting future values based on a given observed series history.\n   - Common metrics for evaluating forecasting performance are discussed, including sMAPE, MAPE, MASE, and OWA.\n\n4. **N-BEATS Architecture**:\n   - The architecture is designed to be simple, generic, and expressive without relying on time-series-specific feature engineering or input scaling.\n   - The architecture aims to be extendable for human interpretability.\n\n5. **Basic Block**:\n   - The basic building block of the architecture has a fork design.\n   - The operation of the \u2113-th block is detailed, focusing on its input and output vectors.\n\n### Key Entities:\n- **Datasets**: M3, M4, TOURISM\n- **Metrics**: sMAPE, MAPE, MASE, OWA\n- **Techniques**: Seasonality-trend-level decomposition\n- **Architecture**: N-BEATS, basic block design\n\nThe section emphasizes the novelty and effectiveness of the N-BEATS architecture in TS forecasting, highlighting its accuracy and interpretability."}}, "data/n_beats.pdf_part_2": {"node_ids": ["a9d7d0a2-44a4-4da9-b3aa-5511ef2482b5"], "metadata": {"page_label": "3", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section describes the architecture of a proposed neural network model for time series forecasting, which was published as a conference paper at ICLR 2020. The key components and concepts include:\n\n1. **Basic Building Block**: A multi-layer fully connected (FC) network with ReLU nonlinearities. Each block predicts basis expansion coefficients for both forward (forecast) and backward (backcast) directions.\n\n2. **Stack Organization**: Blocks are organized into stacks using a doubly residual stacking principle, allowing for the construction of a deep neural network with interpretable outputs. Each stack aggregates forecasts hierarchically.\n\n3. **Input and Output**: The model input is a lookback window, and the output is a forecast period. The input window is typically a multiple of the forecast horizon \\( H \\), ranging from \\( 2H \\) to \\( 7H \\).\n\n4. **Block Operation**: Each block has two outputs: a forward forecast (\\( \\hat{y}_\\ell \\)) and a backcast (\\( \\hat{x}_\\ell \\)). The block consists of two parts:\n   - **Fully Connected Network**: Produces forward (\\( \\theta_f \\)) and backward (\\( \\theta_b \\)) expansion coefficients.\n   - **Basis Layers**: Maps these coefficients to outputs via basis functions, producing the backcast (\\( \\hat{x}_\\ell \\)) and forecast (\\( \\hat{y}_\\ell \\)).\n\n5. **Equations**: The section provides detailed equations describing the operations within each block, including the use of linear projection layers and ReLU non-linearities.\n\n### Key Entities:\n- **ICLR 2020**: Conference where the paper was published.\n- **FC Stack**: Fully connected layers used in the network.\n- **Forecast and Backcast**: Forward and backward predictions made by the network.\n- **Stack Residual**: Residual connections between stacks.\n- **Lookback Window**: Model input.\n- **Forecast Period**: Model output.\n- **Horizon \\( H \\)**: Forecast horizon.\n- **Expansion Coefficients (\\( \\theta_f \\), \\( \\theta_b \\))**: Coefficients predicted by the network.\n- **Basis Layers (\\( g_f \\), \\( g_b \\))**: Layers that map coefficients to outputs.\n- **ReLU**: Non-linearity used in the fully connected layers.\n\nThis architecture aims to optimize forecasting accuracy by effectively utilizing basis functions and residual connections."}}, "data/n_beats.pdf_part_3": {"node_ids": ["c9d41cf9-2096-49da-99b2-3d2b025213b9", "7de37615-f700-400a-8d29-5cb9dd7ae8ab"], "metadata": {"page_label": "4", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section discusses the architecture and interpretability of the N-BEATS model, a neural network for time series forecasting, presented at ICLR 2020. Key topics and entities include:\n\n1. **Forecast and Backcast Basis Vectors**:\n   - **vf** and **vb**: Basis vectors for forecast and backcast.\n   - **\u03b8f\u2113,i**: Expansion coefficients for these vectors.\n   - Functions **gb\u2113** and **gf\u2113**: Provide rich sets of basis vectors, which can be learnable or set to specific functional forms to reflect problem-specific inductive biases.\n\n2. **Doubly Residual Stacking**:\n   - Inspired by classical residual networks and DenseNet architectures.\n   - Introduces a novel hierarchical doubly residual topology with two residual branches: one for backcast prediction and one for forecast prediction.\n   - Equations describing the operation:\n     - \\( x\u2113 = x\u2113\u22121 \u2212 \u02c6x\u2113\u22121 \\)\n     - \\( \u02c6y = \u2211 \u2113 \u02c6y\u2113 \\)\n   - Facilitates gradient backpropagation and hierarchical decomposition of forecasts.\n\n3. **Interpretability**:\n   - Two configurations: generic deep learning (DL) and interpretable with inductive biases.\n   - **Generic DL**: Uses linear projections for **gb\u2113** and **gf\u2113**.\n     - Outputs described as:\n       - \\( \u02c6y\u2113 = Vf\u2113\u03b8f\u2113 + bf\u2113 \\)\n       - \\( \u02c6x\u2113 = Vb\u2113\u03b8b\u2113 + bb\u2113 \\)\n     - **Vf\u2113**: Matrix with dimensions H\u00d7dim(\u03b8f\u2113), interpreted as waveforms in the time domain.\n\nThe section emphasizes the model's ability to decompose forecasts hierarchically and its potential for interpretability through structured configurations."}}, "data/n_beats.pdf_part_4": {"node_ids": ["23540b6f-b665-4ea7-b47a-16ded4c88f24", "5b2b3328-37fe-4a99-8584-6a3075514fc3"], "metadata": {"page_label": "5", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThis section of the paper, published at ICLR 2020, discusses the architecture and methodology of the N-BEATS model, focusing on trend and seasonality modeling, as well as ensembling techniques.\n\n1. **Trend Model**:\n   - **Characteristics**: Trend is typically a monotonic or slowly varying function.\n   - **Modeling Approach**: Constrains the forecast function to be a polynomial of small degree \\( p \\), ensuring it varies slowly across the forecast window.\n   - **Mathematical Representation**: \n     - Partial forecast: \\( \\hat{y}_{s,\\ell} = \\sum_{i=0}^{p} \\theta^f_{s,\\ell,i} t^i \\)\n     - Matrix form: \\( \\hat{y}^{tr}_{s,\\ell} = T \\theta^f_{s,\\ell} \\)\n     - \\( \\theta^f_{s,\\ell} \\) are polynomial coefficients predicted by a fully connected (FC) network.\n\n2. **Seasonality Model**:\n   - **Characteristics**: Seasonality is a regular, cyclical, recurring fluctuation.\n   - **Modeling Approach**: Uses Fourier series to model periodic functions.\n   - **Mathematical Representation**:\n     - Partial forecast: \\( \\hat{y}_{s,\\ell} = \\sum_{i=0}^{\\lfloor H/2-1 \\rfloor} \\theta^f_{s,\\ell,i} \\cos(2\\pi it) + \\theta^f_{s,\\ell,i+\\lfloor H/2 \\rfloor} \\sin(2\\pi it) \\)\n     - Matrix form: \\( \\hat{y}^{seas}_{s,\\ell} = S \\theta^f_{s,\\ell} \\)\n     - \\( \\theta^f_{s,\\ell} \\) are Fourier coefficients predicted by a FC network.\n\n3. **Interpretable Architecture**:\n   - Consists of two stacks: trend stack followed by seasonality stack.\n   - Uses doubly residual stacking and forecast/backcast principle.\n   - Each stack has several blocks with residual connections, sharing non-learnable functions \\( g^b_{s,\\ell} \\) and \\( g^f_{s,\\ell} \\).\n   - Number of blocks is 3 for both trend and seasonality stacks.\n   - Sharing all weights across blocks in a stack improves validation performance.\n\n4. **Ensembling**:\n   - Essential for top performance in competitions like M4.\n   - Found to be a more powerful regularization technique compared to dropout or L2-norm penalty.\n   - Core property: diversity.\n   - Ensemble models are fit on three different metrics: sMAPE, MASE, and MAPE.\n   - Individual models are trained on input windows of different lengths for each horizon \\( H \\).\n\n### Key Entities:\n- **N-BEATS Model**: Neural Basis Expansion Analysis Time Series model.\n- **Trend and Seasonality**: Key components modeled using polynomial and Fourier series respectively.\n- **Ensembling**: Technique used to improve model performance by leveraging diversity.\n- **Metrics**: sMAPE, MASE, MAPE.\n- **Fully Connected (FC) Network**: Used to predict polynomial and Fourier coefficients.\n- **Residual Connections**: Used within stacks to improve model performance."}}, "data/n_beats.pdf_part_5": {"node_ids": ["13695662-913c-4328-b8eb-b715c0b9d841", "c928c3fe-77b7-4389-a7c7-5fb90715bd44"], "metadata": {"page_label": "6", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section primarily discusses the performance of various time series (TS) forecasting models on three datasets: M4, M3, and TOURISM. Key topics and entities include:\n\n1. **Performance Metrics and Datasets**:\n   - **Datasets**: M4 (100,000 time series), M3 (3,003 time series), and TOURISM (1,311 time series).\n   - **Evaluation Metrics**: sMAPE, OWA for M4; sMAPE for M3; MAPE for TOURISM.\n   - **Performance Table**: Shows the performance of different models on these datasets, with lower values indicating better performance.\n\n2. **Models Compared**:\n   - **Pure ML**: Machine Learning models.\n   - **Statistical**: Traditional statistical models like ETS, Theta.\n   - **ProLogistica**: A specific statistical model.\n   - **ML/TS Combination**: Hybrid models combining machine learning and time series methods.\n   - **DL/TS Hybrid**: Deep learning models combined with time series methods.\n   - **N-BEATS**: Variants of the N-BEATS model (N-BEATS-G, N-BEATS-I, N-BEATS-I+G) which show superior performance.\n\n3. **Related Work**:\n   - **Statistical Approaches**: Exponential smoothing, Theta method, ARIMA, auto-ARIMA.\n   - **ML/TS Combination Approaches**: Use outputs of statistical engines as features, including top entries in the M4 competition.\n   - **Deep Learning Approaches**: Variations of recurrent neural networks (RNNs), including combinations with dilation, residual connections, and attention mechanisms.\n   - **Hybrid Models**: The winning entry of the M4 competition, which combines Holt-Winters style seasonality model with deep learning techniques.\n\n4. **Experimental Results**:\n   - **Datasets**: Detailed descriptions of M4, M3, and TOURISM datasets.\n   - **Comparison**: Results compared with the best entries reported in the literature for each dataset using customary metrics.\n\n### Key Entities:\n- **Datasets**: M4, M3, TOURISM.\n- **Models**: Pure ML, Statistical, ProLogistica, ML/TS Combination, DL/TS Hybrid, N-BEATS (N-BEATS-G, N-BEATS-I, N-BEATS-I+G).\n- **Metrics**: sMAPE, OWA, MAPE.\n- **Competitions**: M4 competition, M3 competition.\n- **Techniques**: Exponential smoothing, Theta method, ARIMA, auto-ARIMA, recurrent neural networks (RNNs), gradient boosted tree, Holt-Winters style seasonality model, dilation, residual connections, attention mechanisms."}}, "data/n_beats.pdf_part_6": {"node_ids": ["6f60ab8c-bcd7-4414-8c1e-d46f7f9f69a7", "a4f1517d-d166-41f4-914f-6f786a41562c"], "metadata": {"page_label": "7", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section discusses the performance of the N-BEATS model in time series forecasting, comparing it against various benchmarks and datasets. Key topics and entities include:\n\n1. **Benchmark Models and Competitions**:\n   - **M4 Competition**: N-BEATS is compared against several models, including the M4 winner (DL/TS hybrid by Smyl, 2020), Pure ML (B. Trotta), Statistical (N.Z. Legaki and K. Koutsouri), ML/TS combination (P. Montero-Manso et al.), and ProLogistica.\n   - **M3 Dataset**: Comparisons include the Theta method (Assimakopoulos & Nikolopoulos, 2000), DOTA (Fiorucci et al., 2016), EXP (Spiliotis et al., 2019), and ForecastPro (Athanasopoulos et al., 2011).\n   - **TOURISM Dataset**: Benchmarks include ETS, Theta method, ForePro, and top entries from the TOURISM Kaggle competition (Stratometrics and LeeCBaker).\n\n2. **Performance and Results**:\n   - N-BEATS demonstrates state-of-the-art performance across three datasets (M4, M3, TOURISM), outperforming both generic and manually crafted models.\n   - The model uses minimal prior knowledge, no feature engineering, scaling, or TS-specific internal components, indicating that deep learning can perform well without statistical approaches or domain-specific feature engineering.\n\n3. **Datasets**:\n   - **M4 Dataset**: Contains 100k time series from various domains with different sampling frequencies and seasonalities.\n   - **M3 Dataset**: Comprises 3003 time series, historically significant for the development of optimal statistical models.\n   - **TOURISM Dataset**: Released as part of a Kaggle competition, used for benchmarking in the study.\n\n4. **Conclusion**:\n   - The results suggest that deep learning models like N-BEATS do not require support from statistical methods or hand-crafted features to achieve high performance in time series forecasting tasks.\n\n### Key Entities:\n- **N-BEATS Model**\n- **M4 Competition**\n- **M3 Dataset**\n- **TOURISM Dataset**\n- **Benchmark Models**: DL/TS hybrid, Pure ML, Statistical, ML/TS combination, ProLogistica, Theta method, DOTA, EXP, ForecastPro, ETS, ForePro, Stratometrics, LeeCBaker.\n- **Researchers and Contributors**: B. Trotta, N.Z. Legaki, K. Koutsouri, P. Montero-Manso, T. Talagala, R.J. Hyndman, G. Athanasopoulos, Smyl, Assimakopoulos, Nikolopoulos, Fiorucci, Spiliotis, Baker, Howard, Makridakis, Hibon."}}, "data/n_beats.pdf_part_7": {"node_ids": ["88868241-866f-4b83-b09b-9af6d31d5ced", "17a6f5b5-7bff-4bbd-a974-d6d61c9346a7"], "metadata": {"page_label": "8", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThis section of the N-BEATS paper, published at ICLR 2020, discusses the implementation, training, and interpretability of the N-BEATS model for time series forecasting. Key topics and entities include:\n\n1. **Implementation and Training**:\n   - N-BEATS is implemented in TensorFlow.\n   - Parameters are shared across different forecasting horizons, allowing one model per horizon for each dataset.\n   - The architecture and hyperparameters (e.g., width, number of layers, number of stacks) are consistent across horizons and datasets.\n   - Training involves sampling batches of size 1024 and using the Adam optimizer with an initial learning rate of 0.001.\n   - Early stopping is used, and training duration varies between 30 minutes to 2 hours depending on settings and hardware.\n\n2. **Generalization**:\n   - The architecture generalizes well across different time series datasets, including the M4 Monthly subset (48k time series) and the M3 Others subset (174 time series).\n   - This generalization is stronger compared to previous models like S. Smyl's, which required different architectures for different horizons.\n\n3. **Interpretability**:\n   - The model can be made interpretable by constraining the first stack to polynomial form and the second stack to Fourier basis form.\n   - Outputs of the interpretable model show distinct trend and seasonality components, with trends being monotonic and seasonality being cyclical.\n   - The magnitude of outputs varies based on the presence of trend or seasonality in the time series.\n   - The interpretable architecture effectively decomposes forecasts into trend and seasonality components without compromising performance.\n\n4. **Training Details**:\n   - The length of historical data (LH) used for training varies based on the number of time series in the dataset.\n   - For large subsets, LH is smaller, while for smaller subsets, LH is larger.\n\n5. **Evaluation**:\n   - The model's performance is evaluated using the sMAPE metric, with gradient flows stopped in the denominator for numerical stability.\n\nOverall, the section highlights the robustness, generalization capability, and interpretability of the N-BEATS model in time series forecasting."}}, "data/n_beats.pdf_part_8": {"node_ids": ["c6e46701-f118-4e1e-8c0b-6ff2db986182", "56532a7f-ba08-4b2e-a2b0-fa53bc555c9b"], "metadata": {"page_label": "9", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section presents a series of graphs illustrating the performance of generic and interpretable configurations of a forecasting model on the M4 dataset. The key topics and entities include:\n\n1. **Forecasting Models**: The section compares two types of forecasting models\u2014generic (FORECAST-G) and interpretable (FORECAST-I).\n2. **Data Frequencies**: The models are evaluated across various data frequencies, including yearly, quarterly, monthly, weekly, daily, and hourly time series.\n3. **Stacked Models**: The outputs of different stacked models (STACK1-G, STACK2-G, STACK1-I, STACK2-I) are shown for each data frequency.\n4. **Normalization**: The magnitudes in each row are normalized by the maximal value of the actual time series for convenience.\n5. **Visualization**: The graphs in column (a) display the actual values, the generic model forecast, and the interpretable model forecast. Subsequent columns show the outputs of the stacked models.\n\nThe section aims to provide a visual comparison of the forecasting accuracy and behavior of the different model configurations across various time series data frequencies."}}, "data/n_beats.pdf_part_9": {"node_ids": ["d97989ac-2c82-4c04-b3a2-0f1cb698362b"], "metadata": {"page_label": "10", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section primarily discusses the conclusions of a study on a novel architecture for univariate time series (TS) forecasting, presented at ICLR 2020. Key points include:\n\n1. **Architecture and Performance**: The proposed architecture is general, flexible, and performs well across various TS forecasting problems. It was tested on three challenging datasets (M4, M3, and TOURISM) and achieved state-of-the-art performance in both generic and interpretable configurations.\n\n2. **Hypotheses Validated**:\n   - The generic deep learning (DL) approach is highly effective for heterogeneous univariate TS forecasting without requiring domain-specific knowledge.\n   - It is feasible to constrain a DL model to produce human-interpretable outputs by decomposing its forecasts.\n\n3. **Multi-task Learning**: The study demonstrated that DL models could be trained on multiple time series in a multi-task fashion, allowing for successful transfer and sharing of individual learnings.\n\n4. **Speculation on Meta-Learning**: The authors speculate that the performance of N-BEATS may be partly due to a form of meta-learning, suggesting this as a topic for future research.\n\n5. **References**: The section includes a list of references, citing works related to machine learning, forecasting methods, and specific models like TensorFlow, the theta model, and XGBoost.\n\n### Key Entities:\n- **N-BEATS**: The novel architecture proposed for univariate TS forecasting.\n- **Datasets**: M4, M3, and TOURISM.\n- **Deep Learning (DL)**: The approach used for forecasting.\n- **Meta-Learning**: A potential factor contributing to the model's performance.\n- **References**: Various authors and works cited, including TensorFlow, the theta model, and XGBoost."}}, "data/n_beats.pdf_part_10": {"node_ids": ["6ce1a22b-3d6f-442b-8f05-7d1f86ab4153"], "metadata": {"page_label": "11", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "The section primarily consists of a list of references cited in a conference paper published at ICLR 2020. The key topics and entities mentioned include:\n\n1. **Forecasting Methods and Models**:\n   - Theta method and state space models (Fiorucci et al., 2016).\n   - DeepAR for probabilistic forecasting (Flunkert et al., 2017).\n   - Exponentially weighted averages for forecasting trends and seasonals (Holt, 1957, 2004).\n   - Automatic time series forecasting with the forecast package for R (Hyndman and Khandakar, 2008).\n   - Feature-based Forecast Model Averaging (FFORMA) (Montero-Manso et al., 2019).\n\n2. **Deep Learning and Neural Networks**:\n   - Deep residual learning for image recognition (He et al., 2016).\n   - Densely connected convolutional networks (Huang et al., 2017).\n   - Residual LSTM for distant speech recognition (Kim et al., 2017).\n   - Rectified linear units (ReLU) in restricted Boltzmann machines (Nair and Hinton, 2010).\n   - Dual-stage attention-based recurrent neural network for time series prediction (Qin et al., 2017).\n\n3. **Forecasting Competitions and Datasets**:\n   - M4 dataset and competition (M4 Team, 2018a, 2018b).\n   - M3-Competition results and implications (Makridakis and Hibon, 2000).\n   - Accuracy of extrapolation methods from a forecasting competition (Makridakis et al., 1982).\n   - M4-Competition results and findings (Makridakis et al., 2018b).\n\n4. **Forecast Accuracy and Impact**:\n   - Measures of forecast accuracy (Hyndman and Koehler, 2006).\n   - Impact of forecast error on enterprises (Kahn, 2003).\n\n5. **General Forecasting Insights**:\n   - Answers to forecasting questions (Jain, 2017).\n   - Concerns and ways forward in statistical and machine learning forecasting methods (Makridakis et al., 2018a).\n\nThe section is a comprehensive collection of references that cover various aspects of forecasting methods, deep learning applications in forecasting, and the results and implications of major forecasting competitions."}}, "data/n_beats.pdf_part_11": {"node_ids": ["5fceb242-60ea-4e4c-8ad7-44d5cfabeb74"], "metadata": {"page_label": "12", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "The section primarily lists references related to time series forecasting, highlighting various methods and models used in the field. Key topics include:\n\n1. **Deep State Space Models**: Referenced in works by Syama Sundar Rangapuram et al. (2018a, 2018b) for time series forecasting.\n2. **Hybrid Methods**: Slawek Smyl's work (2020) on combining exponential smoothing with recurrent neural networks, and another hybrid method by Evangelos Spiliotis et al. (2019) involving data smoothing, the theta method, and seasonal factor shrinkage.\n3. **Data Preprocessing and Augmentation**: Discussed by Slawek Smyl and Karthik Kuber (2016) for short time series forecasting using recurrent neural networks.\n4. **Demand Pattern Categorization**: Addressed by A. A. Syntetos et al. (2005).\n5. **Multivariate Probabilistic Forecasting**: J. Toubeau et al. (2019) focus on short-term scheduling in power markets using deep learning.\n6. **Deep Factors for Forecasting**: Yuyang Wang et al. (2019) discuss this in the context of ICML.\n7. **Temporal Regularized Matrix Factorization**: Hsiang-Fu Yu et al. (2016) for high-dimensional time series prediction.\n8. **Residual Recurrent Highway Networks**: Tehseen Zia and Saad Razzaq (2018) for deep sequence prediction models.\n9. **Exponential Smoothing**: Peter R. Winters (1960) on forecasting sales using exponentially weighted moving averages.\n10. **U.S. Census Bureau**: Reference manual for the X-13ARIMA-SEATS Program (2013).\n\nEntities include researchers, journals, conferences (ICLR, NeurIPS, ICML), and specific forecasting methods and models."}}, "data/n_beats.pdf_part_12": {"node_ids": ["778044aa-ad89-4dcc-a73b-0067ea674ceb"], "metadata": {"page_label": "13", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section primarily discusses the details of three datasets used in forecasting: the M4, M3, and TOURISM datasets. Key topics and entities include:\n\n1. **M4 Dataset**:\n   - Composition: Number of time series based on frequency (Yearly, Quarterly, Monthly, Weekly, Daily, Hourly) and type (Demographic, Finance, Industry, Macro, Micro, Other).\n   - Summary statistics: Minimum, maximum, mean, and standard deviation of series lengths.\n   - Characteristics: Classification of series as smooth or erratic based on the squared coefficient of variation.\n   - Notable: All series have positive observed values at all time-steps, meaning none are intermittent or lumpy.\n\n2. **M3 Dataset**:\n   - Composition: Similar to M4, listing the number of time series based on frequency and type.\n   - Summary statistics: Similar metrics as M4.\n   - Characteristics: Also classified as smooth or erratic.\n   - Notable: All series have positive observed values at all time-steps, with no intermittent or lumpy series.\n\n3. **TOURISM Dataset**:\n   - Composition: Number of time series based on frequency.\n   - Summary statistics: Similar metrics as M4 and M3.\n   - Characteristics: Higher fraction of erratic series compared to M4 and M3.\n   - Notable: All series have positive observed values at all time-steps.\n\nThe section emphasizes the diversity and heterogeneity of the datasets, particularly in the context of business, financial, and economic forecasting."}}, "data/n_beats.pdf_part_13": {"node_ids": ["ae6b3568-0f6f-460b-82b0-9efec80d6402"], "metadata": {"page_label": "14", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary:\nThe section provides detailed compositions of two datasets, M3 and TOURISM, used in time series forecasting. \n\n1. **M3 Dataset**:\n   - **Types and Frequencies**: The dataset is categorized by sampling frequency (Yearly, Quarterly, Monthly, Other) and type (Demographic, Finance, Industry, Macro, Micro, Other).\n   - **Statistics**: It includes the number of time series for each category, minimum, maximum, mean, and standard deviation (SD) lengths of the time series, and the percentage of smooth and erratic series.\n   - **Total Time Series**: 3,003 time series in total.\n\n2. **TOURISM Dataset**:\n   - **Frequencies**: The dataset is categorized by sampling frequency (Yearly, Quarterly, Monthly).\n   - **Statistics**: It includes the number of time series for each frequency, minimum, maximum, mean, and SD lengths of the time series, and the percentage of smooth and erratic series.\n   - **Total Time Series**: 1,311 time series in total.\n\nThe section provides a comprehensive overview of the datasets' compositions, which are crucial for understanding the characteristics and variability of the time series data used in the study."}}, "data/n_beats.pdf_part_14": {"node_ids": ["f58e6968-bee1-4597-93bc-f7225760642e"], "metadata": {"page_label": "15", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section primarily discusses various ablation studies conducted on the N-BEATS architecture to evaluate its performance using the sMAPE metric. Key topics and entities include:\n\n1. **Layer Stacking and Basis Synergy**:\n   - **Generic Architecture**: The study shows that increasing the number of stacks in the generic architecture reduces the error, with diminishing returns after a certain point. The network with 30 stacks (150 layers deep) performs best.\n   - **Interpretable Architecture**: This architecture combines trend and seasonality models in two stacks. The study finds that sharing weights within stacks and using different basis functions for each stack improves performance. The best configuration is a 60-layer deep network.\n\n2. **Ensemble Size**:\n   - Increasing the ensemble size improves performance. N-BEATS achieves state-of-the-art performance with an ensemble size of 18 models, indicating computational efficiency.\n\n3. **Doubly Residual Stacking (DRESS)**:\n   - The DRESS principle involves running a residual backcast connection and producing partial block-level forecasts aggregated at stack and model levels. The study confirms the effectiveness of this topology by comparing it to alternatives where either the backcast or partial forecast links are removed.\n\n### Key Entities:\n- **sMAPE (Symmetric Mean Absolute Percentage Error)**: The performance metric used.\n- **Stacks**: Layers of residual blocks in the network.\n- **Residual Blocks**: Basic building units of the network.\n- **FC Layers (Fully Connected Layers)**: Layers within each residual block.\n- **Trend and Seasonality Models**: Components of the interpretable architecture.\n- **Ensemble Size**: Number of models used in the ensemble.\n- **DRESS (Doubly Residual Stacking)**: A topological principle used in N-BEATS.\n\n### Key Findings:\n- Increasing the number of stacks improves performance up to a point.\n- Combining different basis functions in the interpretable architecture yields better results.\n- Larger ensemble sizes enhance performance, but N-BEATS performs well even with smaller ensembles.\n- The DRESS principle is effective in improving forecasting accuracy."}}, "data/n_beats.pdf_part_15": {"node_ids": ["b480e810-917e-46f6-a546-0525660b8293"], "metadata": {"page_label": "16", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "The section primarily discusses the performance and architecture variations of the N-BEATS model, a neural network for time series forecasting, as evaluated on the M4 dataset. Key topics include:\n\n1. **Ensemble Size and Performance**: Figure 3 illustrates that N-BEATS maintains high performance (measured by OWA) even with a significantly reduced ensemble size.\n2. **N-BEATS-DRESS**: The default configuration using doubly residual stacking, described in Section 3.2.\n3. **Alternative Architectures**:\n   - **PARALLEL**: Disables backward residual connections, with each block forecasting in parallel.\n   - **NO-RESIDUAL**: Disables backward residual connections, with each block's backcast forecast fed to the next block.\n   - **LAST-FORWARD**: Uses backward residual connections but only the last block's forecast is used.\n   - **NO-RESIDUAL-LAST-FORWARD**: Disables both backward residual and partial forward connections, resembling a deep feed-forward network.\n4. **Ablation Study Results**: Quantitative results on the M4 dataset (Tables 7\u201310) show that the doubly residual stacking topology (N-BEATS-DRESS) outperforms other architectures. The study used an ensemble size of 18 for N-BEATS-DRESS, leading to a higher OWA metric compared to N-BEATS-G with an ensemble size of 180.\n\nThe section concludes that the doubly residual stacking topology provides a clear advantage over other architectural variations."}}, "data/n_beats.pdf_part_16": {"node_ids": ["d2f1be17-0961-48eb-a916-72dfb50220da"], "metadata": {"page_label": "17", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThis section of the document, published as a conference paper at ICLR 2020, focuses on the architectural configurations and performance evaluations of the N-BEATS model, specifically in the context of an ablation study of the doubly residual stack. The section includes:\n\n1. **Architectural Configurations**:\n   - **N-BEATS-DRESS**: A specific configuration of the N-BEATS model.\n   - **PARALLEL**: Another configuration where stacks operate in parallel.\n   - **NO-RESIDUAL**: Configuration without residual connections.\n   - **LAST-FORWARD**: Configuration where the last stack's output is forwarded.\n   - **NO-RESIDUAL-LAST-FORWARD**: Combination of no residuals and last-forward.\n   - **RESIDUAL-INPUT**: Configuration with residuals at the input.\n\n2. **Performance Evaluation**:\n   - **Tables 7 and 8**: These tables present the performance of different configurations on the M4 test set using the sMAPE metric (Symmetric Mean Absolute Percentage Error). Lower sMAPE values indicate better performance.\n   - **Table 7**: Performance of an ensemble of 18 generic models across different data frequencies (Yearly, Quarterly, Monthly, Others) and their average.\n   - **Table 8**: Performance of an ensemble of 18 interpretable models across the same data frequencies and their average.\n\n3. **Key Findings**:\n   - **N-BEATS-DRESS-G** and **N-BEATS-DRESS-I** configurations generally show the best performance among the tested configurations.\n   - **NO-RESIDUAL-LAST-FORWARD-G** and **NO-RESIDUAL-LAST-FORWARD-I** configurations tend to perform worse, especially in the \"Others\" category.\n\n### Key Entities:\n- **N-BEATS Model**: A neural network-based forecasting model.\n- **sMAPE**: Symmetric Mean Absolute Percentage Error, a performance metric.\n- **M4 Test Set**: A dataset used for evaluating forecasting models.\n- **Architectural Configurations**: Different setups of the N-BEATS model (e.g., N-BEATS-DRESS, PARALLEL, NO-RESIDUAL, etc.).\n- **Ensemble Models**: Groups of models used together to improve forecasting accuracy.\n- **Data Frequencies**: Categories of data (Yearly, Quarterly, Monthly, Others).\n\nThis section provides a detailed comparison of various architectural configurations of the N-BEATS model and their performance on a standard forecasting dataset."}}, "data/n_beats.pdf_part_17": {"node_ids": ["f8c9dd29-8ce2-4b77-b757-f1e627a59bc1"], "metadata": {"page_label": "18", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary: \n\nThe section presents performance results of various models on the M4 test set, measured using the Overall Weighted Average (OWA) metric, where lower values indicate better performance. The results are divided into two tables: one for an ensemble of 18 generic models and another for an ensemble of 18 interpretable models. Each table provides OWA scores across different time series categories: Yearly, Quarterly, Monthly, and Others, along with an overall average.\n\nKey topics and entities include:\n- Performance metrics (OWA) for time series forecasting models.\n- Comparison of different model configurations: PARALLEL, NO-RESIDUAL, LAST-FORWARD, NO-RESIDUAL-LAST-FORWARD, RESIDUAL-INPUT, and N-BEATS-DRESS.\n- Categories of time series data: Yearly, Quarterly, Monthly, and Others.\n- Two types of model ensembles: generic models and interpretable models.\n- Specific OWA scores for each model configuration and category."}}, "data/n_beats.pdf_part_18": {"node_ids": ["38f09dd6-6aa0-4e1b-861b-5951c618ebee", "b0ba3c88-afc7-40c4-8e66-ccb74692fdc3"], "metadata": {"page_label": "19", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section presents detailed empirical results of the N-BEATS model on the M4 dataset, highlighting its state-of-the-art performance. Key topics and entities include:\n\n1. **Performance Metrics**:\n   - **sMAPE (Symmetric Mean Absolute Percentage Error)**: Lower values indicate better performance.\n   - **OWA (Overall Weighted Average)**: Lower values are better, with rankings provided.\n\n2. **Model Configurations**:\n   - **N-BEATS-G**: Generic configuration.\n   - **N-BEATS-I**: Interpretable configuration.\n   - **N-BEATS-I+G**: Ensemble of both generic and interpretable models.\n\n3. **Comparative Analysis**:\n   - **Best Pure ML**: Submission by B. Trotta.\n   - **Best Statistical**: Model by N.Z. Legaki and K. Koutsouri.\n   - **Best ML/TS Combination**: Model by P. Montero-Manso et al.\n   - **DL/TS Hybrid (M4 Winner)**: Model by S. Smyl.\n\n4. **Results**:\n   - N-BEATS models outperform other approaches across all subsets of time series in the M4 dataset.\n   - The average OWA gap between N-BEATS and the M4 winner is significant, indicating superior performance.\n   - Detailed statistical analysis shows a preponderance of statistically significant differences favoring N-BEATS.\n\n5. **Tables**:\n   - **Table 11**: sMAPE performance across different time series categories (Yearly, Quarterly, Monthly, Others).\n   - **Table 12**: OWA performance and M4 rank across the same categories.\n   - **Table 13**: Granular sMAPE results and statistical significance analysis.\n\nOverall, the section underscores the effectiveness of the N-BEATS model in time series forecasting, particularly in comparison to other leading models from the M4 competition."}}, "data/n_beats.pdf_part_19": {"node_ids": ["f3e4e04c-1705-4346-adfe-5319cfc2bf39"], "metadata": {"page_label": "20", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section presents a performance analysis of the N-BEATS model compared to the Smyl model on non-overlapping subsets of the M4 test set. The analysis is broken down by different series types (Demographic, Finance, Industry, Macro, Micro, Other) and sampling frequencies (Yearly, Quarterly, Monthly, Weekly, Daily, Hourly). Key metrics include the symmetric Mean Absolute Percentage Error (sMAPE) for each series type and frequency.\n\n**Key Topics:**\n1. **Performance Decomposition:** The section details the performance of the N-BEATS model across various subsets of the M4 test set.\n2. **Comparison with Smyl Model:** The performance of N-BEATS is compared against the Smyl model, with differences in sMAPE highlighted.\n3. **Statistical Significance:** Bold entries indicate statistically significant results at the 99% confidence level, determined using a 2-sided paired t-test.\n4. **Error Metrics:** The standard error of the mean is provided for each comparison.\n\n**Key Entities:**\n1. **N-BEATS Model:** The model being evaluated.\n2. **Smyl Model:** The benchmark model for comparison.\n3. **M4 Test Set:** The dataset used for performance evaluation.\n4. **sMAPE (symmetric Mean Absolute Percentage Error):** The primary performance metric.\n5. **Series Types:** Demographic, Finance, Industry, Macro, Micro, Other.\n6. **Sampling Frequencies:** Yearly, Quarterly, Monthly, Weekly, Daily, Hourly.\n\nThe section provides detailed numerical results and statistical analysis to support the performance claims of the N-BEATS model."}}, "data/n_beats.pdf_part_20": {"node_ids": ["53368c95-b561-405f-acfc-d7ca3a02e4bf"], "metadata": {"page_label": "21", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section primarily discusses the performance of various forecasting methods on the M3 dataset, with a focus on the sMAPE (Symmetric Mean Absolute Percentage Error) metric. Key topics and entities include:\n\n1. **Performance Metrics**:\n   - The section provides a detailed table (Table 14) showing the sMAPE performance of different forecasting methods across various data splits (Yearly, Quarterly, Monthly, Others, and Average).\n   - Lower sMAPE values indicate better performance.\n\n2. **Forecasting Methods**:\n   - Various methods are compared, including Na\u00efve2, ARIMA, Comb S-H-D, ForecastPro, Theta, DOTM, EXP, LGT, BaggedETS.BC, and different versions of N-BEATS (N-BEATS-G, N-BEATS-I, N-BEATS-I+G).\n\n3. **Specific Observations**:\n   - Some methods did not report sMAPE for certain splits or overall averages.\n   - LGT had issues with Monthly and Quarterly data due to computational constraints.\n   - EXP's average performance was recomputed using a different methodology for consistency.\n\n4. **Calculation Methodology**:\n   - The section explains the formula used to compute the average sMAPE, which aggregates performance over all time series and forecast horizons.\n   - The formula takes into account the number of time series and the largest forecast horizon for each data split.\n\n5. **References**:\n   - The section references various studies and authors, including Fiorucci et al. (2016), Spiliotis et al. (2019), Smyl & Kuber (2016), and Bergmeir et al. (2016).\n\nOverall, the section provides a comprehensive comparison of forecasting methods using the sMAPE metric on the M3 dataset, highlighting the strengths and limitations of each method."}}, "data/n_beats.pdf_part_21": {"node_ids": ["a4ca3105-6443-47af-801c-fea94043106b"], "metadata": {"page_label": "22", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section primarily discusses the performance of various forecasting models on the TOURISM test set, with a focus on the Mean Absolute Percentage Error (MAPE) metric. Key topics and entities include:\n\n1. **Performance Metrics**: The section presents a detailed comparison of MAPE values for different models across Yearly, Quarterly, and Monthly forecast horizons. Lower MAPE values indicate better performance.\n\n2. **Models Compared**:\n   - **Statistical Benchmarks**: Includes models like SNa\u00efve, Theta, ForePro, ETS, Damped, and ARIMA.\n   - **Kaggle Competitors**: Includes teams like SaliMali, LeeCBaker, Stratometrics, Robert, and Idalgo.\n   - **N-BEATS Variants**: Includes N-BEATS-G, N-BEATS-I, and N-BEATS-I+G, which are the authors' models.\n\n3. **Results**:\n   - **N-BEATS Models**: Achieved state-of-the-art performance across all subsets of the TOURISM dataset, outperforming other models.\n   - **Comparison**: N-BEATS models showed a significant improvement over the best-known approach (LeeCBaker) and auto-ARIMA.\n\n4. **Calculation Methodology**: The section explains how the Average MAPE was calculated using the M4 competition methodology, aggregating results over Yearly, Quarterly, and Monthly splits.\n\n5. **Additional Information**: \n   - The Kaggle competition was divided into Yearly and Quarterly/Monthly forecasting parts.\n   - Some participants only took part in the Quarterly/Monthly forecasting.\n   - The SaliMali team won the Quarterly/Monthly competition using a weighted ensemble of statistical methods.\n\nOverall, the section highlights the superior performance of the N-BEATS models in time series forecasting for the TOURISM dataset."}}, "data/n_beats.pdf_part_22": {"node_ids": ["891ff756-e87e-43bb-af2b-cf8524b6cb53", "326d5ef8-10b3-4593-8964-64c93d044057"], "metadata": {"page_label": "23", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section details an experiment comparing the performance of various forecasting models\u2014MatFact, DeepAR, Deep State, Deep Factors, and N-BEATS\u2014on the ELECTRICITY and TRAFFIC datasets. Key points include:\n\n1. **Datasets and Aggregation**:\n   - ELECTRICITY dataset is aggregated using the sum operation.\n   - TRAFFIC dataset is aggregated using the mean operation.\n   - Aggregation is done hourly, with specific handling of time points.\n\n2. **Data Preparation**:\n   - The first year of the ELECTRICITY dataset is removed to match previous studies.\n   - Efforts were made to match data points and splits used in previous research for comparability.\n\n3. **Challenges in Data Handling**:\n   - Different split points used by various papers create challenges in reproducibility.\n   - Specific difficulties were encountered with the TRAFFIC dataset, including matching dates and handling gaps due to holidays and anomalies.\n\n4. **List of Gaps**:\n   - A detailed list of holidays and anomaly days was provided, which were excluded from the dataset.\n\n5. **Metrics**:\n   - Performance is measured using Normalized Deviation (ND), equivalent to the p50 loss metric used in some models.\n\n### Key Entities:\n- **Models**: MatFact, DeepAR, Deep State, Deep Factors, N-BEATS\n- **Datasets**: ELECTRICITY, TRAFFIC\n- **Metrics**: Normalized Deviation (ND), p50 loss\n- **Dates and Events**: Specific holidays and anomaly days between Jan 1, 2008, and Mar 30, 2009\n\n### Key Topics:\n- Data aggregation methods\n- Data preparation and cleaning\n- Challenges in data handling and reproducibility\n- Performance metrics for forecasting models"}}, "data/n_beats.pdf_part_23": {"node_ids": ["f8c3ee76-6f95-4cd0-8de3-5dc337da4d6d"], "metadata": {"page_label": "24", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary:\nThis section, published as part of a conference paper at ICLR 2020, discusses the Normalized Deviation (ND) performance of various forecasting models on the ELECTRICITY and TRAFFIC test sets. The models compared include MatFact, DeepAR, Deep State, Deep Factors, and different variants of N-BEATS (N-BEATS-G, N-BEATS-I, and N-BEATS-I+G). The results show that N-BEATS models, which do not use any covariates like day-of-week or hour-of-day, generally perform well compared to other models. The N-BEATS architecture used in this experiment is consistent with its application in M4, M3, and TOURISM datasets, with adjustments only in history size and the number of iterations based on validation set performance. The validation set consists of 7 consecutive days before the test set, and the model is retrained on the training set, including the validation set, before being tested using a rolling window operation."}}, "data/n_beats.pdf_part_24": {"node_ids": ["40e7bb39-54b7-4699-b9e2-98de51550dac"], "metadata": {"page_label": "25", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "Summary:\nThe section, published as a conference paper at ICLR 2020, presents detailed results comparing the performance of different forecasting models: DeepAR, DeepState, and N-BEATS. Specifically, it focuses on the ND (Normalized Deviation) performance metrics of these models on the M4-Hourly and TOURISM datasets. The results are summarized in Table 17, which shows that N-BEATS variants (N-BEATS-G, N-BEATS-I, and N-BEATS-I+G) generally outperform DeepAR and DeepState models across the datasets. The key entities discussed are the forecasting models (DeepAR, DeepState, N-BEATS) and the datasets (M4-Hourly, TOURISM Monthly, TOURISM Quarterly)."}}, "data/n_beats.pdf_part_25": {"node_ids": ["da8a39c8-376e-4408-a15e-0ddefd38603e"], "metadata": {"page_label": "26", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section from the document \"n_beats.pdf\" primarily discusses the hyperparameter settings used for training models on different subsets of the M4, M3, and TOURISM datasets. Key topics and entities include:\n\n1. **Datasets and Subsets**:\n   - **M4, M3, TOURISM**: These are the datasets used.\n   - **Subsets**: Yearly (Yly), Quarterly (Qly), Monthly (Mly), Weekly (Wly), Daily (Dly), Hourly (Hly), and Other frequency subsets.\n\n2. **Model Configurations**:\n   - **N-BEATS-I**: Interpretable model configuration.\n   - **N-BEATS-G**: Generic model configuration.\n\n3. **Hyperparameters**:\n   - **LH (Lookback Horizon)**: Defines the length of training history used to generate training samples.\n   - **Iterations**: Number of batches used to train N-BEATS.\n   - **Losses**: Metrics used for model evaluation (e.g., sMAPE, MAPE, MASE).\n   - **S-width, S-blocks, S-block-layers, T-width, T-degree, T-blocks, T-block-layers**: Structural parameters for the models.\n   - **Sharing**: Indicates whether parameters are shared at the stack level.\n   - **Lookback period**: Time periods considered for lookback (e.g., 2H, 3H, etc.).\n   - **Batch size**: Number of samples per batch (1024).\n\n4. **Parameter Settings**:\n   - Detailed settings for both N-BEATS-I and N-BEATS-G across different subsets of the datasets.\n\n5. **Common Parameters**:\n   - Explanation of how LH is used to generate training samples.\n   - Observations on the relationship between the number of time series in subsets and the value of LH.\n\nThe section provides a comprehensive overview of the hyperparameter configurations and their implications for training the N-BEATS models on various time series datasets."}}, "data/n_beats.pdf_part_26": {"node_ids": ["697afe0f-89f1-4a89-82d4-45fb7719586e"], "metadata": {"page_label": "27", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThis section from the N-BEATS paper, published at ICLR 2020, discusses various parameters and configurations used in the N-BEATS model, specifically focusing on the ensemble building, model sharing, lookback period, and batch size. It also details the parameters for two variants of the N-BEATS model: N-BEATS-I (interpretable) and N-BEATS-G (generic).\n\n#### Key Topics and Entities:\n\n1. **Loss Functions**:\n   - Different loss functions are used to build ensembles.\n   - Mixing models trained on various metrics improved performance for M4 and M3 datasets.\n   - For the TOURISM dataset, training only on MAPE yielded the best validation scores.\n\n2. **Sharing**:\n   - Defines whether coefficients in fully-connected layers are shared.\n   - Interpretable model performs best with shared weights across stacks.\n   - Generic model performs best with no shared weights.\n\n3. **Lookback Period**:\n   - Length of the history window forming the input to the model.\n   - Function of the forecast horizon length (H).\n   - Models with lookback periods of 2H, 3H, 4H, 5H, 6H, and 7H were mixed in one ensemble.\n\n4. **Batch Size**:\n   - Batch size used was 1024.\n   - Larger batch sizes sped up training, but gains were minimal beyond 1024.\n\n#### N-BEATS-I Parameters:\n- **S-width**: Width of fully connected layers in the seasonality stack.\n- **S-blocks**: Number of blocks in the seasonality stack.\n- **S-block-layers**: Number of fully-connected layers in one block of the seasonality stack.\n- **T-width**: Width of fully connected layers in the trend stack.\n- **T-degree**: Degree of polynomial in the trend stack.\n- **T-blocks**: Number of blocks in the trend stack.\n- **T-block-layers**: Number of fully-connected layers in one block of the trend stack.\n\n#### N-BEATS-G Parameters:\n- **Width**: Width of fully connected layers in the blocks.\n- **Blocks**: Number of blocks in the stack.\n- **Block-layers**: Number of fully-connected layers in one block of the stack.\n\nThe section provides detailed insights into the configurations and parameters that influence the performance and structure of the N-BEATS model, highlighting the differences between the interpretable and generic variants."}}, "data/n_beats.pdf_part_27": {"node_ids": ["0022fdcf-c7a0-4568-bb48-cda98a48edf9", "956d8558-3ac8-46c3-ace8-69856f9ba622"], "metadata": {"page_label": "28", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section presents a series of graphs and figures illustrating the performance of generic and interpretable configurations of a forecasting model on the M4 dataset. Each row in the figures represents a different time series example, categorized by data frequency: Yearly, Quarterly, Monthly, Weekly, Daily, and Hourly. The magnitudes in each row are normalized by the maximal value of the actual time series for convenience. The columns in the figures show:\n\n- **Column (a):** Actual values (ACTUAL), generic model forecast (FORECAST-G), and interpretable model forecast (FORECAST-I).\n- **Column (b):** STACK1-G (first stack of the generic model).\n- **Column (c):** STACK2-G (second stack of the generic model).\n- **Column (d):** STACK1-I (first stack of the interpretable model).\n- **Column (e):** STACK2-I (second stack of the interpretable model).\n\nThe figures aim to compare the outputs of the generic and interpretable configurations across different time series frequencies, providing insights into their forecasting performance."}}, "data/n_beats.pdf_part_28": {"node_ids": ["82db0eb3-2e6f-4ac1-baec-e58704ee8706", "2298b389-60ff-41ff-9f2f-8849aaecf3e6"], "metadata": {"page_label": "29", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section presents detailed traces of signals for different time series data, corresponding to specific figures and rows in a referenced figure (Fig. 5). The data is categorized into three types of time series: Yearly, Quarterly, and Monthly, each identified by unique IDs (Y3974, Q11588, and M19006 respectively). For each time series, the tables provide actual values and various forecast values (FORECAST-I, FORECAST-G) along with intermediate stack values (STACK1-I, STACK2-I, STACK1-G, STACK2-G) at different time points (t).\n\n#### Key Topics:\n1. **Time Series Analysis**: The section focuses on analyzing time series data.\n2. **Forecasting**: It includes different forecast models (FORECAST-I, FORECAST-G).\n3. **Stacked Models**: Intermediate stack values (STACK1-I, STACK2-I, STACK1-G, STACK2-G) are provided.\n4. **Data Representation**: Detailed numerical data is presented in tabular form.\n\n#### Key Entities:\n1. **Time Series Types**:\n   - Yearly (ID: Y3974)\n   - Quarterly (ID: Q11588)\n   - Monthly (ID: M19006)\n2. **Forecast Models**:\n   - FORECAST-I\n   - FORECAST-G\n3. **Stack Values**:\n   - STACK1-I\n   - STACK2-I\n   - STACK1-G\n   - STACK2-G\n4. **Time Points (t)**: Specific time points at which data is recorded.\n\nThe section is part of a conference paper published at ICLR 2020 and provides a detailed numerical analysis of the forecasting performance for different time series data."}}, "data/n_beats.pdf_part_29": {"node_ids": ["5f1d4f20-a765-40f1-b0a6-6c1f8f7448b1", "cc2b9c8b-0307-4334-ae4a-289c97e92026"], "metadata": {"page_label": "30", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section provides detailed traces of signals for two specific time series datasets, Weekly (id W246) and Daily (id D404), as depicted in rows 4 and 5 of Figure 5 from the paper published at ICLR 2020. The tables (Table 22 and Table 23) list the actual values and various forecast values (FORECAST-I, FORECAST-G) along with intermediate stack values (STACK1-I, STACK2-I, STACK1-G, STACK2-G) for different time steps (t).\n\n#### Key Topics:\n1. **Time Series Analysis**: The section focuses on the analysis of time series data, specifically weekly and daily datasets.\n2. **Forecasting**: It includes forecast values generated by different models or methods.\n3. **Signal Traces**: Detailed traces of signals are provided, showing how the actual values compare with forecasted values and intermediate stack values.\n\n#### Key Entities:\n1. **Time Series IDs**: W246 (Weekly), D404 (Daily)\n2. **Forecast Values**: FORECAST-I, FORECAST-G\n3. **Intermediate Stack Values**: STACK1-I, STACK2-I, STACK1-G, STACK2-G\n4. **Actual Values**: Represented as \"ACTUAL\"\n5. **Time Steps**: Represented as \"t\" ranging from 0 to 12 for Weekly and 0 to 10 for Daily.\n\nThe section is highly data-centric, providing numerical insights into the performance and behavior of forecasting models over specified time periods."}}, "data/n_beats.pdf_part_30": {"node_ids": ["c82aee7a-594a-4708-9ad7-f89c0a84b4a6", "0f1440b0-4581-4f95-8c77-0c01ef8d1b2b"], "metadata": {"page_label": "31", "file_name": "n_beats.pdf", "file_path": "data/n_beats.pdf", "file_type": "application/pdf", "file_size": 1119473, "creation_date": "2024-08-27", "last_modified_date": "2024-08-16", "section_summary": "### Summary:\n\nThe section presents detailed traces of signals for a specific time series identified as \"Hourly: id H344,\" which is depicted in row 6 of Figure 5 in the document. The data is organized in a table (Table 24) and includes various columns representing actual values and different forecast models (FORECAST-I, FORECAST-G) as well as stack values (STACK1-I, STACK2-I, STACK1-G, STACK2-G) over a series of time points (t). The table provides numerical values for each of these categories at different time intervals, illustrating the performance and comparison of the forecast models against the actual data.\n\n### Key Topics and Entities:\n- **Time Series Data**: Specifically for \"Hourly: id H344.\"\n- **Forecast Models**: FORECAST-I and FORECAST-G.\n- **Stack Values**: STACK1-I, STACK2-I, STACK1-G, STACK2-G.\n- **Actual Values**: Represented in the column labeled \"ACTUAL.\"\n- **Time Points (t)**: Sequential time intervals from 0 to 25.\n- **Numerical Data**: Detailed values for each category at each time point.\n\nThe section is part of a conference paper published at ICLR 2020 and provides a granular view of the performance of different forecasting models in comparison to actual observed values."}}}}